{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Custom 데이터셋 (입력/출력 쌍)\n",
    "# 질문과 답변을 딕셔너리로 구성\n",
    "data = {\n",
    "    'input': ['안녕', '이름이 뭐야?', '뭐해?', '날씨 어때?', '배고파', '심심해', '너 몇 살이야?', '잘자', '고마워', '안녕히 계세요'],\n",
    "    'output': ['안녕하세요', '나는 챗봇이야', '그냥 쉬고 있어', '맑고 좋아', '밥 먹어야지!', '놀아줄까?', '비밀이야', '좋은 꿈 꿔!', '천만에요', '또 만나요']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 출력 문장에 시작과 끝 토큰 추가\n",
    "# >> [start]부터 [end]까지가 답변이다\"를 모델에게 알려주기 위해서\n",
    "df['output'] = df['output'].apply(lambda x: '[start] ' + x + ' [end]')\n",
    "\n",
    "\n",
    "# 2. Tokenizer 준비: 텍스트를 숫자로 바꾸는 Tokenizer\n",
    "# > 컴퓨터는 글자를 이해 못하니까 숫자로 바꾸는 작업\n",
    "# > tokenizer는 각 단어에 고유 번호를 부여해주는 도구\n",
    "tokenizer_in = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer_out = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer_in.fit_on_texts(df['input'])\n",
    "tokenizer_out.fit_on_texts(df['output'])\n",
    "\n",
    "input_seq = tokenizer_in.texts_to_sequences(df['input'])\n",
    "output_seq = tokenizer_out.texts_to_sequences(df['output'])\n",
    "\n",
    "# 3. Padding: 시퀀스를 길이에 맞게 패딩\n",
    "# > 문장마다 길이가 다르기 때문에 같은 길이로 맞춰주는 작업\n",
    "# > 빈 칸은 0으로 채워져요.\n",
    "# → 마치 “시험지 한 줄에 다 안 들어가면 빈칸에 0점 처리하는 것”과 비슷\n",
    "max_len_in = max(len(i) for i in input_seq)\n",
    "max_len_out = max(len(i) for i in output_seq)\n",
    "input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_len_in, padding='post')\n",
    "output_seq = tf.keras.preprocessing.sequence.pad_sequences(output_seq, maxlen=max_len_out, padding='post')\n",
    "\n",
    "# 디코더 예측 타깃 시퀀스 (디코더의 출력은 한 칸씩(오른쪽) shift 되어 있음)\n",
    "# 예)\n",
    "# > 입력: [start] 안녕하세요\n",
    "# > 타깃: 안녕하세요 [end]\n",
    "target_seq = np.concatenate([output_seq[:, 1:], np.zeros((len(output_seq), 1))], axis=-1)\n",
    "\n",
    "\n",
    "# 4. Positional Encoding 정의\n",
    "# transformer는 순서를 모르기 때문에, 위치 정보를 따로 더해줘야 해요\n",
    "# 이건 각 단어가 문장 내에서 몇 번째에 위치하는지 알려주는 값\n",
    "# 비유: 포지셔널 인코딩은 마치 '나 이 문장의 첫 번째 단어야', '난 세 번째 단어야' 하고 스스로 이름표 붙이는 작업\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # numpy로 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(d_model)[np.newaxis, :],\n",
    "            d_model\n",
    "        )\n",
    "\n",
    "        # 짝수 인덱스에는 sin, 홀수 인덱스에는 cos 적용\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "\n",
    "# 5. Transformer 기반 간단 챗봇 모델 정의\n",
    "vocab_in = len(tokenizer_in.word_index) + 1\n",
    "vocab_out = len(tokenizer_out.word_index) + 1\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "dff = 256\n",
    "\n",
    "class ChatBot(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_embedding = tf.keras.layers.Embedding(vocab_in, d_model)\n",
    "        self.dec_embedding = tf.keras.layers.Embedding(vocab_out, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(100, d_model)\n",
    "\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.final = tf.keras.layers.Dense(vocab_out)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        enc_inputs = inputs['enc_inputs']\n",
    "        dec_inputs = inputs['dec_inputs']\n",
    "\n",
    "        enc_embed = self.enc_embedding(enc_inputs)\n",
    "        enc_embed = self.pos_encoding(enc_embed)\n",
    "\n",
    "        dec_embed = self.dec_embedding(dec_inputs)\n",
    "        dec_embed = self.pos_encoding(dec_embed)\n",
    "\n",
    "        attn_output = self.attention(dec_embed, enc_embed, enc_embed)\n",
    "        ffn_output = self.ffn(attn_output)\n",
    "        return self.final(ffn_output)\n",
    "\n",
    "\n",
    "# 6. 모델 컴파일 및 학습\n",
    "model = ChatBot()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "model.fit({'enc_inputs': input_seq, 'dec_inputs': output_seq}, target_seq, epochs=300, verbose=0)\n",
    "\n",
    "\n",
    "# 7. 챗봇 응답 함수\n",
    "def chat(sentence):\n",
    "    seq = tokenizer_in.texts_to_sequences([sentence])\n",
    "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len_in, padding='post')\n",
    "\n",
    "    output = [tokenizer_out.word_index['[start]']]\n",
    "    for _ in range(max_len_out):\n",
    "        output_pad = tf.keras.preprocessing.sequence.pad_sequences([output], maxlen=max_len_out, padding='post')\n",
    "        # 딕셔너리 형태로 predict 입력\n",
    "        prediction = model.predict({'enc_inputs': seq, 'dec_inputs': output_pad}, verbose=0)\n",
    "        pred_id = tf.argmax(prediction[0, len(output)-1]).numpy()\n",
    "        if pred_id == tokenizer_out.word_index.get('[end]', 0):\n",
    "            break\n",
    "        output.append(pred_id)\n",
    "\n",
    "    response = tokenizer_out.sequences_to_texts([output])[0]\n",
    "    response = response.replace('[start]', '').replace('[end]', '').strip()\n",
    "    print(f\"Me: {sentence}\\n챗봇: {response}\\n\")\n",
    "\n",
    "\n",
    "# 8. 테스트\n",
    "chat(\"안녕\")\n",
    "chat(\"이름이 뭐야?\")\n",
    "chat(\"심심해\")\n",
    "chat(\"고마워\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
