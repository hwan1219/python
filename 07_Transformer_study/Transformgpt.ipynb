{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 유사도 예측 (두 문장 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #1\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense #2 \n",
    "from tensorflow.keras.models import Model #3\n",
    "#1) tensorflow : 딥러닝 프레임워크\n",
    "#2) Input, Embedding, Dense : 모델 구성에 필요한 층\n",
    "#3) Model : 전체 모델을 정의하기 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 입력 정의\n",
    "input1 = Input(shape=(10,), name='input1')\n",
    "input2 = Input(shape=(10,), name='input2')\n",
    "# shape=(10,): 각 문장은 최대 10개의 단어(정수로 바뀐)로 구성돼 있다고 가정\n",
    "# Input(): 모델의 입력을 정의\n",
    "\n",
    "embedding = Embedding(input_dim=1000, output_dim=32, name='shared_embedding')\n",
    "# input_dim=1000: 전체 단어 사전 크기 (즉, 정수 인덱스는 0~999까지 가능)\n",
    "# output_dim=32: 각 단어를 32차원 벡터로 바꿈\n",
    "# 공통 임베딩 레이어를 쓰는 이유: 두 문장을 같은 기준으로 표현하기 위해서야 (공정한 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 임베딩 적용\n",
    "x1 = embedding(input1)\n",
    "x2 = embedding(input2)\n",
    "# input1, input2에 embedding 레이어를 적용해서 벡터 형태로 변환\n",
    "# 두 문장은 이제 (batch_size, 10, 32) 크기의 벡터 시퀀스가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transformer Encoder 적용\n",
    "import keras_nlp\n",
    "\n",
    "encoder_layer = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=64,\n",
    "    num_heads=2,\n",
    ")\n",
    "x1 = encoder_layer(x1)\n",
    "x2 = encoder_layer(x2)\n",
    "# TransformerEncoder: 문장 내 단어들 사이 관계를 파악함 (자기 주의, Self-Attention)\n",
    "# num_heads=2: attention을 2개의 관점에서 보겠다는 의미\n",
    "# intermediate_dim=64: 내부 feed-forward 네트워크 크기\n",
    "# 두 문장에 같은 인코더를 쓰는 이유 : 같은 방식으로 의미를 추출해서 비교할 수 있게 하기위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 문장 요약 (Pooling)\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "x1 = Lambda(lambda x: tf.reduce_mean(x, axis=1))(x1)\n",
    "x2 = Lambda(lambda x: tf.reduce_mean(x, axis=1))(x2)\n",
    "# 각 문장의 단어 벡터들을 평균 내서 하나의 문장 벡터로 축약\n",
    "# shape: (batch_size, 32) → 한 문장을 대표하는 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 두 문장 비교 (차이 계산)\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "x = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([x1, x2])\n",
    "# 절댓값 차이를 계산하면 두 문장의 벡터가 얼마나 다른지를 수치로 볼 수 있음\n",
    "# 차이가 작으면 비슷한 문장, 크면 다른 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 출력층 구성 (시그모이드)\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "# 시그모이드는 0~1 사이 값을 줌 → 유사도 확률로 해석 가능\n",
    "# 1에 가까울수록 비슷한 문장\n",
    "# Dense(1) : 출력 뉴런 1개 (→ 유사도 점수 하나만 출력함)\n",
    "# activation='sigmoid' : 0~1 사이 확률로 출력되게 함\n",
    "# (x) : 이전 단계에서 나온 두 문장의 차이값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 모델 정의 & 컴파일\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Model: 입력 2개, 출력 1개인 구조 정의\n",
    "# optimizer='adam' : 가장 많이 쓰는 옵티마이저\n",
    "# binary_crossentropy: 두 문장이 같은지(1), 다른지(0)를 판단하는 이진 분류 문제이기 때문\n",
    "# metrics=['accuracy'] : 정확도 확인용\n",
    "# adam: 최적화 알고리즘 (학습을 효율적으로 해줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 예측 테스트\n",
    "import numpy as np\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 두 문장을 정수 시퀀스로 변환\n",
    "    seq1 = tokenizer.texts_to_sequences([sentence1])\n",
    "    seq2 = tokenizer.texts_to_sequences([sentence2])\n",
    "    \n",
    "    # 패딩 (입력 길이에 맞게 0으로 채워줌)\n",
    "    seq1 = tf.keras.preprocessing.sequence.pad_sequences(seq1, maxlen=10)\n",
    "    seq2 = tf.keras.preprocessing.sequence.pad_sequences(seq2, maxlen=10)\n",
    "\n",
    "    # 예측\n",
    "    pred = model.predict([seq1, seq2])\n",
    "    \n",
    "    print(f'유사도 예측 결과: {pred[0][0]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. 훈련 문장 준비 (예시)\n",
    "sentences = [\n",
    "    \"나는 영화를 좋아해\",\n",
    "    \"너는 축구를 좋아하니?\",\n",
    "    \"오늘 날씨가 좋다\",\n",
    "    \"나는 영화를 정말 좋아해\",\n",
    "    \"축구는 재미있어\"\n",
    "]\n",
    "\n",
    "# 2. 토크나이저 생성 및 훈련\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# 3. 최대 길이 정의\n",
    "max_len = max(len(s.split()) for s in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_similarity(\"나는 영화를 좋아해\", \"나는 영화를 정말 좋아해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
