{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 문장 전처리 함수\n",
    "# -----------------------------\n",
    "def tokenize(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 데이터 정의\n",
    "# -----------------------------\n",
    "\n",
    "df = pd.read_csv(\"ko_en_100k.csv\")\n",
    "raw_inputs = df['ko'].tolist()\n",
    "raw_outputs = df['en'].tolist()\n",
    "# 전처리\n",
    "inputs = [tokenize(s) for s in raw_inputs]\n",
    "outputs = [tokenize(s) for s in raw_outputs]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 토크나이저 및 시퀀스 변환\n",
    "# -----------------------------\n",
    "inp_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "inp_tokenizer.fit_on_texts(inputs)\n",
    "inp_seq = pad_sequences(inp_tokenizer.texts_to_sequences(inputs), padding='post')\n",
    "\n",
    "out_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "out_tokenizer.fit_on_texts(outputs)\n",
    "out_seq = pad_sequences(out_tokenizer.texts_to_sequences(outputs), padding='post')\n",
    "\n",
    "# 시작/끝 토큰 ID\n",
    "start_token = out_tokenizer.word_index['<start>']\n",
    "end_token = out_tokenizer.word_index['<end>']\n",
    "\n",
    "# 인덱스 → 단어 맵\n",
    "index_to_word = {idx: word for word, idx in out_tokenizer.word_index.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Transformer 모델 정의\n",
    "# -----------------------------\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "FF_DIM = 128\n",
    "ENC_VOCAB = len(inp_tokenizer.word_index) + 1\n",
    "DEC_VOCAB = len(out_tokenizer.word_index) + 1\n",
    "\n",
    "class SimpleEncoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = layers.Embedding(ENC_VOCAB, EMBED_DIM)\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            layers.Dense(FF_DIM, activation='relu'),\n",
    "            layers.Dense(EMBED_DIM)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.attn(x, x, x) + x\n",
    "        return self.ff(x)\n",
    "\n",
    "class SimpleDecoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = layers.Embedding(DEC_VOCAB, EMBED_DIM)\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            layers.Dense(FF_DIM, activation='relu'),\n",
    "            layers.Dense(EMBED_DIM)\n",
    "        ])\n",
    "        self.out = layers.Dense(DEC_VOCAB)\n",
    "\n",
    "    def call(self, x, enc_output):\n",
    "        x = self.embed(x)\n",
    "        x = self.attn(x, enc_output, enc_output) + x\n",
    "        x = self.ff(x)\n",
    "        return self.out(x)\n",
    "\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder()\n",
    "        self.decoder = SimpleDecoder()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp, out = inputs\n",
    "        enc_out = self.encoder(inp)\n",
    "        dec_out = self.decoder(out, enc_out)\n",
    "        return dec_out\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 모델 학습\n",
    "# -----------------------------\n",
    "model = TransformerModel()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "decoder_input = out_seq[:, :-1]\n",
    "decoder_target = out_seq[:, 1:]\n",
    "decoder_target = np.expand_dims(decoder_target, -1)\n",
    "\n",
    "model.fit([inp_seq, decoder_input], decoder_target, epochs=100, verbose=0)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. 번역 함수\n",
    "# -----------------------------\n",
    "max_input_len = inp_seq.shape[1]\n",
    "max_output_len = out_seq.shape[1]\n",
    "\n",
    "def sample_from_probs(probs, temperature=1.0):\n",
    "    probs = np.asarray(probs).astype('float64')\n",
    "    probs = np.clip(probs, 1e-10, 1.0)\n",
    "    probs = np.log(probs) / temperature\n",
    "    exp_probs = np.exp(probs - np.max(probs)) \n",
    "    probs = exp_probs / np.sum(exp_probs)\n",
    "    if np.any(np.isnan(probs)) or np.sum(probs) == 0:\n",
    "        probs = np.ones_like(probs) / len(probs)\n",
    "\n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def translate(sentence):\n",
    "    sentence = tokenize(sentence)\n",
    "    input_seq = inp_tokenizer.texts_to_sequences([sentence])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_input_len, padding='post')\n",
    "\n",
    "    decoded_sentence = \"\"\n",
    "    decoder_input_ids = [start_token]\n",
    "\n",
    "    for _ in range(max_output_len):\n",
    "        dec_input = pad_sequences([decoder_input_ids], maxlen=max_output_len, padding='post')\n",
    "        preds = model.predict([input_seq, dec_input], verbose=0)\n",
    "\n",
    "        time_step = min(len(decoder_input_ids) - 1, preds.shape[1] - 1)\n",
    "        pred_id = sample_from_probs(preds[0, time_step], temperature=0.8)\n",
    "\n",
    "        if pred_id == end_token:\n",
    "            break\n",
    "\n",
    "        decoded_sentence += index_to_word.get(pred_id, '') + \" \"\n",
    "        decoder_input_ids.append(pred_id)\n",
    "\n",
    "    return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. 테스트\n",
    "# -----------------------------\n",
    "print(\"입력:\", \"나는 밥을 먹었다\")\n",
    "print(\"번역 결과:\", translate(\"나는 밥을 먹었다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
