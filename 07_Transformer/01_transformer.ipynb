{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "# Seq2Seq_attention_번역_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 실습1 - Seq2Seq 모델을 학습시켜서 번역기 만들기\n",
    "\n",
    "# Encoder, Decoder 를 만든 뒤 Seq2Seq 모델로 조립한 후, 프랑스어와 영어로 매칭된 사전을 학습시켜 번역기 만들기\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "lines = pd.read_csv(\n",
    "  'fra.txt',\n",
    "  names=['src', 'tar', 'lic'],\n",
    "  sep='\\t' # \\t 을 기준으로 컬럼 분리\n",
    ")\n",
    "del lines['lic'] # license 컬럼이 존재하는 데이터셋의 경우 필요없기 때문에 삭제\n",
    "print(f\"전체 샘플의 개수: {len(lines)}\")\n",
    "\n",
    "# 데이터셋 슬라이싱\n",
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[:30000]\n",
    "display(lines.sample(10)) # 전체 DataFrame에서 랜덤하게 (n)개의 행을 추출\n",
    "\n",
    "# seq2seq 모델 학습을 위한 특수 토큰 삽입\n",
    "# \\t = 시작(start-of-sequence) 토큰\n",
    "# \\n = 시작(end-of-sequence) 토큰\n",
    "lines.tar = lines.tar.apply(lambda x: '\\t' + x + '\\n')\n",
    "display(lines.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "  for char in line:\n",
    "    src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "  for char in line:\n",
    "    tar_vocab.add(char)\n",
    "\n",
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "print(f\"source 문장의 char집합: {src_vocab_size}\")\n",
    "print(f\"target 문장의 char집합: {tar_vocab_size}\\n\")\n",
    "\n",
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(f\"{src_vocab[:10]}\")\n",
    "print(f\"{tar_vocab[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 인덱스화\n",
    "\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(f\"{src_to_index}\")\n",
    "print(f\"{tar_to_index}\\n\")\n",
    "\n",
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    encoded_line.append(src_to_index[char])\n",
    "  encoder_input.append(encoded_line)\n",
    "print(f\"원래 source 문장:\\n{lines.src[:3]}\")\n",
    "print(f\"인코더 입력 시퀀스: {encoder_input[:3]}\\n\")\n",
    "\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    encoded_line.append(tar_to_index[char])\n",
    "  decoder_input.append(encoded_line)\n",
    "print(f\"디코더 입력 시퀀스: {decoder_input[:3]}\\n\")\n",
    "\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "  timestep = 0\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    if timestep > 0:\n",
    "      encoded_line.append(tar_to_index[char])\n",
    "    timestep = timestep + 1\n",
    "  decoder_target.append(encoded_line)\n",
    "print(f\"디코더 정답 시퀀스: {decoder_target[:3]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제로 패딩을 적용하기 위한 최대 길이\n",
    "# + One-Hot Encoding\n",
    "\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(f\"source 문장의 최대 길이: {max_src_len}\")\n",
    "print(f\"source 문장의 최대 길이: {max_tar_len}\\n\")\n",
    "\n",
    "encoder_input = pad_sequences(\n",
    "  encoder_input,\n",
    "  maxlen=max_src_len,\n",
    "  padding='post'\n",
    ")\n",
    "decoder_input = pad_sequences(\n",
    "  decoder_input,\n",
    "  maxlen=max_tar_len,\n",
    "  padding='post'\n",
    ")\n",
    "decoder_target = pad_sequences(\n",
    "  decoder_target,\n",
    "  maxlen=max_tar_len,\n",
    "  padding='post'\n",
    ")\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(256, return_state=True) # Encoder는 return_state를 설정한 LSTM 선언\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c] # LSTM 은 상태가 두개 (은닉 상태와 셀 상태)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(256, return_state=True, return_sequences=True) # Decoder는 return_state와 return_sequences를 설정한 LSTM 선언\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) # Decoder에게 Encoder의 은닉 상태, 셀 상태를 전달\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "seq2seq_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model(\n",
    "  [encoder_inputs, decoder_inputs],\n",
    "  seq2seq_outputs\n",
    ")\n",
    "model.compile(\n",
    "  optimizer='rmsprop',\n",
    "  loss='categorical_crossentropy'\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "  monitor='val_loss',\n",
    "  patience=5\n",
    ")\n",
    "history = model.fit(\n",
    "  x=[encoder_input, decoder_input],\n",
    "  y=decoder_target,\n",
    "  batch_size=64,\n",
    "  epochs=50,\n",
    "  validation_split=0.2,\n",
    "  callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론용 인코더, 디코더 모델 재정의\n",
    "\n",
    "encoder_model = Model(\n",
    "  inputs=encoder_inputs,\n",
    "  outputs=encoder_states\n",
    ")\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용\n",
    "# decoder_lstm 을 사용하면서 state_h, state_c 같이 저장하기\n",
    "decoder_outputs_inf, state_h, state_c = decoder_lstm(\n",
    "  decoder_inputs,\n",
    "  initial_state=decoder_states_inputs\n",
    ")\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM 의 리턴하는 은닉 상태와 셀 상태를 버리지 않음\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_probs = decoder_softmax_layer(decoder_outputs_inf)\n",
    "decoder_model = Model(\n",
    "  inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "  outputs=[decoder_probs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 디코딩\n",
    "\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "  # 입력으로부터 인코더의 상태를 얻음\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "  \n",
    "  # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "  target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "  target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "  \n",
    "  stop_condition = False\n",
    "  decoded_sentence = \"\"\n",
    "  \n",
    "  # stop_condition 이 True 가 될 때까지 루프 반복\n",
    "  while not stop_condition:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    # 예측 결과를 문자로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "    \n",
    "    # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "    decoded_sentence += sampled_char\n",
    "    \n",
    "    # <eos>에 도달하거나 최대 길이를 넘으면 중단\n",
    "    if(sampled_char == '\\n' or len(decoded_sentence) > max_tar_len):\n",
    "      stop_condition = True\n",
    "      \n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "    \n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "  \n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input[seq_index:seq_index+1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "  print(f\"입력 문장: {lines.src[seq_index]}\")\n",
    "  print(f\"정답 문장: {lines.tar[seq_index][1:len(lines.tar[seq_index])]}\")\n",
    "  print(f\"번역 문장: {decoded_sentence[0:len(decoded_sentence)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
