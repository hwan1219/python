{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "# Seq2Seq_attention_번역_Q\n",
    "\n",
    "# Seq2Seq 모델 + Attention\n",
    "# Transformer 모델과는 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 실습2 - Seq2Seq 모델 + Attention\n",
    "\n",
    "# RNN 기반 Seq2Seq 모델에 Attention 을 추가\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Dot, Activation, Lambda, Softmax\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print(f\"전체 샘플의 개수: {len(lines)}\")\n",
    "\n",
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:30000]\n",
    "display(lines.sample(10))\n",
    "\n",
    "lines.tar = lines.tar.apply(lambda x: '\\t' + x + '\\n')\n",
    "display(lines.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "  for char in line:\n",
    "    src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "  for char in line:\n",
    "    tar_vocab.add(char)\n",
    "\n",
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "print(f\"source 문장의 char집합: {src_vocab_size}\")\n",
    "print(f\"target 문장의 char집합: {tar_vocab_size}\\n\")\n",
    "\n",
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(f\"{src_vocab[:10]}\")\n",
    "print(f\"{tar_vocab[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 인덱스화\n",
    "\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(f\"{src_to_index}\")\n",
    "print(f\"{tar_to_index}\\n\")\n",
    "\n",
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    encoded_line.append(src_to_index[char])\n",
    "  encoder_input.append(encoded_line)\n",
    "print(f\"원래 source 문장:\\n{lines.src[:3]}\")\n",
    "print(f\"인코더 입력 시퀀스: {encoder_input[:3]}\\n\")\n",
    "\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    encoded_line.append(tar_to_index[char])\n",
    "  decoder_input.append(encoded_line)\n",
    "print(f\"디코더 입력 시퀀스: {decoder_input[:3]}\\n\")\n",
    "\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "  timestep = 0\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    if timestep > 0:\n",
    "      encoded_line.append(tar_to_index[char])\n",
    "    timestep = timestep + 1\n",
    "  decoder_target.append(encoded_line)\n",
    "print(f\"디코더 정답 시퀀스: {decoder_target[:3]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제로 패딩을 적용하기 위한 최대 길이\n",
    "# + One-Hot Encoding\n",
    "\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(f\"source 문장의 최대 길이: {max_src_len}\")\n",
    "print(f\"target 문장의 최대 길이: {max_tar_len}\")\n",
    "\n",
    "encoder_input = pad_sequences(\n",
    "  encoder_input,\n",
    "  maxlen=max_src_len,\n",
    "  padding='post'\n",
    ")\n",
    "decoder_input = pad_sequences(\n",
    "  decoder_input,\n",
    "  maxlen=max_tar_len,\n",
    "  padding='post'\n",
    ")\n",
    "decoder_target = pad_sequences(\n",
    "  decoder_target,\n",
    "  maxlen=max_tar_len,\n",
    "  padding='post'\n",
    ")\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "# Attention lyaer 를 추가한 Seq2Seq 모델 사용\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(AttentionLayer, self).__init__()\n",
    "    \n",
    "  def call(self, query, key, value):\n",
    "    scores = tf.matmul(query, key, transpose_b=True)\n",
    "    attention_weights = Softmax(axis=-1)(scores)\n",
    "    context_vector = tf.matmul(attention_weights, value)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 정의 \n",
    "encoder_inputs = Input(shape=(None, src_vocab_size)) \n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True) \n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs) \n",
    " \n",
    "# 디코더 정의 \n",
    "decoder_inputs = Input(shape=(None, tar_vocab_size)) \n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True) \n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 레이어 추가 \n",
    "\n",
    "# AttentioLayer  선언 \n",
    "attention_layer = AttentionLayer() \n",
    "\n",
    "# context_vector, attention_weights 에 출력 담기\n",
    "context_vector, attention_weights = attention_layer(decoder_outputs, encoder_outputs, encoder_outputs)\n",
    "\n",
    "# 컨텍스트 벡터와 디코더 출력을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
    "\n",
    "# 출력 레이어 \n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax') \n",
    "decoder_outputs = decoder_dense(decoder_concat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "model = Model(\n",
    "  [encoder_inputs, decoder_inputs],\n",
    "  decoder_outputs\n",
    ")\n",
    "model.compile(\n",
    "  optimizer=RMSprop(),\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 모델 학습 \n",
    "model.fit(\n",
    "  [encoder_input, decoder_input],\n",
    "  decoder_target,\n",
    "  batch_size=64,\n",
    "  epochs=2,\n",
    "  validation_split=0.2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론용 인코더, 디코더 모델 재정의\n",
    "\n",
    "# 인코더 모델\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 디코더\n",
    "# 입력 정의\n",
    "decoder_state_input_h = Input(shape=(256,), name=\"decoder_state_input_h\")\n",
    "decoder_state_input_c = Input(shape=(256,), name=\"decoder_state_input_c\")\n",
    "\n",
    "# 디코더 LSTM\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# 어텐션 레이어 추가\n",
    "context_vector, attention_weights = attention_layer(decoder_outputs, encoder_outputs, encoder_outputs)\n",
    "\n",
    "# 컨텍스트 벡터와 디코더 출력을 결합\n",
    "decoder_concat_input = Concatenate(name=\"concatenate_layer\")([context_vector, decoder_outputs])\n",
    "\n",
    "# 최종 출력 레이어\n",
    "decoder_final_output = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# 디코더 모델 생성\n",
    "decoder_model = Model(\n",
    "  inputs=[decoder_inputs, encoder_outputs, decoder_state_input_h, decoder_state_input_c],\n",
    "  outputs=[decoder_final_output, state_h, state_c, attention_weights]\n",
    ")\n",
    "\n",
    "# 오류 메시지에 나타난 문제를 디버깅하기 위해 모델의 개요를 출력\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 디코딩\n",
    "\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "  # 인코더의 상태를 얻음\n",
    "  encoder_output, state_h, state_c = encoder_model.predict(input_seq)\n",
    "\n",
    "  # 디코더의 초기 입력 (시작 심볼)\n",
    "  target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "  target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "  # 디코딩 루프\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "  while not stop_condition:\n",
    "    output_tokens, h, c, a = decoder_model.predict([target_seq, encoder_output, state_h, state_c])\n",
    "\n",
    "    # 샘플링\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "    decoded_sentence += sampled_char\n",
    "\n",
    "    # 종료 조건: 최대 길이 초과 또는 종료 심볼\n",
    "    if (sampled_char == '\\n' or len(decoded_sentence) > max_tar_len):\n",
    "      stop_condition = True\n",
    "\n",
    "    # 다음 디코더 입력 업데이트\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    # 상태 업데이트\n",
    "    state_h, state_c = h, c\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input[seq_index:seq_index+1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "  print(f\"입력 문장: {lines.src[seq_index]}\")\n",
    "  print(f\"정답 문장: {lines.tar[seq_index][1:len(lines.tar[seq_index])]}\")\n",
    "  print(f\"번역 문장: {decoded_sentence[0:len(decoded_sentence)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
