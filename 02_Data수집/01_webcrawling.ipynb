{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install requests\n",
    "!python -m pip install beautifulsoup4\n",
    "!python -m pip install lxml\n",
    "!python -m pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robots.txt\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://www.naver.com/robots.txt\"\n",
    "response = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. urllib\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://google.com\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urlopen() 으로 웹 페이지 열기\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://www.example.com\"\n",
    "response = urlopen(url)\n",
    "html = response.read().decode('utf-8')\n",
    "print(html[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request 객체로 User-Agent 추가\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "url = \"https://www.example.com\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "req = Request(url, headers=headers)\n",
    "response = urlopen(req)\n",
    "html = response.read().decode('utf-8')\n",
    "print(html[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.parse를 이용한 URL 분석 \n",
    "\n",
    "from urllib.parse import urlparse, parse_qs \n",
    " \n",
    "url = \"https://www.example.com/search?q=python&sort=latest\" \n",
    " \n",
    "parsed = urlparse(url) \n",
    "print(parsed) \n",
    " \n",
    "params = parse_qs(parsed.query) \n",
    "print(params)  # {'q': ['python'], 'sort': ['latest']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.parse.urlencode()로 URL 파라미터 생성\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "params = {\n",
    "    'q': 'chatgpt',\n",
    "    'lang': 'ko',\n",
    "    'page': 1\n",
    "} \n",
    " \n",
    "query_string = urlencode(params)\n",
    "url = f\"https://www.example.com/search?{query_string}\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일 다운로드 예제\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "image_url = \"https://picsum.photos/500/300\"\n",
    "save_path = \"image/downloaded_image.png\"\n",
    "\n",
    "urlretrieve(image_url, save_path)\n",
    "print(\"이미지 다운로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 시 robots.txt 분석 (크롤링 허용 여부 확인) \n",
    "from urllib.robotparser import RobotFileParser \n",
    " \n",
    "rp = RobotFileParser() \n",
    "rp.set_url(\"https://www.example.com/robots.txt\") \n",
    "rp.read() \n",
    " \n",
    "can_fetch = rp.can_fetch(\"*\", \"https://www.example.com/somepage\") \n",
    "print(\"접근 가능 여부:\", can_fetch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. requests (외부 라이브러리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본요청-get\n",
    "\n",
    "import requests\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "# requests 라이브러리를 불러온다 (웹 요청을 보내기 위한 라이브러리) \n",
    "\n",
    "url = 'https://www.python.org/' \n",
    "# 접속하고자 하는 URL을 정의한다 \n",
    "\n",
    "response = requests.get(url) \n",
    "# 해당 URL로 GET 요청을 보내고, 응답(response) 객체를 저장한다 \n",
    "\n",
    "print(\"상태 코드:\", response.status_code) \n",
    "# 응답의 상태 코드(status code)를 출력한다 \n",
    "# 200 이면 성공, 404는 페이지 없음, 403은 접근 금지 등등 \n",
    "\n",
    "print(\"본문 일부:\\n\", response.text[:300])\n",
    "# 응답의 본문 내용 중 앞부분 300글자만 출력한다 (전체는 너무 길 수 있으므로) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post 요청 보내기 \n",
    "\n",
    "import requests \n",
    "data = {'id': 'bc_kim', 'pw': '1234'} \n",
    "response = requests.post('https://httpbin.org/post', data=data) \n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests.Request \n",
    "from requests import Request, Session \n",
    "s = Session() \n",
    "req = Request('GET', 'https://httpbin.org/get') \n",
    "prepped = s.prepare_request(req) \n",
    "resp = s.send(prepped) \n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests.Session \n",
    "s = requests.Session() \n",
    "s.headers.update({'User-Agent': 'my-app/0.0.1'}) \n",
    "response = s.get('https://httpbin.org/headers') \n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BeautifulSoup (HTML/XML 파서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "# 1. requests 로 웹페이지 요청 \n",
    "response = requests.get(\"https://example.com\") \n",
    "# 2. BeautifulSoup 으로 HTML 파싱 \n",
    "soup = BeautifulSoup(response.text, \"html.parser\") \n",
    "# 3. 원하는 요소 추출 \n",
    "title = soup.find(\"title\").text \n",
    "print(\"웹 페이지 제목:\", title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# 분석할 HTML 문자열을 정의한다 (간단한 테스트용 HTML) \n",
    "html = ''' \n",
    "<html> \n",
    "<head><title>테스트 페이지</title></head> \n",
    "<body> \n",
    "<h1>안녕하세요!</h1> \n",
    "<p class=\"desc\">웹 크롤링 예제입니다.</p> \n",
    "<a href=\"https://example.com\">Example 사이트</a> \n",
    "</body> \n",
    "</html> \n",
    "''' \n",
    "\n",
    "# HTML 문자열을 BeautifulSoup 객체로 파싱한다 \n",
    "# 'html.parser'는 파이썬 기본 HTML 파서 \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    " \n",
    "# <p> 태그의 class 속성 값을 출력한다 \n",
    "print(soup.p['class'])  # 출력: ['desc'] → class 속성은 리스트로 반환됨 \n",
    "\n",
    "# <title> 태그 전체를 출력한다 (태그 포함) \n",
    "print(soup.title)  # 출력: <title>테스트 페이지</title> \n",
    "\n",
    "# <h1> 태그의 텍스트만 출력한다 \n",
    "print(soup.h1.text)  # 출력: 안녕하세요! \n",
    "\n",
    "# <a> 태그의 href 속성 값을 출력한다 (링크 주소) \n",
    "print(soup.a['href'])  # 출력: https://example.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    " \n",
    "# 크롤링할 웹 페이지의 주소 (Hacker News 메인 페이지) \n",
    "url = 'https://news.ycombinator.com' \n",
    " \n",
    "# requests를 이용해 URL에 GET 요청을 보냄 \n",
    "response = requests.get(url) \n",
    " \n",
    "# 받은 응답(response)의 HTML 텍스트를 BeautifulSoup 객체로 파싱함 \n",
    "# 'html.parser'는 기본 파서로, HTML 문서를 분석할 수 있게 함 \n",
    "soup = BeautifulSoup(response.text, 'html.parser') \n",
    " \n",
    "# 'span.titleline > a' 선택자를 이용해 뉴스 제목이 담긴 링크를 모두 선택함 \n",
    "# Hacker News는 각 뉴스 항목을 <span class=\"titleline\"> 내부의 <a> 태그로 감싸고 있음 \n",
    "titles = soup.select('span.titleline > a') \n",
    " \n",
    "# 상위 5개의 뉴스 제목과 링크를 출력 \n",
    "for i, title in enumerate(titles[:5]): \n",
    "  # i+1: 순번 표시, title.text: 뉴스 제목, title['href']: 뉴스 링크 \n",
    "  print(f\"{i+1}. {title.text} → {title['href']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 필요한 모듈 가져오기\n",
    "from bs4 import BeautifulSoup \n",
    " \n",
    "# 2. 예시로 사용할 HTML 코드 준비\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "  <head><title>나의 첫 번째 크롤링</title></head>\n",
    "  <body>\n",
    "    <h1 class=\"title\">Hello, BeautifulSoup!</h1>\n",
    "    <p id=\"first\">첫 번째 문단입니다.</p>\n",
    "    <p id=\"second\">두 번째 문단입니다.</p>\n",
    "    <a href=\"http://example.com\" class=\"link\">Example 링크</a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\" \n",
    "# 3. BeautifulSoup 객체로 HTML 파싱하기 \n",
    "soup = BeautifulSoup(html_doc, 'html.parser') # 'html.parser'는 기본 파서 사용\n",
    "\n",
    "# 4. 다양한 접근 방법들\n",
    "# 4-1. 문서 구조를 트리처럼 따라가기\n",
    "print(\"4-1 타이틀 태그 전체:\", soup.title)\n",
    "print(\"4-1 타이틀 텍스트:\", soup.title.string)\n",
    "\n",
    "# 4-2. 태그 이름으로 바로 접근하기\n",
    "print(\"4-2 h1 태그:\", soup.h1)\n",
    "print(\"4-2 h1 텍스트:\", soup.h1.text)\n",
    "\n",
    "# 4-3. find(): 첫 번째 해당 태그 찾기\n",
    "p_tag = soup.find('p')\n",
    "print(\"4-3 첫 번째 p 태그:\", p_tag)\n",
    "\n",
    "# 4-4. find_all(): 모든 해당 태그 리스트로 반환\n",
    "p_tags = soup.find_all('p')\n",
    "print(\"4-4 모든 p태그 리스트:\")\n",
    "for p in p_tags:\n",
    "  print(\"-\", p.text)\n",
    "  \n",
    "# 4-5. 속성(attribute) 조회하기\n",
    "a_tag = soup.find('a')\n",
    "print(\"4-5 a 태그 href 속성:\", a_tag['href'])\n",
    "print(\"4-5 a 태그 class 속성:\", a_tag['class'])\n",
    "\n",
    "# 4-6. select(): css 선택자 방식으로 찾기\n",
    "title_class = soup.select_one('h1.title')\n",
    "print(\"4-6 css 선택자로 찾은 h1:\", title_class.text)\n",
    "\n",
    "# 4-7. select.all(): css 선택자 방식으로 여러 개 찾기\n",
    "all_p = soup.select('p')\n",
    "print(\"4-7 css 선택자로 찾은 모든 p 태그:\")\n",
    "for p in all_p:\n",
    "  print(\"-\", p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개 parser\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    " \n",
    "html_doc = \"\"\" \n",
    "<html> \n",
    "  <head><title>테스트 페이지</title></head> \n",
    "  <body> \n",
    "    <h1>Welcome</h1> \n",
    "    <p>첫 번째 문단</p> \n",
    "    <p>두 번째 문단</p> \n",
    "  </body> \n",
    "</html> \n",
    "\"\"\"\n",
    "\n",
    "# html.parser 사용\n",
    "soup1 = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(\"1. html.parser 사용 결과:\")\n",
    "print(soup1.prettify()) # 들여쓰기 정리된 예쁜 형태 출력\n",
    "\n",
    "# lxml 사용\n",
    "soup2 = BeautifulSoup(html_doc, 'lxml')\n",
    "print(\"\\nlxml 사용 결과:\")\n",
    "print(soup2.prettify())\n",
    "\n",
    "# html5lib 사용\n",
    "soup3 = BeautifulSoup(html_doc, 'html5lib')\n",
    "print(\"\\nhtml5lib 사용 결과:\")\n",
    "print(soup3.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautifulSoup 고급 기능\n",
    "# 기능 / 메소드 / 설명\n",
    "\n",
    "# 1. 형제찾기 /\n",
    "# find_next_sibling(), find_previous_sibling() /\n",
    "# 다음or이전 형제 태그 찾기, \n",
    "# ex) tag.find_next_sibling()\n",
    "\n",
    "# 2. 부모 찾기 /\n",
    "# find_parent(), find_parents() /\n",
    "# 부모 태그 또는 모든 상위 태그 찾기, \n",
    "# ex) tag.find_parent()\n",
    "\n",
    "# 3. 조건 검색 /\n",
    "# find() + attrs, find(attrs={}) /\n",
    "# 속성 기반 검색, \n",
    "# ex) soup.find('tag', class_=\"name\")\n",
    "\n",
    "# 4. 텍스트 검색 /\n",
    "# find(text=\"텍스트\") /\n",
    "# ex) soup.find(text=\"Welcome\")\n",
    "\n",
    "# 5. 여러 조건 검색 /\n",
    "# find_all + 리스트, find_all(['tag1', 'tag2']) /\n",
    "# 여러 태그를 동시에 찾기, \n",
    "# ex) soup.find_all(['h1', 'p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 고급 기능: 형제(sibling) 찾기 \n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "html_sibling = \"\"\" \n",
    "<div> \n",
    "  <p id=\"first\">첫 번째</p> \n",
    "  <p id=\"second\">두 번째</p> \n",
    "  <p id=\"third\">세 번째</p> \n",
    "</div> \n",
    "\"\"\" \n",
    " \n",
    "soup = BeautifulSoup(html_sibling, 'html.parser') \n",
    " \n",
    "first_p = soup.find('p', id='first')\n",
    "print(\"현재 태그:\", first_p.text) \n",
    " \n",
    "next_p = first_p.find_next_sibling('p') # 다음 형제 찾기 \n",
    "print(\"다음 형제 태그:\", next_p.text) \n",
    " \n",
    "third_p = next_p.find_next_sibling('p') \n",
    "print(\"또 다음 형제 태그:\", third_p.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 고급 기능: 부모(parent) 찾기 \n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "html_parent = \"\"\" \n",
    "<div class=\"container\"> \n",
    "  <p>문단 안에 <span>스팬 텍스트</span>가 있어요.</p> \n",
    "</div> \n",
    "\"\"\" \n",
    " \n",
    "soup = BeautifulSoup(html_parent, 'html.parser') \n",
    " \n",
    "span_tag = soup.find('span') \n",
    "print(\"현재 span 태그 텍스트:\", span_tag.text) \n",
    " \n",
    "parent_p = span_tag.find_parent('p') \n",
    "print(\"span의 부모 p 태그:\", parent_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 고급 기능: 속성으로 찾기\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "html_attr = \"\"\" \n",
    "<div> \n",
    "  <a href=\"https://example.com\" class=\"btn primary\">Example</a> \n",
    "  <a href=\"https://another.com\" class=\"btn secondary\">Another</a> \n",
    "</div> \n",
    "\"\"\" \n",
    " \n",
    "soup = BeautifulSoup(html_attr, 'html.parser') \n",
    " \n",
    "# class=\"btn primary\"인 a 태그 찾기 \n",
    "primary_btn = soup.find('a', class_=\"btn primary\") \n",
    "print(\"Primary 버튼 텍스트:\", primary_btn.text) \n",
    " \n",
    "# href 속성이 https로 시작하는 a 태그 찾기 \n",
    "https_links = soup.find_all('a', href=True) \n",
    "for link in https_links: \n",
    "    print(\"링크:\", link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web crawling (기사 링크 수집) 예\n",
    "\n",
    "# 웹 크롤링을 위한 모듈\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 크롤링할 웹사이트\n",
    "url = \"https://news.ycombinator.com/\"\n",
    "\n",
    "# 해당 url에 get 요청을 보내고, 값을 받아옴\n",
    "# 그 값에는 HTML 전체가 텍스트 형태로 담겨 있음\n",
    "response = requests.get(url)\n",
    "\n",
    "# HTML 텍스트를 BeautifulSoup으로 파싱하여 분석할 수 있는 구조로 바꿈\n",
    "# 'html.parser'는 기본 제공되는 HTML 파서로 갇난한 HTML 문서를 분석할 때 사용\n",
    "# lxml, html5lib 라는 이름의 성능적으로 더 뛰어는 parser도 존재\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 뉴스 기사 제목이 포함된 링크를 추출\n",
    "links = soup.select('span.titleline > a')\n",
    "\n",
    "# 수집한 뉴스 링크들 중 하나씩 꺼내어 제목과 링크를 출력\n",
    "for link in links:\n",
    "  # link.text: 링크(a태그) 안의 텍스트(기사 제목)\n",
    "  # link['href']: 링크(a속성)의 실제 URL\n",
    "  # strip(): 문자열 앞뒤 공백 제거 메서드\n",
    "  print(link.text.strip(), \"==>\", link['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping (기사 상세 정보 추출) 예\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 실제 뉴스 기사 url (연합뉴스)\n",
    "article_url = \"https://www.yna.co.kr/view/AKR20240419039400017\"\n",
    "\n",
    "# 크롤링 차단 방지를 위해 브라우저처럼 보이게끔 User-Agent 설정\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 해당 url에 get 요청을 보내고, 값을 받아옴\n",
    "response = requests.get(article_url, headers=headers)\n",
    "\n",
    "# 응답 받은 HTML 을 BeautifulSoup로 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 1. 제목 추출\n",
    "# h1 tag + class 'tit'에서 텍스트 추출\n",
    "title_tag = soup.find('h1', class_='tit')\n",
    "# 존재하면 텍스트 추출, 없으면 '제목 없음' 처리\n",
    "title = title_tag.text.strip() if title_tag else \"제목 없음\"\n",
    "\n",
    "# 2. 날짜 추출\n",
    "# <p class=\"update-time\">에서 텍스트 추출\n",
    "date_tag = soup.find('p', class_='update-time')\n",
    "# 존재하면 텍스트 추출, 없으면 '날짜 없음' 처리\n",
    "date = date_tag.text.strip() if date_tag else \"날짜 없음\"\n",
    "\n",
    "# 3. 본문 추출\n",
    "# <div class=\"story-news article\">에서 텍스트 추출\n",
    "content_tag = soup.find('div', class_='story-news article')\n",
    "# 여러 줄의 텍스트가 있으므로 전체 문자열 추출\n",
    "body = content_tag.text.strip() if content_tag else \"본문 없음\"\n",
    "\n",
    "# 4. 결과 출력\n",
    "print(\"제목:\", title)\n",
    "print(\"날짜:\", date)\n",
    "print(\"내용:\\n\", body[:500], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
