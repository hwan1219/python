{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "'''\n",
    "- 블로그 검색기 (CSV 저장 포함)\n",
    "네이버 블로그 검색 API 로 키워드에 해당하는 블로그 글 정보를 가져오기\n",
    "검색 결과를 CSV 로 저장\n",
    "'''\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "client_id = \"TkWr5Zx1CvlRddtpzNZA\"\n",
    "client_secret = \"n9uK6nHknt\"\n",
    "\n",
    "query = \"전기자전거\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/blog.json\"\n",
    "\n",
    "params = {\n",
    "  'query': query,\n",
    "  'display': 5\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  \"X-Naver-Client-Id\": client_id,\n",
    "  \"X-Naver-Client-Secret\": client_secret\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  result = response.json()\n",
    "  items = result['items']\n",
    "  \n",
    "  if not os.path.exists('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "  save_path = \"dataset/03_Q1.csv\"\n",
    "  with open(save_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['title', 'link', 'description', 'bloggername', 'bloggerlink', 'postdate'])\n",
    "    for item in items:\n",
    "      clean_title = re.sub('<.*?>', '', item['title'])\n",
    "      clean_description = re.sub('<.*?>', '', item['description'])\n",
    "      writer.writerow([\n",
    "        clean_title,\n",
    "        item['link'],\n",
    "        clean_description,\n",
    "        item['bloggername'],\n",
    "        item['bloggerlink'],\n",
    "        item['postdate']\n",
    "      ])\n",
    "      \n",
    "  for i, item in enumerate(items, 1):\n",
    "    clean_title = re.sub('<.*?>', '', item['title'])\n",
    "    link = item['link']\n",
    "    \n",
    "    print(f\"{str(i)+'.':<3}제목: {clean_title}\")\n",
    "    print(f\"{'':<3}링크: {link}\\n\")\n",
    "else:\n",
    "    print(\"Error Code:\" + response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "'''\n",
    "- 뉴스 검색기\n",
    "사용자가 입력한 키워드로 뉴스 기사 검색 후 제목, 링크 출력 및 CSV 저장\n",
    "'''\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "client_id = \"TkWr5Zx1CvlRddtpzNZA\"\n",
    "client_secret = \"n9uK6nHknt\"\n",
    "\n",
    "query = \"자연어 처리\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/news.json\"\n",
    "\n",
    "params = {\n",
    "  'query': query,\n",
    "  'display': 5\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  \"X-Naver-Client-Id\": client_id,\n",
    "  \"X-Naver-Client-Secret\": client_secret\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  result = response.json()\n",
    "  items = result['items']\n",
    "  \n",
    "  if not os.path.exists('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "  save_path = \"dataset/03_Q2.csv\"\n",
    "  with open(save_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['title', 'originallink', 'link', 'description', 'pubDate'])\n",
    "    for item in items:\n",
    "      clean_title = re.sub('<.*?>', '', item['title'])\n",
    "      clean_description = re.sub('<.*?>', '', item['description'])\n",
    "      writer.writerow([\n",
    "        clean_title,\n",
    "        item['originallink'],\n",
    "        item['link'],\n",
    "        clean_description,\n",
    "        item['pubDate']\n",
    "      ])\n",
    "      \n",
    "  for i, item in enumerate(items, 1):\n",
    "    clean_title = re.sub('<.*?>', '', item['title'])\n",
    "    link = item['link']\n",
    "    \n",
    "    print(f\"{str(i)+'.':<3}제목: {clean_title}\")\n",
    "    print(f\"{'':<3}링크: {link}\\n\")\n",
    "else:\n",
    "    print(\"Error Code:\" + response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "'''\n",
    "- 책 정보 검색기\n",
    "사용자가 입력한 책 제목으로 네이버 책 검색\n",
    "제목, 저자, 가격, 링크 출력 및 CSV 저장장\n",
    "'''\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "client_id = \"TkWr5Zx1CvlRddtpzNZA\"\n",
    "client_secret = \"n9uK6nHknt\"\n",
    "\n",
    "query = \"전기자전거\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/book.json\"\n",
    "\n",
    "params = {\n",
    "  'query': query,\n",
    "  'display': 5\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  \"X-Naver-Client-Id\": client_id,\n",
    "  \"X-Naver-Client-Secret\": client_secret\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  result = response.json()\n",
    "  items = result['items']\n",
    "  \n",
    "  if not os.path.exists('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "  save_path = \"dataset/03_Q3.csv\"\n",
    "  with open(save_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['title', 'link', 'image', 'author', 'discount', 'publisher', 'isbn', 'description', 'pubDate'])\n",
    "    for item in items:\n",
    "      clean_title = re.sub('<.*?>', '', item['title'])\n",
    "      clean_description = re.sub('<.*?>', '', item['description'])\n",
    "      writer.writerow([\n",
    "        clean_title,\n",
    "        item['link'],\n",
    "        item['image'],\n",
    "        item['author'],\n",
    "        item['discount'],\n",
    "        item['publisher'],\n",
    "        item['isbn'],\n",
    "        clean_description,\n",
    "        item['pubdate']\n",
    "      ])\n",
    "      \n",
    "  for i, item in enumerate(items, 1):\n",
    "    clean_title = re.sub('<.*?>', '', item['title'])\n",
    "    author = item['author']\n",
    "    discount = item.get('discount', '가격 정보 없음')\n",
    "    if discount == '0' or discount == 0:\n",
    "      discount = '가격 정보 없음'\n",
    "    elif discount != '가격 정보 없음':\n",
    "      discount = str(discount) + '원'\n",
    "    link = item['link']\n",
    "    \n",
    "    print(f\"{str(i)+'.':<3}제목: {clean_title}\")\n",
    "    print(f\"{'':<3}저자: {author}\")\n",
    "    print(f\"{'':<3}가격: {discount}\")\n",
    "    print(f\"{'':<3}링크: {link}\\n\")\n",
    "else:\n",
    "    print(\"Error Code:\" + response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "'''\n",
    "- 쇼핑 검색기\n",
    "사용자가 입력한 상품명으로 네이버 쇼핑 검색\n",
    "제목, 가격, 쇼핑몰명, 링크 출력 및 CSV 저장\n",
    "'''\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "client_id = \"TkWr5Zx1CvlRddtpzNZA\"\n",
    "client_secret = \"n9uK6nHknt\"\n",
    "\n",
    "query = \"로봇 청소기\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/shop.json\"\n",
    "\n",
    "params = {\n",
    "  'query': query,\n",
    "  'display': 5\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  \"X-Naver-Client-Id\": client_id,\n",
    "  \"X-Naver-Client-Secret\": client_secret\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  result = response.json()\n",
    "  items = result['items']\n",
    "  \n",
    "  if not os.path.exists('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "  save_path = \"dataset/03_Q4.csv\"\n",
    "  with open(save_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['title', 'lprice', 'mallName', 'link'])\n",
    "    for item in items:\n",
    "      clean_title = re.sub('<.*?>', '', item['title'])\n",
    "      writer.writerow([\n",
    "        clean_title,\n",
    "        item['lprice'],\n",
    "        item['mallName'],\n",
    "        item['link']\n",
    "      ])\n",
    "      \n",
    "  for i, item in enumerate(items, 1):\n",
    "    clean_title = re.sub('<.*?>', '', item['title'])\n",
    "    lprice = item['lprice']\n",
    "    mallName = item['mallName']\n",
    "    link = item['link']\n",
    "    \n",
    "    print(f\"{str(i)+'.':<3}상품명: {clean_title}\")\n",
    "    print(f\"{'':<3}가격: {lprice}\")\n",
    "    print(f\"{'':<3}쇼핑몰명: {mallName}\")\n",
    "    print(f\"{'':<3}링크: {link}\\n\")\n",
    "else:\n",
    "    print(\"Error Code:\" + response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "'''\n",
    "- Blog 검색기 시각화 (WordCloud)\n",
    "네이버 블로그 검색 결과 제목을 워드클라우드로 시각화\n",
    "'''\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "client_id = \"TkWr5Zx1CvlRddtpzNZA\"\n",
    "client_secret = \"n9uK6nHknt\"\n",
    "\n",
    "query = \"인공지능\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/blog.json\"\n",
    "\n",
    "params = {\n",
    "  'query': query,\n",
    "  'display': 20\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  \"X-Naver-Client-Id\": client_id,\n",
    "  \"X-Naver-Client-Secret\": client_secret\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  result = response.json()\n",
    "  items = result['items']\n",
    "  \n",
    "  if not os.path.exists('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "  save_path = \"dataset/03_Q5.csv\"\n",
    "  with open(save_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['title', 'link'])\n",
    "    for item in items:\n",
    "      clean_title = re.sub('<.*?>', '', item['title'])\n",
    "      writer.writerow([\n",
    "        clean_title,\n",
    "        item['link']\n",
    "      ])\n",
    "      \n",
    "  for i, item in enumerate(items, 1):\n",
    "    clean_title = re.sub('<.*?>', '', item['title'])\n",
    "    link = item['link']\n",
    "    \n",
    "    print(f\"{str(i)+'.':<3}제목: {clean_title}\")\n",
    "    print(f\"{'':<3}링크: {link}\\n\")\n",
    "  \n",
    "  all_titles = \" \".join(re.sub('<.*?>', '', item['title']) for item in items)\n",
    "  \n",
    "  wordcloud = WordCloud(\n",
    "    font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "    background_color='white',\n",
    "    width=1000,\n",
    "    height=500\n",
    "  ).generate(all_titles)\n",
    "  \n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis('off')\n",
    "  plt.title(f\"'{query}' Blog WordCloud\")\n",
    "  plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error Code:\" + response.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
