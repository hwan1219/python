{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install xgboost\n",
    "!python -m pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "# 그래프 한글 깨짐 방지\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 버전 확인\n",
    "print('numpy:', np.__version__)\n",
    "print('pandas:', pd.__version__)\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "print('matplotlib:', mpl.__version__)\n",
    "print('seaborn:', sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3 과제 (머신러닝 이해도 점검)\n",
    "\n",
    "# 1) 머신러닝이 사용되는 실생활 예시 3가지 작성\n",
    "\n",
    "# 2) 파이썬으로 간단한 데이터 생성 및 평균 구하기\n",
    "#    예시 데이터: 학생들의 점수(78, 85, 90, 95, 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝이 사용되는 실생활 예시 3가지\n",
    "\n",
    "# 1. 유튜브 추천 영상 (콘텐츠 기반 추천)\n",
    "# 2. 이메일 스팸 필터링 (분류)\n",
    "# 3. 자율주행차의 도로 인식 (딥러닝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬으로 간단한 데이터 생성 및 평균 구하기\n",
    "# 예시 데이터: 학생들의 점수(78, 85, 90, 95, 88)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "score = np.array([78, 85, 90, 95, 88])\n",
    "score_avg = score.mean()\n",
    "\n",
    "print(score)\n",
    "print(score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 지도학습(Supervised) vs 비지도학습(Unsupervised)\n",
    "\n",
    "# 지도학습 (Supervised)\n",
    "# 입력과 출력이 있음 / x(특징) → y(정답)으로 학습\n",
    "# 예측,분류 목적 / 새로운 예측 가능\n",
    "# 예시 알고리즘 / 선형회귀, 로지스틱 회귀, 결정트리, SVM\n",
    "\n",
    "# 비지도학습 (Unsupervised)\n",
    "# 출력(정답)이 없음 / 데이터 구조나 패턴을 스스로 학습\n",
    "# 군집화, 차원 축소 / 비슷한 성질을 가진 데이터끼리 묶음\n",
    "# 예시 알고리즘 / K=평균, 계층적 군집, PCA\n",
    "\n",
    "# 왜 \"K=평균\"이라고 부를까? / KMeans 모듈을 말하는 건가?\n",
    "# 군집의 중심점(centroid, 중심좌표)을 계속 계산하면서, 각 데이터를 가장 가까운 군집 중심으로 배정하고, 다시 중심을 평균으로 갱신하면서 반복\n",
    "# 그래서 K개의 군집을 만들고, 각 군집의 중심(평균)을 찾기 때문에 K-평균이라는 이름이 붙음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2 실습\n",
    "# iris 데이터로 분류와 군집 비교\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 지도학습 - LogisticRegression (분류)\n",
    "logistic = LogisticRegression(max_iter=200)\n",
    "logistic.fit(x_train, y_train)\n",
    "logistic_pred = logistic.predict(x_test)\n",
    "\n",
    "# 비지도학습 - KMeans (군집)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_pred = kmeans.fit_predict(x_test)\n",
    "\n",
    "# 시각화를 위해 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_test)\n",
    "\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=x_pca[:,0], y=x_pca[:,1], hue=logistic_pred, palette='Set1')\n",
    "plt.title('지도학습: 로지스틱 회귀 결과')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=x_pca[:,0], y=x_pca[:,1], hue=kmeans_pred, palette='Set1')\n",
    "plt.title('비지도학습: KMeans 결과')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3 과제\n",
    "\n",
    "# 1) 지도/비지도 구분 실습\n",
    "# 스팸 메일 감지 = ?\n",
    "# 상품 추천 = ?\n",
    "# 뉴스 클러스터링 = ?\n",
    "# 날씨에 따른 우산 판매 예측 = ?\n",
    "\n",
    "# 2) Wine 데이터로 KMeans 군집화 실습\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 데이터 로드\n",
    "wine = load_wine()\n",
    "x = wine.data\n",
    "print(wine.feature_names)\n",
    "print(wine.target_names)\n",
    "print(wine.DESCR)\n",
    "\n",
    "# KMeans 군집화\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(x)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"군집 레이블:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# 3~4 사이 안함\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2 실습\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예제 데이터\n",
    "# y = ax + b\n",
    "# y: 종속변수, x: 독립변수(설명)\n",
    "# a: weight(가중치, 기울기), b: bias(절편, 오차)\n",
    "x = np.array([1, 2, 3, 4, 5]).reshape(-1,1)\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# 모델 훈련\n",
    "model = LinearRegression()\n",
    "model.fit(x,y) # 훈련(학습)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# 회귀계수\n",
    "print(f\"기울기 a: {model.coef_[0]:.2f}\") # .coef_\n",
    "print(f\"절편 b: {model.intercept_:.2f}\") # .intercept_\n",
    "\n",
    "# 평가\n",
    "print(f\"MSE(Mean Squared Error): {mean_squared_error(y, y_pred):.2f}\") # mean_squared_error()\n",
    "print(f\"R^2: {r2_score(y, y_pred):.2f}\") # r2_score()\n",
    "\n",
    "# 시각화\n",
    "plt.scatter(x, y, color='blue', label='실제값')\n",
    "plt.plot(x, y_pred, color='red', label='예측값')\n",
    "plt.title('산형 회귀 분석')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-3 실습 (선형 회귀 width 켈리포니아 집값)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "display(df.head())\n",
    "\n",
    "df['MedHouseVal'] = data.target\n",
    "display(df['MedHouseVal'].head())\n",
    "\n",
    "# 간단한 1개 feature 사용:\n",
    "# 중간 소득(MedInc) → 집값 예측\n",
    "x = df[['MedInc']]\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# 평가\n",
    "mse = mean_squared_error(y_test, y_pred) \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# 시각화\n",
    "plt.scatter(x_test, y_test, label='실제값')\n",
    "plt.plot(x_test, y_pred, color='red', label='예측값')\n",
    "plt.title('선형 회귀 예측')\n",
    "plt.xlabel('평균 속득 (MedInc)')\n",
    "plt.ylabel('집값 (MedHouseVal)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-4\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "x = np.array([10, 20, 30, 40, 50]).reshape(-1,1)\n",
    "y = np.array([15, 30, 45, 40, 60])\n",
    "\n",
    "# 모델 훈련\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# 회귀계수\n",
    "print(f\"기울기 a: {model.coef_[0]:.2f}\")\n",
    "print(f\"절편 b: {model.intercept_:.2f}\")\n",
    "\n",
    "# 평가\n",
    "print(f\"MSE(Mean Squared Error): {mean_squared_error(y, y_pred):.2f}\")\n",
    "print(f\"R^2: {r2_score(y, y_pred):.2f}\")\n",
    "\n",
    "# 시각화\n",
    "plt.scatter(x, y, color='blue', label='실제값')\n",
    "plt.plot(x, y_pred, color='red', label='예측값')\n",
    "plt.title('산형 회귀 분석')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-5\n",
    "# 여러 feature 로 예측 (다중 선형 회귀)\n",
    "# MedInc - 중간 소득\n",
    "# AveRooms - 평균 방 개수\n",
    "# HouseAge - 집의 나이(즉, 지어진 지 얼마나 되었는지)\n",
    "# → 집값 예측\n",
    "# y = ax + bx + cx + d\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['MedHouseVal'] = data.target\n",
    "\n",
    "x = df[['MedInc', 'AveRooms', 'HouseAge']] # 독립변수\n",
    "y = df['MedHouseVal'] # 종속변수\n",
    "\n",
    "# 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(x_train, y_train)\n",
    "\n",
    "# 평가\n",
    "y_pred_multi = model_multi.predict(x_test)\n",
    "print(\"다중 회귀 MSE:\", mean_squared_error(y_test, y_pred_multi))\n",
    "print(\"다중 회귀 R^2:\", r2_score(y_test, y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02_machineLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위 회귀(make_regression)\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "x, y = make_regression(n_samples=5, n_features=2, noise=1, random_state=42)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 실습\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1) 회귀 평가: 예제 코드 (Linear Regression)\n",
    "'''\n",
    "y = ax + b\n",
    "데이터 생성: (\n",
    "  샘플갯수: 100,\n",
    "  특징: 1,\n",
    "  노이즈: 20,\n",
    "  random_state: 42\n",
    ")\n",
    "'''\n",
    "x, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 예측\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# 평가\n",
    "print(f\"MSE: {mean_squared_error(y_test, pred)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3 회귀 평가 과제\n",
    "\n",
    "# diabetes 데이터를 사용하여 선형회귀 모델을 만들고, MSE, MAE, R&2 를 평가\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def main():\n",
    "  print(\"Diabetes 데이터 기반 선형 회귀 모델 평가\")\n",
    "  \n",
    "  # 데이터 로드 및 분할\n",
    "  x, y = load_diabetes(return_X_y=True)\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  # 모델 학습\n",
    "  model = LinearRegression()\n",
    "  model.fit(x_train, y_train)\n",
    "\n",
    "  # 예측\n",
    "  y_pred = model.predict(x_test)\n",
    "\n",
    "  # 평가\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  \n",
    "  # 결과 출력\n",
    "  print(f\"MSE (Mean Squared Error): {mse:.2f}\")\n",
    "  print(f\"MAE (Mean Absolute Error): {mae:.2f}\")\n",
    "  print(f\"R^2 (R_squared): {r2:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3 과제\n",
    "# titanic 데이터를 불러와 전처리 후 생존 여부를 분류\n",
    "# accuracy, precision, recall, F1-score 를 평가\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "  try:\n",
    "    # seaborn 없이 직접 데이터 불러오기 (csv 경로가 필요하면 교체)\n",
    "    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # 필요한 열 선택 및 결측치 제거\n",
    "    df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # 범주형 변수 인코딩\n",
    "    df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "    df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
    "    \n",
    "    return df\n",
    "  \n",
    "  except Exception as e:\n",
    "    print(\"데이터 로드 또는 전처리 중 오류 발생:\", e)\n",
    "    return None\n",
    "    \n",
    "def train_and_evaluate(df):\n",
    "  x = df.drop('Survived', axis=1)\n",
    "  y = df['Survived']\n",
    "  \n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "  \n",
    "  model = RandomForestClassifier(random_state=42)\n",
    "  model.fit(x_train, y_train)\n",
    "  y_pred = model.predict(x_test)\n",
    "  \n",
    "  print(\"모델 평과 결과\")\n",
    "  print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "  print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "  print(f\"Recall: {recall_score(y_test, y_pred):.2f}\")\n",
    "  print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")\n",
    "  \n",
    "def main():\n",
    "  print(\"Titanic 생존자 분류 모델 평가\")\n",
    "  df = load_and_preprocess_data()\n",
    "  if df is not None:\n",
    "    train_and_evaluate(df)\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_machineLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-2 Logistic Regression (로지스틱 회귀) 실습\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# 필요한 컬럼 선택 및 결측치 제거\n",
    "df = df[['sex', 'age', 'fare', 'class', 'embarked', 'survived']].dropna()\n",
    "\n",
    "# 범주형 인코딩\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df = pd.get_dummies(df, columns=['class', 'embarked'], drop_first=True)\n",
    "\n",
    "# 입력(x), 출력(y)\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# 정확도 평가\n",
    "print(f\"정확도: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"혼돈 행렬:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"분류 리포트:\\n{classification_report(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 시그모이드 함수\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import numpy as np\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "plt.plot(z, sigmoid)\n",
    "plt.title('시그모이드 함수')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Sigmoid(z)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-3 과제\n",
    "\n",
    "# 1. 임계값 조정해서 분류 성능 비교\n",
    "# 2. 사용자 정의 예측 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임계값 조정해서 분류 성능 비교\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# 필요한 컬럼 선택 및 결측치 제거\n",
    "df = df[['sex', 'age', 'fare', 'class', 'embarked', 'survived']].dropna()\n",
    "\n",
    "# 범주형 인코딩\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df = pd.get_dummies(df, columns=['class', 'embarked'], drop_first=True)\n",
    "\n",
    "# 입력(x), 출력(y)\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(x_test) # 예측\n",
    "\n",
    "y_proba = model.predict_proba(x_test)[:,1]\n",
    "y_pred_threshold = (y_proba >= 0.4).astype(int)\n",
    "# predict_proba를 통해 x_test 각 행의 0(사망), 1(생존) 확률을 구하고 1에 해당하는 값을 y_proba에 할당\n",
    "# 해당 y_proba에 할당된 값을 이용해 기존 임계값 0.5 이상일 때 1인 로직에서 임계값을 0.4로 바꿈으로 인해 0.4 이상일 때 1로 간주함 \n",
    "\n",
    "print(f\"0.5(기본값) 정확도: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"0.5(기본값) 혼돈 행렬:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"0.5(기본값) 분류 리포트:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# 임계값 0.4 적용 정확도 평가\n",
    "print(f\"0.4 적용 정확도: {accuracy_score(y_test, y_pred_threshold)}\")\n",
    "print(f\"0.4 적용 혼돈 행렬:\\n{confusion_matrix(y_test, y_pred_threshold)}\")\n",
    "print(f\"0.4 적용 분류 리포트:\\n{classification_report(y_test, y_pred_threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 예측 함수 만들기\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# 필요한 컬럼 선택 및 결측치 제거\n",
    "df = df[['sex', 'age', 'fare', 'class', 'embarked', 'survived']].dropna()\n",
    "\n",
    "# 범주형 인코딩\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df = pd.get_dummies(df, columns=['class', 'embarked'], drop_first=True)\n",
    "\n",
    "# 입력(x), 출력(y)\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "def predict_survival(sex: str, age: float, fare: float) -> str:\n",
    "  sex_val = 1 if sex == 'male' else 0\n",
    "  \n",
    "  # 입력값을 DataFrame 으로 변환 (컬럼 이름은 학습할 때 사용한 순서와 동일해야 함)\n",
    "  input_df = pd.DataFrame(\n",
    "    [[sex_val, age, fare, 0, 0, 0, 1]],\n",
    "    columns = [\n",
    "      'sex', 'age', 'fare', 'class_Second', 'class_Third', 'embarked_Q', 'embarked_S'\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  prob1 = model.predict_proba(input_df)\n",
    "  print(prob1)\n",
    "  # input_df는 사실상 x_test의 역할을 하고 있음\n",
    "  # prob1 과 prob2 의 차이는 1열(사망=0) 값을 포함했느냐 안했느냐 뿐\n",
    "  # 실질적인 코드 진행은 prob2 가 담당하고 있음\n",
    "  prob2 = model.predict_proba(input_df)[0][1]\n",
    "  print(prob2)\n",
    "  \n",
    "  result = \"생존\" if prob2 >= 0.5 else \"사망\"\n",
    "  return f\"예측 결과: {result}, 생존확률: {prob2:.2f}\\n\"\n",
    "\n",
    "# test\n",
    "print(predict_survival(\"female\", 30, 100))\n",
    "print(predict_survival(\"male\", 60, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. KNN (K-최근접 이웃)\n",
    "# 새로운 데이터를 주변의 K개의 이웃과 비교하여 분류하는 알고리즘\n",
    "# 학습 자체는 없고, 예측 시 거리 계산을 통해 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-2 실습\n",
    "\n",
    "# Iris 데이터 기반 KNN 분류\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1. 데이터 로드\n",
    "iris = load_iris()\n",
    "print(f\"iris.target_names: {iris.target_names}\")\n",
    "# print(iris.target)\n",
    "print(f\"iris.feature_names: {iris.feature_names}\")\n",
    "# print(iris.data)\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "print(f\"iris.data.shape: {x.shape}\")\n",
    "print(f\"iris.target.shape: {y.shape}\")\n",
    "# print(f\"iris.DESCR: {iris.DESCR}\") # info 와 같음\n",
    "\n",
    "# 2. 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. KNN 모델 학습 (K=3)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# 4. 예측 및 평가\n",
    "y_pred = knn.predict(x_test)\n",
    "print(\"\\n정확도:\", accuracy_score(y_test, y_pred))\n",
    "print(f\"\\n혼동 행렬:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"\\n분류 리포트:\\n{classification_report(y_test, y_pred, target_names=iris.target_names)}\")\n",
    "\n",
    "# K 값 변화에 따른 정확도 시각화\n",
    "k_range = range(1, 21)\n",
    "accuracies = []\n",
    "for k in k_range:\n",
    "  model = KNeighborsClassifier(n_neighbors=k)\n",
    "  model.fit(x_train, y_train)\n",
    "  acc = accuracy_score(y_test, model.predict(x_test))\n",
    "  accuracies.append(acc)\n",
    "  \n",
    "plt.plot(k_range, accuracies, marker='o')\n",
    "plt.title('K 값에 따른 정확도 변화')\n",
    "plt.xlabel('K 값')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-3 과제\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# 새로운 샘플 데이터 분류\n",
    "new_sample = [[5.1, 3.5, 1.4, 0.2]]\n",
    "predicted = knn.predict(new_sample)\n",
    "print(predicted)\n",
    "print(f\"예측된 품종:\\n{iris.target_names[predicted[0]]}\\n\")\n",
    "\n",
    "# K 값별 F1-score 비교\n",
    "for k in [1,3,5,7,9]:\n",
    "  model = KNeighborsClassifier(n_neighbors=k)\n",
    "  model.fit(x_train, y_train)\n",
    "  pred = model.predict(x_test)\n",
    "  f1 = f1_score(y_test, pred, average='macro')\n",
    "  print(f\"K={k} → F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Decision Tree (의사결정 나무)\n",
    "\n",
    "# 데이터를 조건문(if-else)처럼 분기해가며 예측하는 모델\n",
    "# 결과가 트리 구조로 표현되어 해석력이 높음\n",
    "# 회귀와 분류 모두 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-2 실습\n",
    "# Titanic 데이터 의사결정나무\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df[['sex', 'age', 'fare', 'class', 'embarked', 'survived']].dropna()\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df = pd.get_dummies(df, columns=['class', 'embarked'], drop_first=True)\n",
    "\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 의사결정나무 모델 학습\n",
    "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "pred = model.predict(x_test)\n",
    "print(f\"정확도: {accuracy_score(y_test, pred)}\")\n",
    "print(f\"\\n혼돈 행렬:\\n{confusion_matrix(y_test, pred)}\")\n",
    "print(f\"\\n분류 리포트:\\n{classification_report(y_test, pred)}\")\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(\n",
    "  model,\n",
    "  feature_names=x.columns,\n",
    "  class_names=['Died', 'Survived'],\n",
    "  filled=True,\n",
    "  rounded=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-3 과제\n",
    "\n",
    "# max_depth 에 따른 정확도 변화 확인\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df[['sex', 'age', 'fare', 'class', 'embarked', 'survived']].dropna()\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df = pd.get_dummies(df, columns=['class', 'embarked'], drop_first=True)\n",
    "\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# K 값 변화에 따른 정확도 시각화\n",
    "depth_list = range(1, 11)\n",
    "accuracies = []\n",
    "for depth in depth_list:\n",
    "  model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "  model.fit(x_train, y_train)\n",
    "  acc = accuracy_score(y_test, model.predict(x_test))\n",
    "  accuracies.append(acc)\n",
    "  \n",
    "plt.plot(depth_list, accuracies, marker='o')\n",
    "plt.title('max_depth에 따른 정확도 변화')\n",
    "plt.xlabel('max_depth 값')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 트리 중요도 출력\n",
    "importances = model.feature_importances_\n",
    "print(\"특성 중요도 출력\")\n",
    "for feature, importance in zip(x.columns, importances):\n",
    "  print(f\"{feature}: {importance:.3f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. RandomForest (랜덤 포레스트)\n",
    "\n",
    "# 여러 개의 결정 트리를 학습시키고, 결과를 투표(분류) 또는 평균(회귀)으로 결정하는 앙상블 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-2 실습\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# titanic 데이터 랜덤 포레스트 분류\n",
    "\n",
    "# 1. 데이터 로드\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# 2. 필요한 컬럼만 추출하고 결측치 제거\n",
    "df = df[['age', 'fare', 'sex', 'embarked', 'class', 'survived']].dropna()\n",
    "\n",
    "# 3. 범주형 인코딩\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df = pd.get_dummies(df, columns=['embarked', 'class'], drop_first=True)\n",
    "\n",
    "# 4. 입력(x), 출력(y) 지정\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "# 5. 학습/테스트 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 6. 모델 학습\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 7. 예측 및 평가\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(f\"정확도: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"\\n혼돈 행렬:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"\\n분류 리포트:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# 시각화\n",
    "# 트리 중요도 출력\n",
    "importances = model.feature_importances_\n",
    "feature_names = x.columns\n",
    "sns.barplot(x=importances, y=feature_names)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# importances = model.feature_importances_\n",
    "# print(importances)\n",
    "# plt.barh(x.columns, importances)\n",
    "# plt.title('Feature Importances')\n",
    "# plt.xlabel('Feature')\n",
    "# plt.ylabel('Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-3 과제\n",
    "\n",
    "# 의사결정나무 vs 랜덤포레스트 성능 비교\n",
    "# 랜덤포레스트의 경우 컬럼의 위치가 바뀌게 되면 결과값이 달라짐\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df[['age', 'fare', 'sex', 'embarked', 'class', 'survived']].dropna()\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "df = pd.get_dummies(df, columns=['embarked', 'class'], drop_first=True)\n",
    "\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "decisionTree.fit(x_train, y_train)\n",
    "\n",
    "randomForest = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "randomForest.fit(x_train, y_train)\n",
    "\n",
    "randomForest_pred = randomForest.predict(x_test)\n",
    "decisionTree_pred = decisionTree.predict(x_test)\n",
    "\n",
    "print(f\"DecisionTree 정확도: {accuracy_score(y_test, decisionTree_pred)}\")\n",
    "print(f\"RandomForest 정확도: {accuracy_score(y_test, randomForest_pred)}\")\n",
    "\n",
    "# 하이퍼파라미터 튜닝 - max_depth 변경\n",
    "# max_depth 값 변화에 따른 정확도 시각화\n",
    "depth_list = range(1, 11)\n",
    "accuracies_DT = []\n",
    "accuracies_RF = []\n",
    "for depth in depth_list:\n",
    "  DT = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "  DT.fit(x_train, y_train)\n",
    "  DT_pred = DT.predict(x_test)\n",
    "  accuracies_DT.append(accuracy_score(y_test, DT_pred))\n",
    "  \n",
    "  RF = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "  RF.fit(x_train, y_train)\n",
    "  RF_pred = RF.predict(x_test)\n",
    "  accuracies_RF.append(accuracy_score(y_test, RF_pred))\n",
    "  \n",
    "plt.plot(depth_list, accuracies_DT, color='#ff7f0e', marker='o', label='Decision Tree')\n",
    "plt.plot(depth_list, accuracies_RF, color='#1f77b4', marker='o', label='Random Forest')\n",
    "plt.title('max_depth에 따른 정확도 변화')\n",
    "plt.xlabel('max_depth 값')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. SVM (서포트 벡터 머신)\n",
    "\n",
    "# 데이터를 구분하는 최적의 결정 경계(초평면)를 찾아 분류\n",
    "# 서포트 벡터: 결정 경계를 가장 가까이에서 '지지'하는 포인트들\n",
    "# 마진: 두 클래스 간 경계 거리를 최대화 하는 선을 찾음\n",
    "# 선형 분리가 어려운 경우 커널 함수로 고차원 공간으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-2 실습\n",
    "\n",
    "# 1) Iris 데이터 SVM 분류\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 데이터 로드\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# SVC 모델 학습\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "# kernel Trick: 저차원에서 선형 분리가 불가능한 데이터를 고차원 공간으로 매핑해서 선형 분리 \n",
    "# C: 오차 허용도 조절\n",
    "# gamma: 커널에서 결정 경계 영향도\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"\\n정확도:\", accuracy_score(y_test, y_pred))\n",
    "print(f\"\\n혼동 행렬:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"\\n분류 리포트:\\n{classification_report(y_test, y_pred, target_names=iris.target_names)}\")\n",
    "\n",
    "# 2) 2D 시각화를 위한 데이터 축소 (PCA 활용)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA 로 2차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "# 다시 분할 및 모델 학습\n",
    "x_train_pca, x_test_pca, y_train, y_test = train_test_split(x_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model2 = SVC(kernel='linear', C=1.0)\n",
    "model2.fit(x_train_pca, y_train)\n",
    "pca_pred = model2.predict(x_test_pca)\n",
    "print(f\"PCA 축소 정확도: {accuracy_score(y_test, pca_pred)}\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_test_pca[:, 0], x_test_pca[:, 1], c=model2.predict(x_test_pca), cmap='coolwarm', edgecolors='k')\n",
    "plt.title('PCA 기반 SVM 예측 분류')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x_test_pca[:, 0], x_test_pca[:, 1], c=y_test, cmap='coolwarm', edgecolors='k')\n",
    "plt.title('PCA 기반 실제 라벨 분류')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-3 과제\n",
    "\n",
    "# 1) 커널 함수에 따른 정확도 비교\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "for k in kernels:\n",
    "  model = SVC(kernel=k, C=1.0, gamma='scale')\n",
    "  model.fit(x_train, y_train)\n",
    "  y_pred = model.predict(x_test)\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  print(f\"커널- {k} 정확도: {acc:.4f}\")\n",
    "\n",
    "# 2) 새로운 샘플 예측\n",
    "sample = [[5.1, 3.5, 1.4, 0.2]]\n",
    "model = SVC(kernel='sigmoid', C=1.0, gamma='scale')\n",
    "# 위에서 작성된 for문으로 인해 가장 마지막 값이었던 sigmoid 가 모델 커널 값으로 적용되어있음\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(sample)\n",
    "\n",
    "print(f\"\\n{y_pred}\")\n",
    "print(f\"새 샘플 예측된 품종: {iris.target_names[y_pred][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Naive Bayes (나이브 베이즈)\n",
    "\n",
    "# 나이브 가정 (Naive Assumption)\n",
    "# 모든 특성(컬럼)이 서로 독립이라고 가정\n",
    "# ex) 단어가 서로 독립적으로 등장한다고 가정\n",
    "# 실제로는 완벽히 독립이 아니어도 성능이 괜찮은 경우가 많음\n",
    "\n",
    "# 나이브 베이즈 종류\n",
    "# <종류: 특징 - 예시>\n",
    "# GaussianNB: 연속형 데이터(정규 분포 가정) - iris 데이터\n",
    "# MultinomialNB: 카운트 기반 데이터 - 텍스트 데이터\n",
    "# BernoulliNB: 이진 데이터 - 스팸 메일 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-2 실습\n",
    "# iris 데이터 with GaussianNB\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 데이터 로드\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(x_test)\n",
    "print(f\"정확도: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"\\n혼동 행렬:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"\\n분류 리포트:\\n{classification_report(y_test, y_pred, target_names=iris.target_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-3 과제\n",
    "# 1) Breast Cancer 데이터에 GaussianNB 적용\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드\n",
    "cancer = load_breast_cancer()\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# 평가\n",
    "print(f\"유방함 데이터 분류 정확도: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-3 과제\n",
    "# 2) 간단한 텍스트 분류 (MultinomialNB)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 샘플 텍스트 데이터\n",
    "texts = [\"free money offer\", \"buy now\", \"important update\", \"let's catch up tomorrow\"]\n",
    "labels = [1, 1, 0, 0]\n",
    "\n",
    "# 텍스트 벡터화\n",
    "vectorizer = CountVectorizer() # 단어별 등장 횟수\n",
    "x = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 모델 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(x, labels)\n",
    "\n",
    "# 새 메세지 예측 ('free buy offer now')\n",
    "new_msg = vectorizer.transform(['free buy offer now'])\n",
    "pred = model.predict(new_msg)\n",
    "\n",
    "print(\"새 메세지 예측 결과:\", \"스팸\" if pred[0] == 1 else \"정상\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. K-Means 알고리즘 / Clustering(군집)분석\n",
    "# > 비지도 학습 유형 <\n",
    "\n",
    "# 라벨(정답)이 없는 데이터에서 패턴을 찾아 그룹(군집)화하는 방법\n",
    "# ex) 고객 세그먼트 분류, 뉴스 자동 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-2 실습\n",
    "\n",
    "# 1) Iris 데이터 K-Means 클러스터링\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로딩\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target # 실제 라벨 (비지도 학습에선 사용X)\n",
    "\n",
    "# K-Means 클러스터링(K=3)\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = model.fit_predict(x)\n",
    "\n",
    "# 시각화를 위해 차원 축소(PCA)\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# 실제 라벨\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=x_pca[:, 0], y=x_pca[:, 1], hue=y, palette='Set1')\n",
    "plt.title('실제 라벨 분류')\n",
    "\n",
    "# K-Means 군집 결과\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=x_pca[:, 0], y=x_pca[:, 1], hue=clusters, palette='Set2')\n",
    "plt.title('K-Means 군집 분류')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) KMeans 군집화 정확도\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import adjusted_rand_score \n",
    "\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "  labels = np.zeros_like(y_pred)\n",
    "  # y_pred 와 같은 shape를 가진 배열을 만들고, 모든 값을 0으로 채우고 labels 라는 변수에 할당\n",
    "  \n",
    "  for i in range(np.max(y_pred) + 1):\n",
    "  # 군집 개수만큼 반복\n",
    "  \n",
    "    mask = (y_pred == i)\n",
    "    # y_pred 의 전체 배열에서 i 값을 가진 위치만 True, 나머지 위치는 False\n",
    "    \n",
    "    labels[mask] = mode(y_true[mask])[0]\n",
    "    # 원본 군집(iris.target)에서 mask로 인해 True가 된 위치에서 가장 많이 나온 값을 뽑아냄\n",
    "    # 뽑아낸 값을 labels의 mask로 인해 True가 된 위치에 전부 대입\n",
    "    \n",
    "  return accuracy_score(y_true, labels)\n",
    "\n",
    "acc = cluster_accuracy(y, clusters)\n",
    "print(\"KMeans 군집 정확도:\", acc)\n",
    "ari = adjusted_rand_score(y, clusters)\n",
    "print(\"Kmenas 군집 정확도:\", ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 축소 없이 K-Means 클러스터링 시각화 (산점도 기반)\n",
    "\n",
    "# 차원 축소(PCA)를 사용하지 않고 원래의 4차원 iris 데이터를 그대로 사용해서 시각화\n",
    "# 쌍플롯(pairplot) 혹은 2개씩 feature를 선택하여 2D 산점도로 시각화하는 방식이 일반적\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. 데이터 로딩\n",
    "iris = load_iris()\n",
    "\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "\n",
    "# 2. K-Means 클러스터링\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "df['cluster'] = model.fit_predict(df[iris.feature_names])\n",
    "\n",
    "# 3. 시각화 (2D 산점도)\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "print(iris.feature_names)\n",
    "features = iris.feature_names\n",
    "\n",
    "plot_idx = 1\n",
    "\n",
    "for i in range(4):\n",
    "  for j in range(i+1, 4):\n",
    "    plt.subplot(3, 2, plot_idx)\n",
    "    sns.scatterplot(\n",
    "      x=df[features[i]],\n",
    "      y=df[features[j]],\n",
    "      hue=df['species'],\n",
    "      palette='Set1',\n",
    "      alpha=0.7,\n",
    "      edgecolor='black'\n",
    "    )\n",
    "    plt.title(f\"실제 라벨: {features[i]} vs {features[j]}\")\n",
    "    plot_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 클러스터 결과도 같은 방식으로 시각화\n",
    "plt.figure(figsize=(14, 10))\n",
    "plot_idx = 1\n",
    "\n",
    "for i in range(4):\n",
    "  for j in range(i+1, 4):\n",
    "    plt.subplot(3, 2, plot_idx)\n",
    "    sns.scatterplot(\n",
    "      x=df[features[i]],\n",
    "      y=df[features[j]],\n",
    "      hue=df['cluster'],\n",
    "      palette='Set2',\n",
    "      alpha=0.7,\n",
    "      edgecolor='black'\n",
    "    )\n",
    "    plt.title(f\"K-Means 군집: {features[i]} vs {features[j]}\")\n",
    "    plot_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 K 값 찾기 (엘보우 기법)\n",
    "# 최적 K값: SSE 가 급격히 감소하다가 완만해지는 지점\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 데이터 로딩\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "# y = iris.target # 실제 라벨 (비지도 학습에선 사용X)\n",
    "\n",
    "sse = []\n",
    "# 군집 수(K)에 따른 SSE(Sum of squared Errors) 저장\n",
    "for k in range(1, 11):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "  kmeans.fit(x)\n",
    "  sse.append(kmeans.inertia_) # inertia_ = SSE\n",
    "\n",
    "# 시각화\n",
    "plt.plot(range(1, 11), sse, marker='o')\n",
    "plt.title(\"엘보우 기법 (SSE vs K)\")\n",
    "plt.xlabel(\"K (군집 수)\")\n",
    "plt.ylabel(\"SSE (Sum of Squared Errors)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-3 과제\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "\n",
    "# 시각화를 위해 차원 축소(PCA)\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "# 1) K = 2~3 까지의 군집 결과 시각화\n",
    "for k in range(2, 4):\n",
    "  model = KMeans(n_clusters=k, random_state=42)\n",
    "  labels = model.fit_predict(x)\n",
    "  \n",
    "  plt.figure(figsize=(6,4))\n",
    "  sns.scatterplot(x=x_pca[:, 0], y=x_pca[:, 1], hue=labels, palette='tab10')\n",
    "  plt.title(f\"K={k} 클러스터 결과\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# 2) 군집 결과 vs 실제 라벨 비교\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# K-Means 클러스터링(K=3)\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = model.fit_predict(x)\n",
    "\n",
    "ari = adjusted_rand_score(y, clusters)\n",
    "# Rand Index (RI): 두 군집화 결과(라벨 두개) 의 유사도를 측정하는 지표 (0~1 사이 값)\n",
    "# Adjusted Rand Index (ARI): RI 를 무작위 군집화로 인한 점수 기대값을 빼고 보정한 지표 (보통 -1 ~ 1 사이)\n",
    "print(f\"ARI: {ari}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Hierarchical Clustering (계층적 군집)\n",
    "\n",
    "# 데이터를 트리 구조로 묶는 방식의 군집 알고리즘\n",
    "# 병합형(agglomerative): 개별 포인트에서 큰 군집으로 병합\n",
    "# 분할형(divisive): 전체에서 점점 세분화(거의 사용X)\n",
    "\n",
    "# 덴드로그램(Dendrogram)\n",
    "# 나무처럼 분기되는 그래프\n",
    "# 어디서 자르느냐에 따라 군집 수(K)가 결정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='image/clustering1.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,2], [2,3], [5,8], [6,9], [8,8], [10,10]])\n",
    "z = linkage(x, method='single')\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(14,6))\n",
    "# 1) scatter()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x=x[:, 0], y=x[:, 1], c='blue')\n",
    "plt.title('Data Points (Scatter Plot)')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid(True)\n",
    "[plt.text(xi+0.1, yi+0.1, f'P{i+1}') for i, (xi, yi) in enumerate(x)]\n",
    "\n",
    "# 2) dendrogram()\n",
    "plt.subplot(1, 2, 2)\n",
    "# dendrogram(z, labels=['P1', 'P2', 'P3', 'P4', 'P5', 'P6'])\n",
    "dendrogram(z, labels=[f\"P{i+1}\" for i in range(len(x))])\n",
    "plt.title('Dendrogram (Single Linkage)')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('distance')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14-2 실습\n",
    "# 1) iris 데이터 + 덴드로그램\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 준비\n",
    "iris = load_iris()\n",
    "x = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# 표준화(거리 기반 알고리즘에서는 중요)\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# 계층적 클러스터링 수행 (linkage 행렬 계산)\n",
    "linkage_matrix = linkage(x_scaled, method='ward')\n",
    "# SSE: Sum Squared Error - 평균 제곱오차합 작은 값\n",
    "\n",
    "# 덴드로이드 시각화\n",
    "plt.figure(figsize=(12,5))\n",
    "dendrogram(linkage_matrix, truncate_mode=\"lastp\", p=30, leaf_rotation=0, leaf_font_size=10, show_contracted=True)\n",
    "plt.title('덴드로그램 (Hierarchical Clustering)')\n",
    "plt.xlabel('데이터 포인트 그룹')\n",
    "plt.ylabel('거리(유사도)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) 군집 결과 생성 및 시각화\n",
    "# 거리 기준으로 군집 수(K=3)로 자르기\n",
    "labels = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
    "\n",
    "# 2D 시각화를 위한 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# 군집 결과 시각화\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(x=x_pca[:,0], y=x_pca[:,1], hue=labels, palette='Set2')\n",
    "plt.title('계층적 군집 결과 (K=3)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14-3 과제\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# 데이터 준비\n",
    "iris = load_iris()\n",
    "x = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# 표준화(거리 기반 알고리즘에서는 중요)\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# 1) linkage 방식 변경하여 덴드로그램 비교\n",
    "for method in ['single', 'complete', 'average', 'ward']:\n",
    "  plt.figure(figsize=(8,4))\n",
    "  linkage_matrix = linkage(x_scaled, method=method)\n",
    "  dendrogram(linkage_matrix, truncate_mode='lastp', p=30, leaf_rotation=90)\n",
    "  plt.title(f\"Linkage 방식: {method}\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  \n",
    "# 2) 군집 결과와 실제 라벨 \n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "for method in ['single', 'complete', 'average', 'ward']:\n",
    "  linkage_matrix = linkage(x_scaled, method=method)\n",
    "  labels = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
    "  ari = adjusted_rand_score(y, labels)\n",
    "  print(f\"ARI(Adjusted Rand Index), ({method}): {ari}\")\n",
    "print(\"ARI: 군집화(clustering) 결과와 실제 정답(ground truth) 라벨 간의 정학도를 측정하는 지표\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. 차원축소 - PCA (Principal component Analysis: 주성분 분석)\n",
    "\n",
    "# 데이터의 분산이 가장 큰 방향으로 새로운 축을 생성해서 차원을 줄임\n",
    "# 주성분(PC: Principal Component)은 원래 feature(컬럼)의 선형 조합\n",
    "# 데이터 손실 없이 최대한 정보(분산)를 유지하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15-2 실습\n",
    "\n",
    "# iris 데이터 PCA로 시각화\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# PCA (2차원)\n",
    "pca = PCA(n_components=2)\n",
    "x_scaled_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "sns.scatterplot(x=x_scaled_pca[:,0], y=x_scaled_pca[:,1], hue=y, palette='Set1')\n",
    "plt.title('PCA: iris 데이터 2D 시각화')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 주성분의 분산 비율 확인\n",
    "pca_full = PCA()\n",
    "pca_full.fit(x_scaled)\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "cumulative = explained.cumsum()\n",
    "\n",
    "for i, (e, c) in enumerate(zip(explained, cumulative), 1):\n",
    "  print(f\"PC{i}: 설명력={e:.4f}, 누적설명력={c:.4f}\")\n",
    "  \n",
    "# 누적 설명력 시각화\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% 설명력')\n",
    "plt.title('누적 분산 비율')\n",
    "plt.xlabel('주성분 개수')\n",
    "plt.ylabel('누적 설명력')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15-3 설명력 95% 이상을 만족하는 최소 주성분 개수는?\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# PCA (2차원)\n",
    "pca = PCA(n_components=2)\n",
    "x_scaled_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# 주성분의 분산 비율 확인\n",
    "pca_full = PCA()\n",
    "pca_full.fit(x_scaled)\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "\n",
    "# 설명력 95퍼 이상을 만족하는 주성분 개수\n",
    "import numpy as np\n",
    "\n",
    "cumulative_explained = np.cumsum(explained)\n",
    "num_components = np.argmax(cumulative >= 0.95) + 1\n",
    "print(f\"설명력 95퍼 이상을 만족하는 주성분 개수: {num_components}개\")\n",
    "\n",
    "# PCA 이후에 K-Means 클러스터링 적용\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(x_scaled_pca)\n",
    "\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(x=x_scaled_pca[:,0], y=x_scaled_pca[:,1], hue=labels, palette='Set2')\n",
    "plt.title(\"PCA + KMeans 클러스터링 결과\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Ensemble Learning (앙상블 학습)\n",
    "\n",
    "# 여러 개의 약한 모델을 결합해 강한 모델을 만드는 방법\n",
    "# 장점 - 과적합 감소, 일반화 성능 향상\n",
    "# 단점 - 학습 속도 느릴 수 있음, 해석 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16-2 실습\n",
    "\n",
    "# 1) 보팅 앙상블 (VotingClassifier)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 준비\n",
    "x, y = load_iris(return_X_y=True)\n",
    "# iris = load_iris()\n",
    "# x = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 개별 모델\n",
    "clf1 = LogisticRegression(max_iter=100, random_state=42)\n",
    "clf2 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf3 = SVC(probability=True)\n",
    "\n",
    "# 보팅 모델 (하드 보팅)\n",
    "voting = VotingClassifier(estimators=[\n",
    "  ('lr', clf1), ('dt', clf2), ('svc', clf3)\n",
    "  ],\n",
    "  voting='hard'\n",
    ")\n",
    "\n",
    "voting = voting.fit(x_train, y_train)\n",
    "voting_pred = voting.predict(x_test)\n",
    "\n",
    "print(f\"보팅 분류기 정확도: {accuracy_score(y_test, voting_pred)}\")\n",
    "\n",
    "# 2) 배깅 - 랜덤 포레스트 (RandomForestClassifier)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "print(f\"랜덤 포레스트 정확도: {accuracy_score(y_test, rf.predict(x_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16-3 과제\n",
    "# 각 앙상블 모델의 성능 비교 표 작성 (titanic data)\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1) 데이터 로드 및 필수 결측치 제거\n",
    "df = sns.load_dataset('titanic')\n",
    "print(df.dtypes, \"\\n\")\n",
    "print(df.isnull().sum()[df.isnull().sum()>0], \"\\n\")\n",
    "df = df.dropna(subset=['age', 'fare', 'sex', 'embarked', 'class', 'survived'])\n",
    "\n",
    "# 2) 사용하지 않을 문자형 컬럼 제거\n",
    "df = df.drop(columns=['who', 'deck', 'embark_town', 'alive', 'adult_male', 'alone'])\n",
    "\n",
    "# 3) sex 컬럼 문자형 인코딩\n",
    "df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# 4) 범주형 컬럼 One-Hot 인코딩\n",
    "df['embarked'] = df['embarked'].astype('category')\n",
    "# object 타입은 문자열을 그대로 저장해서 메모리를 더 많이 차지함\n",
    "# category 타입은 내부적으로 숫자 코드로 저장하기 때문에 메모리를 훨씬 절약할 수 있음\n",
    "df = pd.get_dummies(df, columns=['embarked', 'class'], drop_first=True)\n",
    "\n",
    "# 5) 숫자형만 남았는지 확인\n",
    "# print(f\"< 숫자형만 남았는지 확인 >\\n{df.dtypes}\")\n",
    "print(\"숫자형만 남았는지 확인:\", df.select_dtypes(include='object').columns.tolist())\n",
    "\n",
    "# 6) 훈련/테스트 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 7) 모델 정의 및 학습\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "voting_s = VotingClassifier(estimators=[('rf', rf), ('gb', gb)], voting='soft')\n",
    "voting_h = VotingClassifier(estimators=[('rf', rf), ('gb', gb)], voting='hard')\n",
    "rf.fit(x_train, y_train)\n",
    "gb.fit(x_train, y_train)\n",
    "voting_s.fit(x_train, y_train)\n",
    "voting_h.fit(x_train, y_train)\n",
    "\n",
    "# 8) 모델 성능 비교\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = {\n",
    "  'RandomForest': rf,\n",
    "  'GradientBoosting': gb,\n",
    "  'Voting_s': voting_s,\n",
    "  'Voting_h': voting_h\n",
    "}\n",
    "print(\"\\n성능 비교\")\n",
    "for name, model in models.items():\n",
    "  y_pred = model.predict(x_test)\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  print(f\"{name} 정확도: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. 고급 Boosting 모델 - XGBoost & LightGBM\n",
    "\n",
    "# XGBoost (Extreme Gradient Boosting)\n",
    "# 이전 모델의 오류를 순차적으로 보완해나가는 방식으로 모델을 형성, 즉 이전 모델에서의 실제값과 예측값의 오차(loss)를 훈련데이터에 투입하고 gradient 를 이용하여 오류를 보완하는 방식\n",
    "# 앙상블 부스팅 기법의 한 종류\n",
    "\n",
    "# LightGBM (Light gradient Boosting Machine)\n",
    "# 머신러닝에서 부스팅 알고리즘을 사용하는 오픈소스 프레임워크\n",
    "# 부스팅 알고리즘 중 하나인 GBDT (Gradient Boosting Decision Tree)를 기반으로 하며, XGBoost 보다 속도와 효율성을 향상시킨 것이 특징\n",
    "# 훈련 속도가 빠르고, 대규모 데이터를 처리할 수 있으며, 병렬 및 분산 학습을 지원\n",
    "# GBM (Graident Boosting Machine) 기반 알고리즘으로 예측에 실패한 부분에 가중치를 더하면서 오차를 보완하는 식으로 순차적 트리를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17-3 실습\n",
    "\n",
    "# 1) XGBoost 분류 실습\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 준비\n",
    "breast_cancer = load_breast_cancer()\n",
    "x = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# DMatrix 변환\n",
    "# xgboost에서 사용하는 고성능 데이터 구조\n",
    "# 속도 향상, 조기종료(Early stopping) \n",
    "dtrain_xgb = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest_xgb = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "params_xgb = {\n",
    "  'objective': 'binary:logistic',\n",
    "  'max_depth': 4,\n",
    "  'eta': 0.1,\n",
    "  'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# 학습\n",
    "xgb_model = xgb.train(params_xgb, dtrain_xgb, num_boost_round=100)\n",
    "\n",
    "# 예측\n",
    "y_pred_prob_xgb = xgb_model.predict(dtest_xgb)\n",
    "y_pred_xgb = [1 if p > 0.5 else 0 for p in y_pred_prob_xgb]\n",
    "print(f\"XGBoost 정확도: {accuracy_score(y_test, y_pred_xgb)}\\n\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "# 2) LightGBM 분류 실습\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 데이터셋 변환\n",
    "dtrain_lgb = lgb.Dataset(x_train, label=y_train)\n",
    "dtest_lgb = lgb.Dataset(x_test, label=y_test, reference=dtrain_lgb)\n",
    "# 파라미터 설정\n",
    "params_lgb = {\n",
    "  'objective': 'binary',\n",
    "  'metric': 'binary+logloss',\n",
    "  'learning_rate': 0.1,\n",
    "  'max_depth': 4,\n",
    "  'verbose': -1\n",
    "}\n",
    "\n",
    "# 학습\n",
    "lgb_model = lgb.train(params_lgb, dtrain_lgb, num_boost_round=100)\n",
    "\n",
    "# 예측\n",
    "y_pred_prob_lgb = lgb_model.predict(x_test)\n",
    "y_pred_lgb = [1 if p > 0.5 else 0 for p in y_pred_prob_lgb]\n",
    "print(f\"LightGBM 정확도: {accuracy_score(y_test, y_pred_lgb)}\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "# feature 중요도 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# XGBoost\n",
    "xgb.plot_importance(xgb_model)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n",
    "# LightGBM\n",
    "lgb.plot_importance(lgb_model, max_num_features=10)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. 모델 성능 개선 & AutoML 소개\n",
    "\n",
    "# 모델 성능 향상 방법\n",
    "# 하이퍼파라미터 튜닝 - 모델 내부 설정값 변경 (max_depth, C, k, ...)\n",
    "# 교차검증 (K-Fold) - 데이터셋을 나누어 반복 검증\n",
    "# 파이프라인 - 전처리, 모델링 과정을 자동 연결\n",
    "# 앙상블 - 여러 모델 결합을 이용한 더 나은 예측\n",
    "\n",
    "# AutoML (자동 머신러닝)\n",
    "# 자동으로 전처리 → 피처(컬럼) 선택 → 모델 선택 → 하이퍼파라미터 튜닝 → 평가\n",
    "# GridSearchCV - 모든 조합을 비교해 성능이 좋은 모델 찾기\n",
    "# RandomizedSearchCV - 일부만 랜덤으로 비교해 빠르게 성능 개선\n",
    "# LazyPredict - 여러 모델의 성능을 빠르게 비교\n",
    "# TPOT - AutoML 전처리부터 모델까지 자동 구성\n",
    "# 교차검증 - 데이터를 나누어 평가 신뢰도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18-2 실습\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 파라미터 후보\n",
    "param_grid = {\n",
    "  'n_estimators': [50, 100, 150],\n",
    "  'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# 그리드 서치, 교차 검증\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "grid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"최고 모델 파라미터: {grid.best_params_}\")\n",
    "print(f\"최고 모델 성능: {grid.best_score_}\")\n",
    "\n",
    "# 테스트 평가\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"테스트 정확도: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "test_score = grid.score(X_test, y_test)\n",
    "print(f\"테스트 스코어: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18-3 과제\n",
    "# RandomizedSearchCV 로 하이퍼파라미터 조정해보기\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 파라미터 후보\n",
    "param_dist = {\n",
    "  'n_estimators': np.arange(50, 200),\n",
    "  'max_depth': np.arange(1, 10)\n",
    "}\n",
    "\n",
    "# 랜덤 서치, 교차 검증\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "rand_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, random_state=42)\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"랜덤서치 결과: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
