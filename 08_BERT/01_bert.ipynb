{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BERT 개요 및 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1 설치 및 개념 요약\n",
    "\n",
    "# BERT - Bidirectional Encoder Representations from Transformers\n",
    "# 사전 학습(pretraining) - 대규모 코퍼스로 미리 학습된 언어 모델\n",
    "# 파인튜닝(fine-tuning) - 다운스트림 태스크에 맞게 추가 학습\n",
    "# Hugging Face - 사전학습된 모델과 토크나이저 제공 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2 실습\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "# Hugging Face 에서 제공하는 TensorFlow 용 BERT 모델과 토크나이저 불러오기\n",
    "model_name = 'bert-base-uncased' # 영어용 모델\n",
    "# 만약 한국어라면 'klue/bert-base' 또는 'skt/kobert-base-v1' 사용 가능\n",
    "\n",
    "# 텍스트를 BERT 입력 형태로 바꿔주는 토크나이저\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 사전 학습된 BERT 모델 로딩\n",
    "# 정수 시퀀스 → 의미가 담긴 벡트(임베딩)으로 변환하는데 사용됨\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "print(\"모델과 토크나이저 로딩 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3 과제\n",
    "\n",
    "# 과제1\n",
    "'''\n",
    "다른 BERT 모델 불러오기\n",
    "'bert-base-cased', 'distilbert-base-uncased' 등 사용해보기\n",
    "주요 함수: from_pretrained()\n",
    "'''\n",
    "\n",
    "# 과제2\n",
    "'''\n",
    "내 이름을 BERT에 넣어보기\n",
    "tokenizer.encode(\"My name is Hwan.\") 결과 분석\n",
    "주요 함수: tokenize(), encode()\n",
    "'''\n",
    "\n",
    "# 과제3\n",
    "'''\n",
    "한국어 모델 로딩\n",
    "'klue/bert-base', 'monologg/kobert' 등 한글 모델 로딩해보기\n",
    "주요 함수: from_pretrained() 값에 klue/bert-base 모델 사용\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3\n",
    "\n",
    "# 과제1\n",
    "# 'bert-base-uncased' 외 다른 사전학습된 BERT 모델을 로딩해보기, ex) 'bert-base-cased', ...\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# from_pretrained()와 같은 함수 안에 넣는 값은 Hugging Face 에서 직접 찾아서 쓰는 것\n",
    "# 모델의 이름은 Hugging Face 모델 허브에서 검색 가능\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 확인\n",
    "print(\"모델 이름:\", model_name)\n",
    "print(\"예시 토큰화:\", tokenizer.tokenize(\"Hello, my name is BERT.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3\n",
    "\n",
    "# 과제2\n",
    "# \"My name is Hwan\" 문장을 tokenizer.encode()로 처리하고 결과를 출력\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"My name is Hwan\"\n",
    "\n",
    "# 인코딩: 토큰 → 정수ID\n",
    "tokens = tokenizer.tokenize(text)\n",
    "encoded_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# 토큰 리스트를 정수ID로 변환하고 싶다면 convert_tokens_to_ids 함수를 사용\n",
    "\n",
    "encoded = tokenizer.encode(text)\n",
    "# encode()는 내부에서 자동으로 tokenize()를 먼저 실행함\n",
    "# 즉 encode 함수엔 이미 토큰화가 진행된 토큰 리스트를 넣으면 오류가 발생함\n",
    "# 또한 encode는 특수 토큰 [CLS], [SEP] 자동 추가\n",
    "\n",
    "print(\"입력 문장:\", text)\n",
    "print(\"토큰화 결과:\", tokens)\n",
    "print(\"토큰 인코딩 결과:\", encoded_tokens)\n",
    "print(\"문장 인코딩 결과:\", encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3\n",
    "\n",
    "# 과제3\n",
    "# 한국어 지원 BERT 모델 중 하나를 선택하여 불러오고 간단한 문장을 토크나이즈\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "\n",
    "# 한국어 모델 로딩\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"안녕하세요, 저는 인공지능을 공부합니다.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"입력 문장:\", text)\n",
    "print(\"토큰화 결과:\", tokens)\n",
    "print(\"문장 인코딩 결과:\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BERT 토크나이저 이해와 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1 WordPiece & 특수 토큰\n",
    "'''\n",
    "개념 - 설명\n",
    "\n",
    "WordPiece - 희귀 단어를 서브워드로 분해 → 어휘집 크기 작아짐, OOV 문제 완화\n",
    "\n",
    "[CLS] - 문장 맨 앞에 추가 ↔ 문장 전체 표현을 담는 분류 토큰\n",
    "\n",
    "[SEP] - 문장 경계 구분, 문장쌍 입력 시 중간·끝에 삽입\n",
    "\n",
    "[PAD] - 길이 맞추기용 패딩 토큰(0)\n",
    "\n",
    "input_ids - 토큰 → 정수 ID 배열\n",
    "\n",
    "attention_mask - 실제 토큰(1) / 패딩(0) 구분\n",
    "\n",
    "token_type_ids - 문장 A(0) / 문장 B(1) 구분 (NSP·문장쌍 분류용)\n",
    "\n",
    "tokenize, encode - 텍스트 → 토큰/ID 변환\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2 실습\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 1) 단일 문장 토큰화\n",
    "sentence = \"Transformers are amazing\"\n",
    "enc = tokenizer(\n",
    "  sentence,\n",
    "  max_length=12,\n",
    "  padding='max_length',\n",
    "  truncation=True,\n",
    "  return_tensors='tf'\n",
    ")\n",
    "print(\"1) 단일 문장 토큰화\")\n",
    "print(f\"{enc}\\n\")\n",
    "print(f\"input_ids: {enc[\"input_ids\"][0].numpy()}\")\n",
    "print(f\"attention_mask: {enc[\"attention_mask\"][0].numpy()}\")\n",
    "\n",
    "# 2) 배치(batch_size == 샘플수) 인코딩\n",
    "sentences = [\"Hello BERT\", \"I love NLP with Transformers\"]\n",
    "batch = tokenizer(\n",
    "  sentences,\n",
    "  padding=True, # 가장 긴 문장에 맞춰 패딩\n",
    "  return_tensors='tf'\n",
    ")\n",
    "print(\"\\n2) 배치(batch_size == 샘플수) 인코딩\")\n",
    "print(f\"{batch}\\n\")\n",
    "print(f\"배치 input_ids shape: {batch[\"input_ids\"].shape}\")\n",
    "\n",
    "# 3) 문장쌍 인코딩\n",
    "q = \"What is BERT?\"\n",
    "a = \"BERT is a language model.\"\n",
    "pair = tokenizer(\n",
    "  q, a,\n",
    "  padding=\"max_length\",\n",
    "  max_length=20,\n",
    "  truncation=True,\n",
    "  return_tensors=\"tf\"\n",
    ")\n",
    "print(\"\\n3) 문장쌍 인코딩\")\n",
    "print(f\"token_type_ids: {pair[\"token_type_ids\"][0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-3 과제\n",
    "\n",
    "# 과제1\n",
    "'''\n",
    "한글 문장 \"안녕 BERT\"의 토큰화 결과, 인코딩 결과, 어텐션 마스크 출력\n",
    "'''\n",
    "\n",
    "# 과제2\n",
    "'''\n",
    "문장 3개 리스트를 길이 16으로 padding, truncation 하여 input_ids 텐서 shape 확인\n",
    "'''\n",
    "\n",
    "# 과제3\n",
    "'''\n",
    "문장쌍(\"A is faster.\", \"B is cheaper.\") 인코딩 후 token_type_ids 에서 0,1 비율 출력\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-3\n",
    "# 과제1\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tok_ko = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "text = \"안녕 BERT\"\n",
    "\n",
    "enc_ko = tok_ko(text, return_tensors=\"tf\")\n",
    "\n",
    "print(f\"토큰화 결과: {tok_ko.tokenize(text)}\")\n",
    "print(f\"token indices: {enc_ko[\"input_ids\"][0].numpy()}\")\n",
    "print(f\"mask: {enc_ko[\"attention_mask\"][0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-3\n",
    "# 과제2\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentences = [\"Quick test\", \"Tokenizer padding demo\", \"BERT & Transformers\"]\n",
    "\n",
    "batch_p16 = tokenizer(\n",
    "  sentences,\n",
    "  padding=\"max_length\",\n",
    "  max_length=16,\n",
    "  truncation=True,\n",
    "  return_tensors=\"tf\"\n",
    ")\n",
    "print(f\"{batch_p16}\\n\")\n",
    "print(f\"shape: {batch_p16[\"input_ids\"].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-3\n",
    "# 과제3\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "pair = tokenizer(\n",
    "  \"A is faster\", \"B is cheaper\",\n",
    "  padding=\"max_length\",\n",
    "  max_length=20,\n",
    "  return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "print(f\"{pair}\\n\")\n",
    "\n",
    "ttids = pair[\"token_type_ids\"][0].numpy()\n",
    "mask = pair[\"attention_mask\"][0].numpy()\n",
    "pct_0 = ((ttids == 0) & (mask == 1)).mean() * 100\n",
    "pct_1 = (ttids == 1).mean() * 100\n",
    "print(f\"0 비율: {pct_0:.1f}% | 1 비율: {pct_1:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BERT 입력 포맷 완전 이해 & 수동 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1 BERT 입력 포맷 3가지\n",
    "\n",
    "# 입력값 / 설명 / 예시\n",
    "'''\n",
    "input_ids / 토큰을 숫자ID로 변환한 배열 / [101, 2023, 2003, 1037, 2742, 102]\n",
    "\n",
    "attention_mask / 실제 토큰(1), 패딩(0) / [1,1,1,1,1,1,0,0]\n",
    "\n",
    "token_type_ids / 문장1(0), 문장2(1) / [0,0,0,0,1,1,1,1,1]\n",
    "'''\n",
    "\n",
    "# <포인트>\n",
    "# [CLS]: 문장 시작 - 항상 맨 앞에\n",
    "# [SEP]: 문장 끝 or 문장 사이 - 문장마다 1개\n",
    "# 최대 길이에 맞춰 패딩(0) 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2 수동 생성 vs 자동 생성 비교\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence1 = \"This is a good example.\"\n",
    "sentence2 = \"This example is great.\"\n",
    "\n",
    "# 자동 생성\n",
    "auto = tokenizer(\n",
    "  sentence1, sentence2,\n",
    "  padding='max_length',\n",
    "  max_length=16,\n",
    "  truncation=True,\n",
    "  return_tensors='tf'\n",
    ")\n",
    "print(\"[Auto]\")\n",
    "print(f\"auto input_ids: {auto[\"input_ids\"][0].numpy()}\")\n",
    "print(f\"auto attention_mask: {auto[\"attention_mask\"][0].numpy()}\")\n",
    "print(f\"auto token_type_ids: {auto[\"token_type_ids\"][0].numpy()}\")\n",
    "\n",
    "# 수동 생성\n",
    "tokens1 = tokenizer.tokenize(sentence1)\n",
    "tokens2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "# WordPiece 토큰 리스트\n",
    "tokens = ['[CLS]'] + tokens1 + ['[SEP]'] + tokens2 + ['[SEP]']\n",
    "manual_input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# attention_mask: 전부 1\n",
    "manual_attention_mask = [1]*len(manual_input_ids)\n",
    "\n",
    "# token_type_ids: 문장1은 0, 문장2는 1, +2(cls, sep), +1(sep)\n",
    "manual_token_type_ids = [0]*(len(tokens1)+2) + [1]*(len(tokens2)+1)\n",
    "\n",
    "# 패딩 처리 (길이 16 맞추기)\n",
    "max_len = 16\n",
    "while len(manual_input_ids) < max_len:\n",
    "  manual_input_ids.append(0)\n",
    "  manual_attention_mask.append(0)\n",
    "  manual_token_type_ids.append(0)\n",
    "  \n",
    "print(\"\\n[Manual]\")\n",
    "print(f\"manual input_ids: {np.array(manual_input_ids)}\")\n",
    "print(f\"manual attention_mask: {np.array(manual_attention_mask)}\")\n",
    "print(f\"manual token_type_ids: {np.array(manual_token_type_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3 과제\n",
    "\n",
    "# 과제1\n",
    "'''\n",
    "\"I like AI.\" 문장을 수동으로 토큰화하고 input_ids 출력\n",
    "'''\n",
    "\n",
    "# 과제2\n",
    "'''\n",
    "\"A\"와 \"B\" 문장쌍에 대해 token_type_ids 수동 생성\n",
    "'''\n",
    "\n",
    "# 과제3\n",
    "'''\n",
    "자동 인코딩 결과와 수동 생성 결과 비교\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3\n",
    "# 과제1\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"I like AI.\"\n",
    "\n",
    "tokens = ['[CLS]'] + tokenizer.tokenize(sentence) + ['[SEP]']\n",
    "inputs_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f\"수동 토큰화: {tokens}\\n\")\n",
    "print(f\"수동 토큰화 후 인코딩 결과:\\n{inputs_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3\n",
    "# 과제2\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokens_A = tokenizer.tokenize(\"A\")\n",
    "tokens_B = tokenizer.tokenize(\"B\")\n",
    "\n",
    "# [CLS] A [SEP] B [SEP]\n",
    "tokens = ['[CLS]'] + tokens_A + ['[SEP]'] + tokens_B + ['[SEP]']\n",
    "\n",
    "token_type_ids = [0]*(len(tokens_A)+2) + [1]*(len(tokens_B)+1)\n",
    "\n",
    "print(\"token_type_ids:\", token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3\n",
    "# 과제3\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "auto = tokenizer(\"I like AI.\", return_tensors='tf')\n",
    "auto_ids = auto[\"input_ids\"][0].numpy().tolist()\n",
    "\n",
    "tokens = ['[CLS]'] + tokenizer.tokenize(\"I like AI.\") + ['[SEP]']\n",
    "manual_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f\"자동 인코딩 결과: {auto_ids}\")\n",
    "print(f\"수동 인코딩 결과: {manual_ids}\")\n",
    "print(f\"두 결과는 동일한가? {auto_ids == manual_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 사전학습 BERT 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-1 개요\n",
    "\n",
    "# BERT 출력 / shape(배치, 토큰수, 차원) / 특징\n",
    "'''\n",
    "last_hidden_state / (B, L, 768) / 각 토큰별 768-D 벡터\n",
    "\n",
    "pooler_output / (B, 768) / [CLS] 벡터 → 분류용\n",
    "\n",
    "hidden_states / tuple(layer) / 모든 층의 출력(옵션)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
