{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이 학습(Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. [참고] 데이터 경로 및 압축 해제 상태 확인\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# 1. 데이터 다운로드 및 경로 설정\n",
    "url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=url, extract=True)\n",
    "base_dir = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_extracted/cats_and_dogs_filtered')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "train_gen = ImageDataGenerator(rescale=1./255.)\n",
    "val_gen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "  train_dir,\n",
    "  target_size = (224, 224),\n",
    "  batch_size = 32,\n",
    "  class_mode = 'binary'\n",
    ")\n",
    "val_data = val_gen.flow_from_directory(\n",
    "  val_dir,\n",
    "  target_size = (224, 224),\n",
    "  batch_size = 32,\n",
    "  class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# 3. 사전학습된 VGG16 모델 로딩\n",
    "base_model = VGG16(\n",
    "  weights='imagenet',\n",
    "  include_top = False,\n",
    "  input_shape = (224, 224, 3)\n",
    ")\n",
    "\n",
    "# 4. 모든 레이어 고정 (가중치 학습 방지)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 5. 새로운 출력층(1) 추가(Fully Connected)\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# 6. 컴파일\n",
    "model.compile(\n",
    "  optimizer = Adam(learning_rate=1e-4),\n",
    "  loss = 'binary_crossentropy',\n",
    "  metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# 7. 학습\n",
    "history = model.fit(\n",
    "  train_data,\n",
    "  epochs = 3,\n",
    "  validation_data = val_data\n",
    ")\n",
    "\n",
    "# 8. summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 실습1\n",
    "\n",
    "# VGG16 을 전이학습으로 사용하고, 데이터는 간단히 랜덤 이미지 데이터로 만들어서 구조 테스트\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import numpy as np\n",
    "\n",
    "# 1) 랜덤 이미지 데이터 생성\n",
    "# 훈련 데이터 100개, 검증 데이터 20개\n",
    "train_data = np.random.rand(100, 224, 224, 3)\n",
    "train_labels = np.random.randint(0, 2, size=(100,1))\n",
    "val_data = np.random.rand(20, 224, 224, 3)\n",
    "val_labels = np.random.randint(0, 2, size=(20,1))\n",
    "\n",
    "# 2) VGG16 사전 학습 모델 로딩\n",
    "# 모든 레이어의 가중치를 고정 (Feature Extractor 로 사용)\n",
    "# 입력 이미지에서 의미 있는 패턴(특징)을 자동 추출해주는 신경망의 앞쪽(Conv Layer) 부분\n",
    "base_model = VGG16(\n",
    "  weights='imagenet',\n",
    "  include_top=False,\n",
    "  # include_top=False: 마지막 분류층 임의 설정\n",
    "  input_shape=(224, 224, 3)\n",
    ")\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "  \n",
    "# 3) 새로운 출력층 추가 (Fine-tunning Head)\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 4) 전체 모델 정의 및 컴파일\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(\n",
    "\toptimizer='adam',\n",
    " loss='binary_crossentropy',\n",
    " metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5) 모델 학습\n",
    "history = model.fit(\n",
    "  train_data, train_labels,\n",
    "  epochs=3,\n",
    "  batch_size=16,\n",
    "  validation_data=(val_data, val_labels)\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 실습2\n",
    "\n",
    "# 1) 일반 합성곱 신경망(CNN)으로 분류 예측\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Conv2D, MaxPooling2D,BatchNormalization\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "SEED = 12\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# CIFAR10 이미지 데이터셋\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 피처 스케일링\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "\n",
    "print(f\"train data size: {X_train.shape} {y_train.shape}\")\n",
    "print(f\"test data size: {X_test.shape} {y_test.shape}\")\n",
    "\n",
    "for i in range(1, 21):\n",
    "  plt.subplot(4, 5, i)\n",
    "  plt.imshow(X_train[i])\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 모델 구축\n",
    "def build_cnn():\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(32, 32, 3)))\n",
    "  model.add(Conv2D(32, 5, strides=(2,2), activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D())\n",
    "  \n",
    "  model.add(Conv2D(64, 5, strides=(2, 2), activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D())\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  \n",
    "  model.compile(\n",
    "\t\toptimizer='adam',\n",
    "\t\tloss='sparse_categorical_crossentropy',\n",
    "\t\tmetrics=['accuracy']\n",
    "\t)\n",
    "  return model\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "cnn_model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "cnn_history = cnn_model.fit(\n",
    "  X_train, y_train,\n",
    "  batch_size = 256,\n",
    "  epochs = 20,\n",
    "  validation_split = 0.1,\n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "# 20 epoch 까지 손실함수와 정확도를 그래프로 나타내는 함수\n",
    "def plot_metrics(history):\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "  \n",
    "  # Loss: 손실함수\n",
    "  axes[0].plot(range(1, 21), history.history['loss'][:20], label='Train')\n",
    "  axes[0].plot(range(1, 21), history.history['val_loss'][:20], label='Validation')\n",
    "  axes[0].set_title('Loss')\n",
    "  axes[0].legend()\n",
    "  \n",
    "  # Accuracy: 예측 정확도\n",
    "  axes[1].plot(range(1, 21), history.history['accuracy'][:20], label='Train')\n",
    "  axes[1].plot(range(1, 21), history.history['val_accuracy'][:20], label='Validation')\n",
    "  axes[1].set_title(\"Accuracy\")\n",
    "  axes[1].legend()\n",
    "  \n",
    "  plt.show()\n",
    "  \n",
    "plot_metrics(cnn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 실습2\n",
    "\n",
    "# 2) 전이학습(Transfer Learning)으로 분류 예측\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터셋 생성 (cifar10)\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "\n",
    "# Pretrained 모델 가져오기 (ResNet50)\n",
    "cnn_base = ResNet50(\n",
    "  weights='imagenet',\n",
    "  include_top=False,\n",
    "  input_shape=(32, 32, 3)\n",
    ")\n",
    "\n",
    "# Transfer 모델 구축\n",
    "def build_transfer():\n",
    "  model = Sequential()\n",
    "  model.add(cnn_base)\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  \n",
    "  model.compile(\n",
    "\t\toptimizer='adam',\n",
    "\t\tloss='sparse_categorical_crossentropy',\n",
    "\t\tmetrics=['accuracy']\n",
    "\t)\n",
    "  \n",
    "  return model\n",
    "\n",
    "transfer_model = build_transfer()\n",
    "transfer_model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "tm_history = transfer_model.fit(\n",
    "  X_train, y_train,\n",
    "  epochs = 20,\n",
    "  batch_size = 256,\n",
    "  validation_split = 0.1,\n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "# 20 epoch 까지 손실함수와 정확도를 그래프로 나타내는 함수\n",
    "def plot_metrics(history):\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "  \n",
    "  axes[0].plot(range(1, 21), history.history['loss'][:20], label='Train')\n",
    "  axes[0].plot(range(1, 21), history.history['val_loss'][:20], label='Validation')\n",
    "  axes[0].set_title('Loss')\n",
    "  axes[0].legend()\n",
    "  \n",
    "  axes[1].plot(range(1, 21), history.history['accuracy'][:20], label='Train')\n",
    "  axes[1].plot(range(1, 21), history.history['val_accuracy'][:20], label='Validation')\n",
    "  axes[1].set_title('Accuracy')\n",
    "  axes[1].legend()\n",
    "  \n",
    "  plt.show()\n",
    "plot_metrics(tm_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
