{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 딥러닝 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI (인공지능) - 가장 넓은 개념\n",
    "# 정의: 인간의 지능을 모방하여 문제를 해결하는 기술\n",
    "# 예시: 체스 AI, 음성비서, 자율주행\n",
    "# 사용기술: 규칙 기반 시스템, 딥러닝, 머신러닝 포함\n",
    "\n",
    "# ML (머신러닝) - AI 의 하위 개념\n",
    "# 정의: 데이터를 이용해 스스로 학습하는 알고리즘\n",
    "# 예시: 스팸 메일 분류, 추천 시스템\n",
    "# 사용기술: 지도학습, 비지도학습, 강화학습\n",
    "\n",
    "# DL (딥러닝) - ML 의 하위 개념\n",
    "# 정의: 인공신경망을 통해 특징을 자동으로 학습하는 기술\n",
    "# 예시: 얼굴인식, 음성인식, 번역\n",
    "# 사용 기술: CNN, RNN, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1-2 실습\n",
    "# 머신러닝 vs 딥러닝\n",
    "\n",
    "# 머신러닝: 특징을 사람이 직접 추출 → 학습\n",
    "# 딥러닝: 특징 추출도 신경망이 자동으로 → 학습\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1) 데이터 불러오기\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 2) 훈련/테스트 나누기\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3) 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "# 4) 머신러닝 모델 (Random Forest)\n",
    "# 정수형 예측이므로 One-Hot Encoding X\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# 5) 딥러닝 모델 (Dense NN)\n",
    "# to_categorical = 딥러닝 모듈의 내장 One-Hot Encoding\n",
    "y_train_cat = to_categorical(y_train, 3)\n",
    "y_test_cat = to_categorical(y_test, 3)\n",
    "\n",
    "dl_model = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(3, activation='softmax')\n",
    "])\n",
    "dl_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  # loss='sparse_categorical_crossentropy'\n",
    "  # 해당 값을 이용하면 굳이 정수형으로 되어있는 클래스 값들을 One-Hot Encoding으로 변환해줄 필요가 없음\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "dl_model.fit(X_train, y_train_cat, epochs=30, verbose=0)\n",
    "\n",
    "# 6) 평가\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "_, dl_acc = dl_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# 7) 결과 비교\n",
    "result_df = pd.DataFrame({\n",
    "  '모델 종류': ['Random Forest (scikit-learn)', 'Neural Network (keras)'],\n",
    "  '정확도': [rf_acc, dl_acc]\n",
    "})\n",
    "print(\"[머신러닝과 딥러닝 비교 결과]\")\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3 과제1\n",
    "# 머신러닝 vs 딥러닝 dataset: wine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터 불러오기\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# 훈련/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 머신러닝 모델 (RandomForest)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# 딥러닝 모델 (Keras)\n",
    "# 1)전처리: scaling + One-Hot Encoding(to_categorical)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "# 2)모델 정의\n",
    "dl = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "# 3)모델 컴파일 및 훈련\n",
    "dl.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "dl.fit(X_train_scaled, y_train_cat, epochs=30, verbose=1)\n",
    "\n",
    "# 평가\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "_, dl_acc = dl.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "\n",
    "# 결과 비교 출력\n",
    "result_df = pd.DataFrame({\n",
    "  '모델 종류': ['Random Forest (scikit-learn)', 'Neural Network (keras)'],\n",
    "  '정확도': [rf_acc, dl_acc]\n",
    "})\n",
    "print(\"[머신러닝과 딥러닝 비교 결과]\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 신경망의 구조\n",
    "\n",
    "# 1) 퍼셉트론 (Perceptron)\n",
    "# 인공 신경망의 기본 단위\n",
    "# 입력값에 가중치를 곱하고 합산 후, 활성화 함수로 출력을 만듦\n",
    "# 입렵값과 가중치가 선형 결합됨\n",
    "# 여기에 편향 bbb를 더하고, 활성화 함수 fff를 거쳐 결과 yyy출력\n",
    "# 가장 기본적인 신경만 단위로 AND, OR 논리 게이트 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 실습1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1) 입력데이터 (AND 연산)\n",
    "# 각 행은 [x1, x2]\n",
    "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "# 2. 정답 레이블 (AND 연산의 결과)\n",
    "# 0 AND 0 = 0,\n",
    "# 0 AND 1 = 0,\n",
    "# 1 AND 0 = 0,\n",
    "# 1 AND 1 = 1\n",
    "y = np.array([0,0,0,1])\n",
    "\n",
    "# 3. 퍼셉트론 파라미터 초기화 (y=wx+b)\n",
    "# 가중치 2개(w1, w2), 편향 b\n",
    "w = np.random.rand(2)\n",
    "b = np.random.rand()\n",
    "\n",
    "# 4. 학습률 (Learning Rate)\n",
    "# 모델이 얼마나 빠르게 학습할 지 속도 조절\n",
    "# 보통 0.01 ~ 0.1 설정 (추후 재확인)\n",
    "lr = 0.1\n",
    "\n",
    "# 5. 활성화 함수 (계단 함수 - step function)\n",
    "def step(x):\n",
    "  return 1 if x >= 0 else 0\n",
    "\n",
    "# 6. 학습 반복\n",
    "for epoch in range(20):\n",
    "  print(f\"\\nEPoch {epoch+1}\")\n",
    "  for i in range(len(x)):\n",
    "    z = np.dot(x[i], w) + b # z = w1*x1 + w2*x2 + b\n",
    "    y_pred = step(z)\n",
    "    error = y[i] - y_pred # 오차 = 정답 - 예측값\n",
    "    w = w + lr * error * x[i]\n",
    "    b = b + lr * error\n",
    "    print(f\"x:{x[i]}, y:{y[i]}, pred:{y_pred}, w:{w}, b:{b}\")\n",
    "\n",
    "for x_input in x:\n",
    "  z = np.dot(x_input, w) + b\n",
    "  y_output = step(z)\n",
    "  print(f\"입력: {x_input} -> 출력: {y_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제1\n",
    "# 퍼셉트론 논리 게이트 구현 (AND, OR)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def perceptron(x1, x2, w1, w2, b):\n",
    "  x = np.array([x1, x2])\n",
    "  w = np.array([w1, w2])\n",
    "  output = np.dot(w, x) + b\n",
    "  return 1 if output > 0 else 0\n",
    "\n",
    "# AND 게이트\n",
    "print(\"AND 게이트 결과:\") \n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],0.5,0.5,-0.7)}\")\n",
    "  \n",
    "# OR 게이트\n",
    "print(\"\\nOR 게이트 결과:\")\n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],1,1,-0.7)}\")\n",
    "  \n",
    "# NAND 게이트\n",
    "print(\"\\nNAND 게이트 결과:\")\n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],-0.5,-0.5,0.7)}\")\n",
    "  \n",
    "# XOR 게이트 - Multi Perceptron (다층 퍼셉트론) 사용해야함\n",
    "print(\"\\nXOR 게이트 결과(Multi Perceptron 사용해야함):\")\n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],0.5,0.5,-0.7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 실습2\n",
    "# MLP (Multi-Layer Perceptron)은 입력층 → 은닉층 → 출력층 구조로 구성되어 있고, 단일 퍼셉트론과 달리 비선형 문제도 해결 가능\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# XOR 을 MLP 에서 동작 확인하기\n",
    "\n",
    "# 1) xor\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]]) # 입력\n",
    "y = np.array([[0], [1], [1], [0]]) # 정답\n",
    "# y_pred = 예측값\n",
    "# error(손실,loss) = 정답 - y_pred\n",
    "\n",
    "# 2) model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(2,)))\n",
    "model.add(Dense(4, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3) compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4) 모델 학습\n",
    "model.fit(X, y, epochs=500, verbose=1)\n",
    "# verbose(학습과정 출력)\n",
    "# 0 = 아무것도 출력하지 않음\n",
    "# 1 = 진행 상황을 1줄씩(epoch별) 출력\n",
    "# 2 = 1과 비슷하지만 조금 더 간략하게 출력\n",
    "\n",
    "# 예측 결과 출력\n",
    "predictions = model.predict(X)\n",
    "print(\"입력값\\t 예측값\\t 라운딩\")\n",
    "for i, pred in enumerate(predictions):\n",
    "  print(f\"{X[i]} → {pred[0]:.4f} → {np.round(pred[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# XOR 을 MLP 에서 동작 확인하기\n",
    "\n",
    "# 입력 데이터 (XOR 문제)\n",
    "X = np.array([\n",
    "  [0,0],\n",
    "  [0,1],\n",
    "  [1,0],\n",
    "  [1,1]\n",
    "])\n",
    "\n",
    "# XOR 의 정답\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "\n",
    "# 입력 데이터 형태를 지정\n",
    "model.add(Input(shape=(2,)))\n",
    "\n",
    "# 은닉층: 2개의 뉴런(노드), 활성화 함수: relu\n",
    "model.add(Dense(2, activation='relu'))\n",
    "\n",
    "# 출력층: 1개의 뉴런(노드), 활성화 함수: sigmoid(이진 분류)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(\n",
    "  optimizer=SGD(learning_rate=0.1),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "# verbose(학습과정 출력)\n",
    "# 0 = 아무것도 출력하지 않음\n",
    "# 1 = 진행 상황을 1줄씩(epoch별) 출력\n",
    "# 2 = 1과 비슷하지만 조금 더 간략하게 출력\n",
    "\n",
    "# 모델 평가\n",
    "loss, acc = model.evaluate(X, y, verbose=0)\n",
    "print(f\"최종 정확도: {acc:.2f}\")\n",
    "\n",
    "# 예측 결과 출력\n",
    "pred = model.predict(X)\n",
    "print(f\"\\n예측 결과 (확률값):\\n{pred.round(3)}\")\n",
    "print(f\"\\n예측 결과 (이진 출력):\\n{(pred > 0.5).astype(int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 2 - 과제2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제2-1\n",
    "# NAND 게이트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제2-2\n",
    "# XOR 게이트가 퍼셉트론으로 구현 안되는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제2-3\n",
    "# MLP 가 XOR 문제를 해결할 수 있는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 2 - 과제2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning 보충학습\n",
    "# 단순 신경망 훈련(선형회귀)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# 샘플 데이터셋 생성\n",
    "x = np.arange(1,6).reshape(-1,1)\n",
    "y = 3*x + 2\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# 시각화\n",
    "plt.plot(x, y)\n",
    "plt.title(\"y = 3x + 2\")\n",
    "plt.show()\n",
    "\n",
    "# 단순선형회귀 모델 생성, 구조\n",
    "\n",
    "dl = Sequential([\n",
    "  Input(shape=(1,)),\n",
    "  Dense(1)\n",
    "])\n",
    "dl.summary()\n",
    "\n",
    "# 컴파일, 훈련\n",
    "dl.compile(\n",
    "  optimizer='sgd',\n",
    "  loss='mse',\n",
    "  metrics=['mae']\n",
    ")\n",
    "history = dl.fit(x, y, epochs=1200, verbose=0)\n",
    "\n",
    "# 20 에포크까지 Loss 수렴에 대한 시각화\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['mae'], label='mae')\n",
    "plt.xlim(-1, 20)\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 검증\n",
    "loss, mae = dl.evaluate(x, y, verbose=0)\n",
    "print(loss, mae)\n",
    "\n",
    "# 예측\n",
    "# x=10 → y=?\n",
    "result = dl.predict(np.array([[10]]))\n",
    "print(\"예측 결과:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 역전파(Backpropagation) 와 학습 \n",
    "\n",
    "# 손실 함수 (Loss Function)\n",
    "# 모델의 예측값과 실제값 사이의 오차 정도를 수치화\n",
    "# 회귀 문제: MSE (Mean Squared Error)\n",
    "# 분류 문제: Cross Entropy Loss\n",
    "\n",
    "# 역전파 알고리즘\n",
    "# 오차를 기준으로 가중치를 거꾸로 전파하여 수정하는 알고리즘\n",
    "# Chain Rule 을 이용하여 각 층의 가중치에 대한 기울기 계산\n",
    "\n",
    "# 학습률 (Learning Rate)\n",
    "# 가중치를 얼마나 크게 수정할 것인지 결정하는 하이퍼파라미터\n",
    "# 너무 크면 발산, 너무 작으면 학습 속도 저하"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2 실습\n",
    "# 간단한 손실 함수와 그래디언트 계산\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# MSE 손실 함수 (Mean Square Error) - 평균제곱오차\n",
    "def mse_loss(y_true, y_pred):\n",
    "  return np.mean((y_true-y_pred)**2)\n",
    "\n",
    "y_true = np.array([1, 0, 1]) # 정답\n",
    "y_pred = np.array([0.8, 0.2, 0.6]) # 예측\n",
    "\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "print(\"손실값(MSE):\", loss)\n",
    "\n",
    "# 가중치에 대한 그래디언트(기울기) 계산 예시 (y=ax+b)\n",
    "x = np.array([1.0, 2.0])\n",
    "w = np.array([0.5, -0.5])\n",
    "y = 1.0 # 정답\n",
    "\n",
    "y_pred = np.dot(w, x)\n",
    "error = y_pred - y\n",
    "gradient = 2 * error * x\n",
    "print(\"Gradient(기울기):\", gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-3 과제\n",
    "# 1) MSE 와 Cross Entropy 의 차이를 서술\n",
    "'''\n",
    "MSE = 회귀에 사용,\n",
    "      연속형 값의 오차를 측정,\n",
    "      큰 오차에 민감\n",
    "\n",
    "Cross Entropy = 분류에 사용,\n",
    "                확률값 기반으로 분류 평가\n",
    "                잘못된 확률에 민감\n",
    "'''\n",
    "\n",
    "# 2) y_true = [1, 0, 1], y_pred = [0.9, 0.3, 0.7]\n",
    "#    해당 데티어에서의 손실값(MSE)을 계산\n",
    "import numpy as np\n",
    "y_true = np.array([1, 0, 1])\n",
    "y_pred = np.array([0.9, 0.3, 0.7])\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "print(\"MSE손실값:\", mse)\n",
    "\n",
    "# 3) 학습률이 너무 큰 경우와 너무 작은 경우의 문제점을 설명\n",
    "'''\n",
    "큰경우: 손실값이 수렴하지 않고 발산할 수 있으며, 최적값 근처에서 오락가락할 수 있음\n",
    "* 발산: 손실값이 점점 줄어들어야 하는데 오히려 점점 커지는 현상을 말함\n",
    "\n",
    "작은경우: 학습 속도가 매우 느려지고, 최적값에 도달하기까지 시간이 오래걸림\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 활성화 함수\n",
    "# 뉴런에서 입력값의 합을 비선형적으로 변환하여 다음 층으로 전달하는 함수\n",
    "# 모델이 비선형 문제를 학습할 수 있게 해줌\n",
    "\n",
    "# 주요 활성화 함수\n",
    "# 함수명 / 특징 / 단점\n",
    "# Sigmoid / 출력값을 0 ~ 1 로 제한 / 기울기 소실, 출력 범위 좁음\n",
    "# Tanh / 출력값 -1 ~ 1, 평균 0 / 기울기 소실\n",
    "# ReLU / 빠른 계산, 좋은 성능 / 음수 입력 시 뉴런 증발\n",
    "# Leaky ReLU / 음수도 일부 통과 / 학습 안정성 향상\n",
    "# Softmax / 다중 클래스 확률 표현 / 분류 문제에서만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 활성화 함수 시각화\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.1 * x)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, sigmoid, label='Sigmoid')\n",
    "plt.plot(x, tanh, label='Tanh')\n",
    "plt.plot(x, relu, label='Relu')\n",
    "plt.plot(x, leaky_relu, label='Leaky relu')\n",
    "plt.legend()\n",
    "plt.title('Activation Functions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1) ReLU 와 Leaky ReLU 를 그래프로 비교\n",
    "'''\n",
    "ReLU 에서 음수를 0으로 만듦으로 인해 뉴런이 죽음\n",
    "Leaky ReLU 에서는 음수를 0.1배 값으로 보존 시킴으로 인해 뉴런이 죽음을 방지, 그로 인해 표현이 확장됨\n",
    "'''\n",
    "x = np.linspace(-10, 10, 100)\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.1 * x)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, relu, label=\"ReLU\")\n",
    "plt.plot(x, leaky_relu, label=\"Leaky ReLU\", linestyle='--')\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.title('ReLU vs Leaky ReLU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2) Sigmoid 함수의 기울기 소실 문제가 왜 발생하는지 설명\n",
    "'''\n",
    "입력값을 적당한 범위로 만들어주는 정규화/표준화를 거치지 않거나,\n",
    "초기 가중치, 편향이 너무 작거나 크면 기울기 소실이 발생할 수 있음 (더이상 학습이 되지 않음)\n",
    "\n",
    "또한 너무 많은 학습이 필요한 경우에는 네트워크가 깊어지면서 각 층의 활성화 함수 입력값들이 반복적으로 곱해지고 누적되어 초기 입력값이 적절했음에도 불구하고 함수의 미분값(기울기)이 0에 가까워져 기울기 소실이 발생함\n",
    "'''\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "sigmoid_prime = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid_prime(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('sigmoid Gradient')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# 중심부(x=0) - sigmoid의 기울기가 최대값, 가장민감 반응\n",
    "# 양극단(x<<0, x>>0) - 출력이 0,1에 매우 가까움, 기울기는 0\n",
    "\n",
    "# 3) 다중 클래스 분류에서 Softmax 함수가 필요한 이유를 설명\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "exps = np.exp(logits)\n",
    "softmax = exps / np.sum(exps)\n",
    "print('Softmax 출력:', softmax)\n",
    "print('총합:', np.sum(softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. HyperParameter Tuning & Optimizer 비교\n",
    "\n",
    "# DeepLearning 에 해당하는 Parameter, HyperParameter\n",
    "\n",
    "# Parameter(매개변수)\n",
    "'''\n",
    "주어진 데이터로부터 학습을 통해 모델 내부에서 결정되는 변수\n",
    "1) 매개변수 최적화\n",
    "학습 모델과 실제 레이블의 차이는 손실 함수로 표현되고, 학습의 목적은 오차와 손실 함수의 값을 최대한 작게 하도록 하는 매개변수(가중치, 편향)를 찾는 것임\n",
    "2) 매개변수 종류\n",
    "가중치(Weight) - 각 입력값에 각기 다르게 곱해지는 수치\n",
    "편향(Bias) - 하나의 뉴런에 입력된 모든 값을 다 더한 값(가중합)에 더해주는 상수\n",
    "3) 매개변수 최적화 과정\n",
    "x축에는 가중치, y축에는 손실값을 갖는 2차원 손실 함수 그래프를 이용하여 최적화 시킴\n",
    "'''\n",
    "\n",
    "# HyperParameter\n",
    "'''\n",
    "모델 외부에서 사람이 설정해주는 값, 성능에 큰 영향 미침\n",
    "1) Learning Rate - 한 번 업데이트 시 가중치 변경 크기\n",
    "2) Batch Size - 한 번에 학습할 데이터 개수\n",
    "3) Epoch - 전체 데이터를 학습시키는 반복 횟수\n",
    "4) Optimizer - 손실 함수를 최소화하는 알고리즘 (Adam, SGD, ...)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2-2 기본학습\n",
    "\n",
    "# 파라미터 & 하이퍼파라미터\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 생성 (이진 분류용)\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_classes=2, random_state=42)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# 상위 5개 샘플\n",
    "print(\"\\nX 샘플:\")\n",
    "print(pd.DataFrame(X).head())\n",
    "print(\"\\ny 샘플:\")\n",
    "print(pd.DataFrame(y).head())\n",
    "\n",
    "# 라벨 분포\n",
    "print(f\"\\n라벨 분포:\\n{pd.Series(y).value_counts()}\")\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"train_data Size: {X_train_scaled.shape}, {y_train.shape}\")\n",
    "print(f\"test_data Size: {X_test_scaled.shape}, {y_test.shape}\")\n",
    "\n",
    "# 3) 하이퍼 파라미터: 입력 형태 정의\n",
    "# 1) 입력층 (특성 수)\n",
    "# 2) 은닉층 (노드 수, activation)\n",
    "# 3) 출력층 (노드 수, activation)\n",
    "dl = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 4) 컴파일 (하이퍼 파라미터 포함)\n",
    "dl.compile(\n",
    "  optimizer=Adam(learning_rate=0.01),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5) 모델 구조 요약 (파라미터 수 확인)\n",
    "print(\"\\n모델 summary()\")\n",
    "dl.summary()\n",
    "\n",
    "# 6) 개별 층의 파라미터 수 출력\n",
    "for i, layer in enumerate(dl.layers):\n",
    "  print(f\"Layer{i+1} ({layer.name}): {layer.count_params():,} parameters\")\n",
    "\n",
    "# 7) 모델 구조 시각화\n",
    "plot_model(\n",
    "  dl,\n",
    "  to_file='start_hyper.png',\n",
    "  show_shapes=True,\n",
    "  show_layer_names=True,\n",
    "  dpi=100\n",
    ")\n",
    "print(\"모델 구조 이미지가 'start_hyper.png'로 저장되었습니다.\\n\")\n",
    "\n",
    "# 8) 모델 학습 (하이퍼 파라미터: epochs, batch_size, validation_split, verbose)\n",
    "history = dl.fit(\n",
    "  X_train_scaled, y_train,\n",
    "  epochs=10,\n",
    "  batch_size=32,\n",
    "  validation_split=0.2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "# 9) 모델 평가\n",
    "loss, acc = dl.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\n딥러닝 정확도: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2-3 HyperParameter 연습\n",
    "\n",
    "# 1) 피마 인디언의 당뇨병 예측\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# 피마 인디언 당뇨병 데이터셋 불러오기\n",
    "data = pd.read_csv('pima-indians-diabetes3.csv')\n",
    "\n",
    "# 데이터 전처리\n",
    "# X = data.drop('diabetes', axis=1)\n",
    "# y = data['diabetes']\n",
    "X = data.iloc[:, 0:8]\n",
    "y = data.iloc[:, 8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(12, activation='relu'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "  X_train_scaled, y_train,\n",
    "  epochs=100,\n",
    "  batch_size=4,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "# 모델 평가\n",
    "_, acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(\"\\n정확도:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2-3 HyperParameter 연습\n",
    "\n",
    "# 2) RandomSearchCV\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 데이터 준비\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 하이퍼 파라미터 후보 설정\n",
    "param_dist = {\n",
    "  'n_estimators': [50, 100, 200],\n",
    "  'max_depth': [3, 5, 10, None],\n",
    "  'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# 모델 생성\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# 랜덤 서치 실행\n",
    "random_search = RandomizedSearchCV(\n",
    "  rf,\n",
    "  param_distributions=param_dist,\n",
    "  n_iter=10,\n",
    "  cv=3\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"최적 하이퍼파라미터:\", random_search.best_params_)\n",
    "print(\"최고 정확도:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2-3 HyperParameter 연습\n",
    "\n",
    "# 3) GridSearchCV\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 하이퍼파라미터 후보 리스트 정의\n",
    "param_dist = {\n",
    "  'n_estimators': [50, 100, 200],\n",
    "  'max_depth': [3, 5, 10, None],\n",
    "  'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# 모델 생성 (RandomForestClassifier)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# GridSearchCV 정의\n",
    "grid_search = GridSearchCV(\n",
    "  rf,\n",
    "  param_grid=param_dist,\n",
    "  n_jobs=-1,\n",
    "  cv=3,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n최적 하이퍼파라미터:\", grid_search.best_params_)\n",
    "print(\"최고 정확도:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-3 실습1\n",
    "\n",
    "# iris 데이터셋으로 최적화 기법 비교\n",
    "\n",
    "# 데이터 - iris (4개의 특성, 3개의 클래스)\n",
    "# 모델 - Dense(64) - Dense(32) - Dense(3)\n",
    "# Optimizer - SGD, SGD+Momentum, RMSprop, Adam\n",
    "# 평가 기준 - Accuracy, Loss, 학습 시간\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizers = {\n",
    "  'SGD': SGD(),\n",
    "  'SGD+Momentum': SGD(momentum=0.9),\n",
    "  'RMSprop': RMSprop(),\n",
    "  'Adam': Adam()\n",
    "}\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "# Optimizer 별 반복 학습\n",
    "for name, opt in optimizers.items():\n",
    "  print(f\"Optimizer: {name} 학습 시작\")\n",
    "  \n",
    "  # 모델 정의\n",
    "  model = Sequential([\n",
    "    Input(shape=(X.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "  ])\n",
    "  \n",
    "  model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "  )\n",
    "  \n",
    "  start = time.time() # Traning time 측정을 위한 타이머 시작 값\n",
    "  \n",
    "  history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    validation_data=(X_test_scaled, y_test)\n",
    "  )\n",
    "  \n",
    "  end = time.time()\n",
    "  \n",
    "  # val_loss, val_accuracy\n",
    "  # 모델 비교에 사용\n",
    "  # 조기종료(EarlyStop)에 사용 fit의 callbacks 하이퍼파라미터 값에 리스트 형태로 사용\n",
    "  \n",
    "  # ex) \n",
    "  # from tensorflow.keras.callbacks import EarlyStopping\n",
    "  \n",
    "  # early_stop = EarlyStopping(\n",
    "  #   monitor='val_loss',\n",
    "  #   # 모니터링할 값\n",
    "  \n",
    "  #   patience=3,\n",
    "  #   # 몇 번까지 참고 기다릴지\n",
    "  \n",
    "  #   restore_best_weights=True\n",
    "  #   # 가장 성능 좋았던 가중치로 복원\n",
    "  # )    \n",
    "  \n",
    "  # history = model.fit(callbacks=[early_stop])\n",
    "  \n",
    "  val_loss = history.history['val_loss'][-1]\n",
    "  val_acc = history.history['val_accuracy'][-1]\n",
    "  \n",
    "  results.append({\n",
    "    \"Optimizer\": name,\n",
    "    \"Final Loss\": round(val_loss, 4),\n",
    "    \"Final Accuracy\": round(val_acc, 4),\n",
    "    \"Traning Time(s)\": round(end - start, 2)\n",
    "  })\n",
    "\n",
    "# 결과 정리 및 출력\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\nOptimizer 비교 결과 (iris):\\n{df}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-3 실습2\n",
    "\n",
    "# Optimizer 비교 실험 (MNIST)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train / X_train.max(), X_test / X_train.max()\n",
    "\n",
    "optimizers = {\n",
    "  'SGD': SGD(),\n",
    "  'Adam': Adam(),\n",
    "  'RMSprop': RMSprop()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, opt in optimizers.items():\n",
    "  print(f\"\\nTraining with {name} optimizer...\")\n",
    "  \n",
    "  model = Sequential([\n",
    "    Input(shape=(28, 28)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "  ])\n",
    "  \n",
    "  model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "  )\n",
    "  \n",
    "  history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    "  )\n",
    "  \n",
    "  loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "  results[name] = acc\n",
    "  \n",
    "print(\"\\n[테스트 결과]\")\n",
    "for opt_name, acc in results.items():\n",
    "  print(f\"{opt_name} 테스트 정확도: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 과적합과 일반화 + Dropout 실습\n",
    "\n",
    "# 과적합(Overfitting)\n",
    "# 모델이 학습 데이터에는 잘 맞지만, 테스트 데이터에는 성능이 떨어지는 현상\n",
    "# 모델이 너무 복잡하거나, 학습이 너무 오래되었을 때\n",
    "\n",
    "# 일반화(Generalization)\n",
    "# 모델이 새로운 데이터에도 잘 작동하는 능력\n",
    "# 일반화를 위해 모델이 너무 외우지 않게끔 훈련해야 함\n",
    "\n",
    "# 과소적합(Underfitting)\n",
    "# 모델이 학습 데이터와 테스트 데이터 모두에서 성능이 안좋은 현상\n",
    "# 모델이 데이터에 대하여 충분한 학습을 못해 데이터셋의 일반적인 패턴을 다 찾아내지 못한 것이 원인\n",
    "\n",
    "# 과적합 방지 기법\n",
    "# Dropout: 학습 중 일부 뉴런을 랜덤하게 제거해 과적합 방지\n",
    "# EarlyStopping: 검증 성능이 나빠지면 조기 종료\n",
    "# Data Augmentation: 학습 데이터를 늘려서 더 다양한 케이스를 학습\n",
    "# 교차 검증: 학습 데이터의 일불ㄹ 따로 모델 성능 검증용으로 사용하는 기법\n",
    "# 정규화: L1, L2 정규화 및 Dropout을 통해 모델의 복잡성을 제한\n",
    "# 모델 단순화: 파라미터 수를 줄이거나 덜 복잡한 구조의 모델을 사용하여 과적합 가능성을 낮춤\n",
    "# Ensemble Methods: 여러 모델의 예측 결과를 결합하여 개별 모델의 약점을 보완하고 과적합을 방지\n",
    "# 충분한 데이터 확보: 더 많은 데이터를 수집하면 모델이 일반적인 패턴을 보다 잘 학습할 수 있어 과적합 가능성을 줄일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-2 실습\n",
    "\n",
    "# Dropout과 EarlyStopping 적용\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 준비\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential([\n",
    "  Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# 콜백 설정\n",
    "early_stop = EarlyStopping(\n",
    "  monitor='val_loss',\n",
    "  patience=3,\n",
    "  restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 컴파일 및 학습\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "  X_train, y_train,\n",
    "  epochs=30,\n",
    "  batch_size=32,\n",
    "  callbacks=[early_stop],\n",
    "  validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# 평가\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print(\"정확도:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-3 과제\n",
    "\n",
    "# 1) 작은 데이터셋으로 과적합 실험\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 데이터 준비 (mnist 에서 일부만 사용)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train[:1000] / 255.0\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# 모델 정의 (Dropout 없음)\n",
    "model = Sequential([\n",
    "  Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "  Flatten(),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 학습\n",
    "history = model.fit(\n",
    "  X_train, y_train,\n",
    "  epochs=30,\n",
    "  batch_size=32,\n",
    "  validation_data=(X_test, y_test),\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title(\"과적합 현상 관찰\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-3 과제\n",
    "\n",
    "# 2) Dropout 적용 비교 실험\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 데이터 준비 (mnist 에서 일부만 사용)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train[:1000] / 255.0\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# 모델 정의 (Dropout 없음)\n",
    "model = Sequential([\n",
    "  Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "  Flatten(),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "model_dropout = Sequential([\n",
    "  Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "  Flatten(),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model_dropout.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 학습\n",
    "history = model.fit(\n",
    "  X_train, y_train,\n",
    "  epochs=30,\n",
    "  batch_size=32,\n",
    "  validation_data=(X_test, y_test),\n",
    "  verbose=1\n",
    ")\n",
    "history_d = model_dropout.fit(\n",
    "  X_train, y_train,\n",
    "  epochs=30,\n",
    "  batch_size=32,\n",
    "  validation_data=(X_test, y_test),\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='Without Dropout')\n",
    "plt.plot(history_d.history['val_accuracy'], label='With Dropout')\n",
    "plt.title(\"Dropout 효과 비교\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-3 과제\n",
    "\n",
    "# 3) EarlyStopping + Dropout 으로 일반화 성능 향상\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "  Input(shape=(28, 28)),\n",
    "  Flatten(),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "  monitor='val_loss',\n",
    "  patience=3,\n",
    "  restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "  X_train, y_train,\n",
    "  epochs=30,\n",
    "  batch_size=32,\n",
    "  verbose=1,\n",
    "  validation_data=(X_test, y_test),\n",
    "  callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(f\"\\n테스트 정확도: {model.evaluate(X_test, y_test)[1]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
