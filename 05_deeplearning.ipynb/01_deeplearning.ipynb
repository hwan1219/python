{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 딥러닝 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI (인공지능) - 가장 넓은 개념\n",
    "# 정의: 인간의 지능을 모방하여 문제를 해결하는 기술\n",
    "# 예시: 체스 AI, 음성비서, 자율주행\n",
    "# 사용기술: 규칙 기반 시스템, 딥러닝, 머신러닝 포함\n",
    "\n",
    "# ML (머신러닝) - AI 의 하위 개념\n",
    "# 정의: 데이터를 이용해 스스로 학습하는 알고리즘\n",
    "# 예시: 스팸 메일 분류, 추천 시스템\n",
    "# 사용기술: 지도학습, 비지도학습, 강화학습\n",
    "\n",
    "# DL (딥러닝) - ML 의 하위 개념\n",
    "# 정의: 인공신경망을 통해 특징을 자동으로 학습하는 기술\n",
    "# 예시: 얼굴인식, 음성인식, 번역\n",
    "# 사용 기술: CNN, RNN, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 실습\n",
    "# 머신러닝 vs 딥러닝\n",
    "\n",
    "# 머신러닝: 특징을 사람이 직접 추출 → 학습\n",
    "# 딥러닝: 특징 추출도 신경망이 자동으로 → 학습\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 1) 데이터 불러오기\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 2) 훈련/테스트 나누기\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3) 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "# 4) 머신러닝 모델 (Random Forest)\n",
    "# 정수형 예측이므로 One-Hot Encoding X\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# 5) 딥러닝 모델 (Dense NN)\n",
    "# to_categorical = 딥러닝 모듈의 내장 One-Hot Encoding\n",
    "y_train_cat = to_categorical(y_train, 3)\n",
    "y_test_cat = to_categorical(y_test, 3)\n",
    "\n",
    "dl_model = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(3, activation='softmax')\n",
    "])\n",
    "dl_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "dl_model.fit(X_train, y_train_cat, epochs=30, verbose=0)\n",
    "\n",
    "# 6) 평가\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "_, dl_acc = dl_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# 7) 결과 비교\n",
    "result_df = pd.DataFrame({\n",
    "  '모델 종류': ['Random Forest (scikit-learn)', 'Neural Network (keras)'],\n",
    "  '정확도': [rf_acc, dl_acc]\n",
    "})\n",
    "print(\"[머신러닝과 딥러닝 비교 결과]\")\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3 과제1\n",
    "# 머신러닝 vs 딥러닝 dataset: wine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 데이터 불러오기\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# 훈련/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 머신러닝 모델 (RandomForest)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# 딥러닝 모델 (Keras)\n",
    "# 1)전처리: scaling + One-Hot Encoding(to_categorical)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "# 2)모델 정의\n",
    "dl = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "# 3)모델 컴파일 및 훈련\n",
    "dl.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "dl.fit(X_train_scaled, y_train_cat, epochs=30, verbose=1)\n",
    "\n",
    "# 평가\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "_, dl_acc = dl.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "\n",
    "# 결과 비교 출력\n",
    "result_df = pd.DataFrame({\n",
    "  '모델 종류': ['Random Forest (scikit-learn)', 'Neural Network (keras)'],\n",
    "  '정확도': [rf_acc, dl_acc]\n",
    "})\n",
    "print(\"[머신러닝과 딥러닝 비교 결과]\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 신경망의 구조\n",
    "\n",
    "# 1) 퍼셉트론 (Perceptron)\n",
    "# 인공 신경망의 기본 단위\n",
    "# 입력값에 가중치를 곱하고 합산 후, 활성화 함수로 출력을 만듦\n",
    "# 입렵값과 가중치가 선형 결합됨\n",
    "# 여기에 편향 bbb를 더하고, 활성화 함수 fff를 거쳐 결과 yyy출력\n",
    "# 가장 기본적인 신경만 단위로 AND, OR 논리 게이트 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 실습1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1) 입력데이터 (AND 연산)\n",
    "# 각 행은 [x1, x2]\n",
    "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "# 2. 정답 레이블 (AND 연산의 결과)\n",
    "# 0 AND 0 = 0,\n",
    "# 0 AND 1 = 0,\n",
    "# 1 AND 0 = 0,\n",
    "# 1 AND 1 = 1\n",
    "y = np.array([0,0,0,1])\n",
    "\n",
    "# 3. 퍼셉트론 파라미터 초기화 (y=wx+b)\n",
    "# 가중치 2개(w1, w2), 편향 b\n",
    "w = np.random.rand(2)\n",
    "b = np.random.rand()\n",
    "\n",
    "# 4. 학습률 (Learning Rate)\n",
    "# 모델이 얼마나 빠르게 학습할 지 속도 조절\n",
    "# 보통 0.01 ~ 0.1 설정 (추후 재확인)\n",
    "lr = 0.1\n",
    "\n",
    "# 5. 활성화 함수 (계단 함수 - step function)\n",
    "def step(x):\n",
    "  return 1 if x >= 0 else 0\n",
    "\n",
    "# 6. 학습 반복\n",
    "for epoch in range(20):\n",
    "  print(f\"\\nEPoch {epoch+1}\")\n",
    "  for i in range(len(x)):\n",
    "    z = np.dot(x[i], w) + b # z = w1*x1 + w2*x2 + b\n",
    "    y_pred = step(z)\n",
    "    error = y[i] - y_pred # 오차 = 정답 - 예측값\n",
    "    w = w + lr * error * x[i]\n",
    "    b = b + lr * error\n",
    "    print(f\"x:{x[i]}, y:{y[i]}, pred:{y_pred}, w:{w}, b:{b}\")\n",
    "\n",
    "for x_input in x:\n",
    "  z = np.dot(x_input, w) + b\n",
    "  y_output = step(z)\n",
    "  print(f\"입력: {x_input} -> 출력: {y_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제1\n",
    "# 퍼셉트론 논리 게이트 구현 (AND, OR)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def perceptron(x1, x2, w1, w2, b):\n",
    "  x = np.array([x1, x2])\n",
    "  w = np.array([w1, w2])\n",
    "  output = np.dot(w, x) + b\n",
    "  return 1 if output > 0 else 0\n",
    "\n",
    "# AND 게이트\n",
    "print(\"AND 게이트 결과:\") \n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],0.5,0.5,-0.7)}\")\n",
    "  \n",
    "# OR 게이트\n",
    "print(\"\\nOR 게이트 결과:\")\n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],1,1,-0.7)}\")\n",
    "  \n",
    "# NAND 게이트\n",
    "print(\"\\nNAND 게이트 결과:\")\n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],-0.5,-0.5,0.7)}\")\n",
    "  \n",
    "# XOR 게이트 - Multi Perceptron (다층 퍼셉트론) 사용해야함\n",
    "print(\"\\nXOR 게이트 결과(Multi Perceptron 사용해야함):\")\n",
    "for x in [(0,0),(0,1),(1,0),(1,1)]:\n",
    "  print(f\"{x}: {perceptron(x[0],x[1],0.5,0.5,-0.7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 실습2\n",
    "# MLP (Multi-Layer Perceptron)은 입력층 → 은닉층 → 출력층 구조로 구성되어 있고, 단일 퍼셉트론과 달리 비선형 문제도 해결 가능\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# XOR 을 MLP 에서 동작 확인하기\n",
    "\n",
    "# 1) xor\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]]) # 입력\n",
    "y = np.array([[0], [1], [1], [0]]) # 정답\n",
    "# y_pred = 예측값\n",
    "# error(손실,loss) = 정답 - y_pred\n",
    "\n",
    "# 2) model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(2,)))\n",
    "model.add(Dense(4, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3) compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4) 모델 학습\n",
    "model.fit(X, y, epochs=500, verbose=1)\n",
    "# verbose(학습과정 출력)\n",
    "# 0 = 아무것도 출력하지 않음\n",
    "# 1 = 진행 상황을 1줄씩(epoch별) 출력\n",
    "# 2 = 1과 비슷하지만 조금 더 간략하게 출력\n",
    "\n",
    "# 예측 결과 출력\n",
    "predictions = model.predict(X)\n",
    "print(\"입력값\\t 예측값\\t 라운딩\")\n",
    "for i, pred in enumerate(predictions):\n",
    "  print(f\"{X[i]} → {pred[0]:.4f} → {np.round(pred[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# XOR 을 MLP 에서 동작 확인하기\n",
    "\n",
    "# 입력 데이터 (XOR 문제)\n",
    "X = np.array([\n",
    "  [0,0],\n",
    "  [0,1],\n",
    "  [1,0],\n",
    "  [1,1]\n",
    "])\n",
    "\n",
    "# XOR 의 정답\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "\n",
    "# 입력 데이터 형태를 지정\n",
    "model.add(Input(shape=(2,)))\n",
    "\n",
    "# 은닉층: 2개의 뉴런(노드), 활성화 함수: relu\n",
    "model.add(Dense(2, activation='relu'))\n",
    "\n",
    "# 출력층: 1개의 뉴런(노드), 활성화 함수: sigmoid(이진 분류)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(\n",
    "  optimizer=SGD(learning_rate=0.1),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "# verbose(학습과정 출력)\n",
    "# 0 = 아무것도 출력하지 않음\n",
    "# 1 = 진행 상황을 1줄씩(epoch별) 출력\n",
    "# 2 = 1과 비슷하지만 조금 더 간략하게 출력\n",
    "\n",
    "# 모델 평가\n",
    "loss, acc = model.evaluate(X, y, verbose=0)\n",
    "print(f\"최종 정확도: {acc:.2f}\")\n",
    "\n",
    "# 예측 결과 출력\n",
    "pred = model.predict(X)\n",
    "print(f\"\\n예측 결과 (확률값):\\n{pred.round(3)}\")\n",
    "print(f\"\\n예측 결과 (이진 출력):\\n{(pred > 0.5).astype(int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 2 - 과제2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제2-1\n",
    "# NAND 게이트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제2-2\n",
    "# XOR 게이트가 퍼셉트론으로 구현 안되는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 과제2-3\n",
    "# MLP 가 XOR 문제를 해결할 수 있는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 2 - 과제2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning 보충학습\n",
    "# 단순 신경망 훈련(선형회귀)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# 샘플 데이터셋 생성\n",
    "x = np.arange(1,6).reshape(-1,1)\n",
    "y = 3*x + 2\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# 시각화\n",
    "plt.plot(x, y)\n",
    "plt.title(\"y = 3x + 2\")\n",
    "plt.show()\n",
    "\n",
    "# 단순선형회귀 모델 생성, 구조\n",
    "\n",
    "dl = Sequential([\n",
    "  Input(shape=(1,)),\n",
    "  Dense(1)\n",
    "])\n",
    "dl.summary()\n",
    "\n",
    "# 컴파일, 훈련\n",
    "dl.compile(\n",
    "  optimizer='sgd',\n",
    "  loss='mse',\n",
    "  metrics=['mae']\n",
    ")\n",
    "history = dl.fit(x, y, epochs=1200, verbose=0)\n",
    "\n",
    "# 20 에포크까지 Loss 수렴에 대한 시각화\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['mae'], label='mae')\n",
    "plt.xlim(-1, 20)\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 검증\n",
    "loss, mae = dl.evaluate(x, y, verbose=0)\n",
    "print(loss, mae)\n",
    "\n",
    "# 예측\n",
    "# x=10 → y=?\n",
    "result = dl.predict(np.array([[10]]))\n",
    "print(\"예측 결과:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 역전파(Backpropagation) 와 학습 \n",
    "\n",
    "# 손실 함수 (Loss Function)\n",
    "# 모델의 예측값과 실제값 사이의 오차 정도를 수치화\n",
    "# 회귀 문제: MSE (Mean Squared Error)\n",
    "# 분류 문제: Cross Entropy Loss\n",
    "\n",
    "# 역전파 알고리즘\n",
    "# 오차를 기준으로 가중치를 거꾸로 전파하여 수정하는 알고리즘\n",
    "# Chain Rule 을 이용하여 각 층의 가중치에 대한 기울기 계산\n",
    "\n",
    "# 학습률 (Learning Rate)\n",
    "# 가중치를 얼마나 크게 수정할 것인지 결정하는 하이퍼파라미터\n",
    "# 너무 크면 발산, 너무 작으면 학습 속도 저하"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2 실습\n",
    "# 간단한 손실 함수와 그래디언트 계산\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# MSE 손실 함수 (Mean Square Error) - 평균제곱오차\n",
    "def mse_loss(y_true, y_pred):\n",
    "  return np.mean((y_true-y_pred)**2)\n",
    "\n",
    "y_true = np.array([1, 0, 1]) # 정답\n",
    "y_pred = np.array([0.8, 0.2, 0.6]) # 예측\n",
    "\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "print(\"손실값(MSE):\", loss)\n",
    "\n",
    "# 가중치에 대한 그래디언트(기울기) 계산 예시 (y=ax+b)\n",
    "x = np.array([1.0, 2.0])\n",
    "w = np.array([0.5, -0.5])\n",
    "y = 1.0 # 정답\n",
    "\n",
    "y_pred = np.dot(w, x)\n",
    "error = y_pred - y\n",
    "gradient = 2 * error * x\n",
    "print(\"Gradient(기울기):\", gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-3 과제\n",
    "# 1) MSE 와 Cross Entropy 의 차이를 서술\n",
    "'''\n",
    "MSE = 회귀에 사용,\n",
    "      연속형 값의 오차를 측정,\n",
    "      큰 오차에 민감\n",
    "\n",
    "Cross Entropy = 분류에 사용,\n",
    "                확률값 기반으로 분류 평가\n",
    "                잘못된 확률에 민감\n",
    "'''\n",
    "\n",
    "# 2) y_true = [1, 0, 1], y_pred = [0.9, 0.3, 0.7]\n",
    "#    해당 데티어에서의 손실값(MSE)을 계산\n",
    "import numpy as np\n",
    "y_true = np.array([1, 0, 1])\n",
    "y_pred = np.array([0.9, 0.3, 0.7])\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "print(\"MSE손실값:\", mse)\n",
    "\n",
    "# 3) 학습률이 너무 큰 경우와 너무 작은 경우의 문제점을 설명\n",
    "'''\n",
    "큰경우: 손실값이 수렴하지 않고 발산할 수 있으며, 최적값 근처에서 오락가락할 수 있음\n",
    "* 발산: 손실값이 점점 줄어들어야 하는데 오히려 점점 커지는 현상을 말함\n",
    "\n",
    "작은경우: 학습 속도가 매우 느려지고, 최적값에 도달하기까지 시간이 오래걸림\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 활성화 함수\n",
    "# 뉴런에서 입력값의 합을 비선형적으로 변환하여 다음 층으로 전달하는 함수\n",
    "# 모델이 비선형 문제를 학습할 수 있게 해줌\n",
    "\n",
    "# 주요 활성화 함수\n",
    "# 함수명 / 특징 / 단점\n",
    "# Sigmoid / 출력값을 0 ~ 1 로 제한 / 기울기 소실, 출력 범위 좁음\n",
    "# Tanh / 출력값 -1 ~ 1, 평균 0 / 기울기 소실\n",
    "# ReLU / 빠른 계산, 좋은 성능 / 음수 입력 시 뉴런 증발\n",
    "# Leaky ReLU / 음수도 일부 통과 / 학습 안정성 향상\n",
    "# Softmax / 다중 클래스 확률 표현 / 분류 문제에서만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 활성화 함수 시각화\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.1 * x)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, sigmoid, label='Sigmoid')\n",
    "plt.plot(x, tanh, label='Tanh')\n",
    "plt.plot(x, relu, label='Relu')\n",
    "plt.plot(x, leaky_relu, label='Leaky relu')\n",
    "plt.legend()\n",
    "plt.title('Activation Functions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1) ReLU 와 Leaky ReLU 를 그래프로 비교\n",
    "'''\n",
    "ReLU 에서 음수를 0으로 만듦으로 인해 뉴런이 죽음\n",
    "Leaky ReLU 에서는 음수를 0.1배 값으로 보존 시킴으로 인해 뉴런이 죽음을 방지, 그로 인해 표현이 확장됨\n",
    "'''\n",
    "x = np.linspace(-10, 10, 100)\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.1 * x)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, relu, label=\"ReLU\")\n",
    "plt.plot(x, leaky_relu, label=\"Leaky ReLU\", linestyle='--')\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.title('ReLU vs Leaky ReLU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2) Sigmoid 함수의 기울기 소실 문제가 왜 발생하는지 설명\n",
    "'''\n",
    "입력값을 적당한 범위로 만들어주는 정규화/표준화를 거치지 않거나,\n",
    "초기 가중치, 편향이 너무 작거나 크면 기울기 소실이 발생할 수 있음 (더이상 학습이 되지 않음)\n",
    "\n",
    "또한 너무 많은 학습이 필요한 경우에는 네트워크가 깊어지면서 각 층의 활성화 함수 입력값들이 반복적으로 곱해지고 누적되어 초기 입력값이 적절했음에도 불구하고 함수의 미분값(기울기)이 0에 가까워져 기울기 소실이 발생함\n",
    "'''\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "sigmoid_prime = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid_prime(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('sigmoid Gradient')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# 중심부(x=0) - sigmoid의 기울기가 최대값, 가장민감 반응\n",
    "# 양극단(x<<0, x>>0) - 출력이 0,1에 매우 가까움, 기울기는 0\n",
    "\n",
    "# 3) 다중 클래스 분류에서 Softmax 함수가 필요한 이유를 설명\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "exps = np.exp(logits)\n",
    "softmax = exps / np.sum(exps)\n",
    "print('Softmax 출력:', softmax)\n",
    "print('총합:', np.sum(softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. HyperParameter Tuning & Optimizer 비교\n",
    "\n",
    "# DeepLearning 에 해당하는 Parameter, HyperParameter\n",
    "\n",
    "# Parameter(매개변수)\n",
    "'''\n",
    "주어진 데이터로부터 학습을 통해 모델 내부에서 결정되는 변수\n",
    "1) 매개변수 최적화\n",
    "학습 모델과 실제 레이블의 차이는 손실 함수로 표현되고, 학습의 목적은 오차와 손실 함수의 값을 최대한 작게 하도록 하는 매개변수(가중치, 편향)를 찾는 것임\n",
    "2) 매개변수 종류\n",
    "가중치(Weight) - 각 입력값에 각기 다르게 곱해지는 수치\n",
    "편향(Bias) - 하나의 뉴런에 입력된 모든 값을 다 더한 값(가중합)에 더해주는 상수\n",
    "3) 매개변수 최적화 과정\n",
    "x축에는 가중치, y축에는 손실값을 갖는 2차원 손실 함수 그래프를 이용하여 최적화 시킴\n",
    "'''\n",
    "\n",
    "# HyperParameter\n",
    "'''\n",
    "모델 외부에서 사람이 설정해주는 값, 성능에 큰 영향 미침\n",
    "1) Learning Rate - 한 번 업데이트 시 가중치 변경 크기\n",
    "2) Batch Size - 한 번에 학습할 데이터 개수\n",
    "3) Epoch - 전체 데이터를 학습시키는 반복 횟수\n",
    "4) Optimizer - 손실 함수를 최소화하는 알고리즘 (Adam, SGD, ...)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2-2 기본학습\n",
    "\n",
    "# 파라미터 & 하이퍼파라미터\n",
    "\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 생성 (이진 분류용)\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_classes=2, random_state=42)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# 상위 5개 샘플\n",
    "print(\"\\nX 샘플:\")\n",
    "print(pd.DataFrame(X).head())\n",
    "print(\"\\ny 샘플:\")\n",
    "print(pd.DataFrame(y).head())\n",
    "\n",
    "# 라벨 분포\n",
    "print(f\"\\n라벨 분포:\\n{pd.Series(y).value_counts()}\")\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"train_data Size: {X_train_scaled.shape}, {y_train.shape}\")\n",
    "print(f\"test_data Size: {X_test_scaled.shape}, {y_test.shape}\")\n",
    "\n",
    "# 3) 하이퍼 파라미터: 입력 형태 정의\n",
    "# 1) 입력층 (특성 수)\n",
    "# 2) 은닉층 (노드 수, activation)\n",
    "# 3) 출력층 (노드 수, activation)\n",
    "dl = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 4) 컴파일 (하이퍼 파라미터 포함)\n",
    "dl.compile(\n",
    "  optimizer=Adam(learning_rate=0.01),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5) 모델 구조 요약 (파라미터 수 확인)\n",
    "print(\"\\n모델 summary()\")\n",
    "dl.summary()\n",
    "\n",
    "# 6) 개별 층의 파라미터 수 출력\n",
    "for i, layer in enumerate(dl.layers):\n",
    "  print(f\"Layer{i+1} ({layer.name}): {layer.count_params():,} parameters\")\n",
    "\n",
    "# 7) 모델 구조 시각화\n",
    "plot_model(\n",
    "  dl,\n",
    "  to_file='start_hyper.png',\n",
    "  show_shapes=True,\n",
    "  show_layer_names=True,\n",
    "  dpi=100\n",
    ")\n",
    "print(\"모델 구조 이미지가 'start_hyper.png'로 저장되었습니다.\\n\")\n",
    "\n",
    "# 8) 모델 학습 (하이퍼 파라미터: epochs, batch_size, validation_split, verbose)\n",
    "history = dl.fit(\n",
    "  X_train_scaled, y_train,\n",
    "  epochs=10,\n",
    "  batch_size=32,\n",
    "  validation_split=0.2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "# 9) 모델 평가\n",
    "loss, acc = dl.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\n딥러닝 정확도: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2-3 HyperParameter 연습\n",
    "\n",
    "# 피마 인디언의 당뇨병 예측\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# 피마 인디언 당뇨병 데이터셋 불러오기\n",
    "data = pd.read_csv('pima-indians-diabetes3.csv')\n",
    "\n",
    "# 데이터 전처리\n",
    "X = data.drop('diabetes', axis=1)\n",
    "y = data['diabetes']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "  Input(shape=(X.shape[1],)),\n",
    "  Dense(12, activation='relu'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "  X_train_scaled, y_train,\n",
    "  epochs=100,\n",
    "  batch_size=4,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "# 모델 평가\n",
    "_, acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(\"\\n정확도:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2-3 HyperParameter 연습\n",
    "\n",
    "# RandomSearchCV\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 데이터 준비\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 하이퍼 파라미터 후보 설정\n",
    "param_dist = {\n",
    "  'n_estimators': [50, 100, 200],\n",
    "  'max_depth': [3, 5, 10, None],\n",
    "  'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
