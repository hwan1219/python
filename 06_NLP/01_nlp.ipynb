{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# 1 ~ 4\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-1 기본 실습\n",
    "\n",
    "# SimpleRNN (기본 순환 구조)\n",
    "'''\n",
    "숫자 시퀀스 예측\n",
    "입력: [1, 2, 3] → 출력: 4\n",
    "입력: [2, 3, 4] → 출력: 5\n",
    "입력: [3, 4, 5] → 출력: 6\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense\n",
    "\n",
    "# 시계열 데이터처럼 연속된 숫자를 기반으로 다음 값을 예측하는 문제\n",
    "\n",
    "# 1. 데이터 준비\n",
    "# 입력 시퀀스 (4개 샘플, 각 샘플은 길이 3인 시퀀스)\n",
    "# 정답 (각 시퀀스의 다음 숫자)\n",
    "def create_data(i):\n",
    "  train_list = []\n",
    "  val_list = []\n",
    "  for j in range(1, i+1):\n",
    "    train = [j, j+1, j+2]\n",
    "    train_list.append(train)\n",
    "    val = j+3\n",
    "    val_list.append(val)\n",
    "  train_list = np.array(train_list)\n",
    "  val_list = np.array(val_list)\n",
    "  return train_list, val_list\n",
    "X, y = create_data(4)\n",
    "\n",
    "# RNN 입력형태로 reshape: (샘플 수, 타임스텝 수, 특성 수)\n",
    "# 숫자 하나씩 보므로 특성(feature)은 1\n",
    "X = X.reshape((-1, 3, 1))\n",
    "\n",
    "# 2. 모델 구성\n",
    "model = Sequential()\n",
    "# Input 레이어로 입력 형태를 명시 (3타임스텝, 1특성)\n",
    "# SimpleRNN 레이어 (유닛 10개, 활성화함수: relu)\n",
    "# Dense 출력층 (출력 노드 1개: 다음 숫자 1개 예측)\n",
    "model.add(Input(shape=(X.shape[1:])))\n",
    "model.add(SimpleRNN(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 3. 모델 컴파일\n",
    "# optimizer = 'adam': 최적화 알고리즘\n",
    "# loss = 'mse': 회귀 문제이므로 평균제곱오차 사용\n",
    "model.compile(\n",
    "  optimizer = 'adam',\n",
    "  loss = 'mse',\n",
    "  metrics = ['mae']\n",
    ")\n",
    "\n",
    "# 4. 모델 학습\n",
    "# epochs = 200: 반복 횟수\n",
    "# verbose = 0: 진행 상황 출력 생략(0)\n",
    "model.fit(\n",
    "  X, y,\n",
    "  epochs = 200,\n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "# 5. 예측\n",
    "# 예측 대상 시퀀스: [5, 6, 7]\n",
    "# 샘플 수 차원 추가 필요\n",
    "test_input = np.array([5, 6, 7]).reshape((1, 3, 1))\n",
    "\n",
    "# 예측 수행\n",
    "predicted = model.predict(test_input, verbose=0)\n",
    "print(\"예측결과:\", predicted[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2 숫자 시퀀스 예측 예제 - SimpleRNN & LSTM & GRU\n",
    "\n",
    "# 시퀀스 [1,2,3] → 4, [2,3,4] → 5 처럼 숫자 패턴을 학습해서 다음 수를 예측하는 간단한 RNN 모델을 만들고, 아래 세 가지 모델을 비교\n",
    "\n",
    "# 모델 성능 해석 (MSE 기준)\n",
    "# 모델 / 예측 정확도 / 특성 요약\n",
    "# SimpleRNN / ex) 0.0152 / 가장 단순, 성능 무난\n",
    "# LSTM / ex) 0.0041 / 장기 의존성 학습 잘함\n",
    "# GRU / ex) 0.0029 / 성능 좋고 빠름\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, LSTM, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. 데이터 준비\n",
    "# 입력 시퀀스 (4개 샘플, 각 샘플은 길이 3인 시퀀스)\n",
    "# 정답 (각 시퀀스의 다음 숫자)\n",
    "def create_data(i):\n",
    "  train_list = []\n",
    "  val_list = []\n",
    "  for j in range(1, i+1):\n",
    "    train = [j, j+1, j+2]\n",
    "    train_list.append(train)\n",
    "    val = j+3\n",
    "    val_list.append(val)\n",
    "  train_list = np.array(train_list)\n",
    "  val_list = np.array(val_list)\n",
    "  return train_list, val_list\n",
    "X, y = create_data(6)\n",
    "\n",
    "# 2. 입력 데이터 shape 정의\n",
    "X = X.reshape((-1, 3, 1))\n",
    "\n",
    "# 3. 모델 생성 함수\n",
    "model_types = [SimpleRNN, LSTM, GRU]\n",
    "results = []\n",
    "\n",
    "def build_model(model_type):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(X.shape[1:])))\n",
    "  model.add(model_type(32, activation='tanh'))\n",
    "  model.add(Dense(1))\n",
    "  \n",
    "  model.compile(\n",
    "  \toptimizer = Adam(0.01),\n",
    "  \tloss = 'mse'\n",
    "\t)\n",
    "  return model\n",
    "\n",
    "# 4. 모델 학습, 예측, MSE 계산\n",
    "for model_type in model_types:\n",
    "  print(f\"\\n{model_type.__name__} 모델 학습 및 예측\")\n",
    "  model = build_model(model_type)\n",
    "  \n",
    "  model.fit(\n",
    "\t\tX, y,\n",
    "\t\tepochs = 100,\n",
    "\t\tverbose = 0\n",
    "\t)\n",
    "  preds = model.predict(X)\n",
    "  mse = mean_squared_error(y, preds)\n",
    "  \n",
    "  for i in range(len(X)):\n",
    "    results.append({\n",
    "      \"모델\": model_type.__name__,\n",
    "      \"입력 시퀀스\": X[i].flatten().tolist(),\n",
    "      \"실제값\": y[i],\n",
    "      \"예측값\": round(preds[i][0], 3),\n",
    "      \"MSE\": round(mse, 4)\n",
    "\t\t})\n",
    "    \n",
    "# 5. 결과 정리 및 출력\n",
    "df = pd.DataFrame(results)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 문장 예측 실습\n",
    "\n",
    "# \"hello\" → 다음 글자 예측하기 (3가지 RNN 비교 예제)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 문자 집합 정의\n",
    "text = \"hello\"\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "print(\"문자-인덱스 딕셔너리 생성\")\n",
    "print(char_to_index)\n",
    "print(index_to_char)\n",
    "\n",
    "# 문자 → 숫자 변환\n",
    "X_data = [char_to_index[c] for c in text[:-1]]\n",
    "y_data = [char_to_index[c] for c in text[1:]]\n",
    "print(\"\\n딕셔너리를 이용한 문자 인덱스화\")\n",
    "print(X_data)\n",
    "print(y_data)\n",
    "\n",
    "# One-Hot Encoding\n",
    "X = to_categorical(X_data, num_classes=len(chars))\n",
    "y = to_categorical(y_data, num_classes=len(chars))\n",
    "\n",
    "# RNN 입력 형식으로 변환: (samples, timesteps, features)\n",
    "X = X.reshape(1, 4, len(chars))\n",
    "y = y.reshape(1, 4, len(chars))\n",
    "\n",
    "# 결과 저장 딕셔너리\n",
    "results = {}\n",
    "\n",
    "# 공통 함수: 모델 생성, 학습 예측, 평가\n",
    "def train_and_predict(model_name, RNNlayer):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(X.shape[1:])))\n",
    "  model.add(RNNlayer(10, return_sequences=True))\n",
    "  model.add(Dense(len(chars), activation='softmax'))\n",
    "  \n",
    "  model.compile(\n",
    "\t\toptimizer = 'adam',\n",
    "\t\tloss = 'categorical_crossentropy'\n",
    "\t)\n",
    "  \n",
    "  # 학습\n",
    "  model.fit(\n",
    "\t\tX, y,\n",
    "\t\tepochs = 500,\n",
    "\t\tverbose = 0\n",
    "\t)\n",
    "  \n",
    "  # 예측\n",
    "  pred = model.predict(X, verbose=0)\n",
    "  \n",
    "  pred_i = np.argmax(pred, axis=2)\n",
    "  true_i = np.argmax(y, axis=2)\n",
    "  \n",
    "  pred_chars = [index_to_char[i] for i in pred_i[0]]\n",
    "  true_chars = [index_to_char[i] for i in true_i[0]]\n",
    "  \n",
    "  mse = mean_squared_error(true_i[0], pred_i[0])\n",
    "  \n",
    "  results[model_name] = {\n",
    "\t\t\"예측값\": pred_chars,\n",
    "\t\t\"실제값\": true_chars,\n",
    "\t\t\"MSE\": round(mse, 4)\n",
    "\t}\n",
    "  \n",
    "# 각 RNN 방식별 실행\n",
    "train_and_predict(\"SimpleRNN\", SimpleRNN)\n",
    "train_and_predict(\"LSTM\", LSTM)\n",
    "train_and_predict(\"GRU\", GRU)\n",
    "  \n",
    "# 결과 출력\n",
    "df = pd.DataFrame(results).T\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-1 RNN 을 이용한 문장 생성(예측) 예제\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. 데이터 전처리\n",
    "# 학습할 문장\n",
    "text = \"hello world\"\n",
    "\n",
    "# 고유 문자 집합 만들기\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "print(\"문자-인덱스 딕셔너리 생성\")\n",
    "print(char_to_index)\n",
    "print(index_to_char)\n",
    "print(f\"문장 길이: {len(text)}\\n\")\n",
    "\n",
    "# 시퀀스 길이 설정 (3글자를 보고 1글자 예측)\n",
    "seq_length = 3\n",
    "\n",
    "# 학습 데이터(X), 라벨(y) 생성\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range(len(text) - seq_length):\n",
    "  seq_in = text[i:i+seq_length] # ex) \"hel\"\n",
    "  seq_out = text[i+seq_length] # ex) \"l\"\n",
    "  X_data.append([char_to_index[c] for c in seq_in])\n",
    "  y_data.append(char_to_index[seq_out])\n",
    "print(f\"X: {X_data}\")\n",
    "print(f\"y: {y_data}\")\n",
    "\n",
    "# One-Hot Encoding\n",
    "X = to_categorical(X_data, num_classes=len(chars))\n",
    "y = to_categorical(y_data, num_classes=len(chars))\n",
    "\n",
    "# 2. 모델 구성 및 학습\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(seq_length, len(chars))))\n",
    "model.add(SimpleRNN(128)) # activation 기본값 tanh\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "  optimizer = 'adam',\n",
    "  loss = 'categorical_crossentropy',\n",
    "  metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# model.summary\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "  X, y,\n",
    "  epochs = 500,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "# 3. 문장 생성 및 함수\n",
    "def generate_text(seed_text, n_chars):\n",
    "  result = seed_text\n",
    "  for _ in range(n_chars):\n",
    "    input_idx = [char_to_index[c] for c in seed_text]\n",
    "    input_seq = to_categorical([input_idx], num_classes=len(chars))\n",
    "    \n",
    "    # 예측\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    next_idx = np.argmax(prediction[0])\n",
    "    next_char = index_to_char[next_idx]\n",
    "    \n",
    "    # 결과 추가\n",
    "    result += next_char\n",
    "    print(f\"문자 생성중...: {result}\")\n",
    "    \n",
    "    # 다음 입력을 위한 시드 업데이트\n",
    "    seed_text = result[-seq_length:]\n",
    "  return result\n",
    "\n",
    "# 실행 예시\n",
    "# \"hel\" 값으로 20글자 생성\n",
    "print(f\"\\n최종 문장: {generate_text(\"hel\", 20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-2 LSTM 을 이용한 문장 생성(예측) 예제\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 학습할 문장\n",
    "text = \"hello world\"\n",
    "\n",
    "# 고유 문자 집합 만들기\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# 시퀀스 길이 설정 (3글자를 보고 다음 1글자 예측)\n",
    "seq_length = 3\n",
    "\n",
    "# 입력 시퀀스(X), 다음 문자(y) 만들기\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range(len(text) - seq_length):\n",
    "  seq_in = text[i:i+seq_length]\n",
    "  seq_out = text[i+seq_length]\n",
    "  X_data.append([char_to_index[c] for c in seq_in])\n",
    "  y_data.append(char_to_index[seq_out])\n",
    "  \n",
    "# One-Hot Encoding\n",
    "X = to_categorical(X_data, num_classes=len(chars))\n",
    "y = to_categorical(y_data, num_classes=len(chars))\n",
    "\n",
    "# 모델 구성 및 학습\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(seq_length, len(chars))))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "  optimizer = 'adam',\n",
    "  loss = 'categorical_crossentropy',\n",
    "  metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# model.summary\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "  X, y,\n",
    "  epochs = 500,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "# 3. 문장 생성 및 함수\n",
    "def generate_text(seed_text, n_chars):\n",
    "  result = seed_text\n",
    "  for _ in range(n_chars):\n",
    "    input_idx = [char_to_index[c] for c in seed_text]\n",
    "    input_seq = to_categorical([input_idx], num_classes=len(chars))\n",
    "    \n",
    "    # 예측\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    next_idx = np.argmax(prediction[0])\n",
    "    next_char = index_to_char[next_idx]\n",
    "    \n",
    "    # 결과 추가\n",
    "    result += next_char\n",
    "    print(f\"문자 생성중...: {result}\")\n",
    "    \n",
    "    # 다음 입력을 위한 시드 업데이트\n",
    "    seed_text = result[-seq_length:]\n",
    "  return result\n",
    "\n",
    "# 실행 예시\n",
    "# \"hel\" 값으로 20글자 생성\n",
    "print(f\"\\n최종 문장: {generate_text(\"hel\", 20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-3 GRU 을 이용한 문장 생성(예측) 예제\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 학습할 문장\n",
    "text = \"hello world\"\n",
    "\n",
    "# 고유 문자 집합 만들기\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# 시퀀스 길이 설정 (3글자를 보고 다음 1글자 예측)\n",
    "seq_length = 3\n",
    "\n",
    "# 입력 시퀀스(X), 다음 문자(y) 만들기\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range(len(text) - seq_length):\n",
    "  seq_in = text[i:i+seq_length]\n",
    "  seq_out = text[i+seq_length]\n",
    "  X_data.append([char_to_index[c] for c in seq_in])\n",
    "  y_data.append(char_to_index[seq_out])\n",
    "  \n",
    "# One-Hot Encoding\n",
    "X = to_categorical(X_data, num_classes=len(chars))\n",
    "y = to_categorical(y_data, num_classes=len(chars))\n",
    "\n",
    "# 모델 구성 및 학습\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(seq_length, len(chars))))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "  optimizer = 'adam',\n",
    "  loss = 'categorical_crossentropy',\n",
    "  metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# model.summary\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "  X, y,\n",
    "  epochs = 500,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "# 3. 문장 생성 및 함수\n",
    "def generate_text(seed_text, n_chars):\n",
    "  result = seed_text\n",
    "  for _ in range(n_chars):\n",
    "    input_idx = [char_to_index[c] for c in seed_text]\n",
    "    input_seq = to_categorical([input_idx], num_classes=len(chars))\n",
    "    \n",
    "    # 예측\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    next_idx = np.argmax(prediction[0])\n",
    "    next_char = index_to_char[next_idx]\n",
    "    \n",
    "    # 결과 추가\n",
    "    result += next_char\n",
    "    print(f\"문자 생성중...: {result}\")\n",
    "    \n",
    "    # 다음 입력을 위한 시드 업데이트\n",
    "    seed_text = result[-seq_length:]\n",
    "  return result\n",
    "\n",
    "# 실행 예시\n",
    "# \"hel\" 값으로 20글자 생성\n",
    "print(f\"\\n최종 문장: {generate_text(\"hel\", 20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 한글 긴 문장이나 짧은 문서 텍스트를 RNN & LSTM & GRU 기반으로 학습하는 방법\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"인공지능은 인간의 지능을 모방하여 다양한 문제를 해결하는 기술입니다. 자연어 처리, 이미지 분석, 추천 시스템 등에 활용됩니다\"\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "seq_length = 10\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range(len(text) - seq_length):\n",
    "  seq_in = text[i:i+seq_length]\n",
    "  seq_out = text[i+seq_length]\n",
    "  X_data.append([char_to_idx[c] for c in seq_in])\n",
    "  y_data.append(char_to_idx[seq_out])\n",
    "\n",
    "X = to_categorical(X_data, num_classes=len(chars))\n",
    "y = to_categorical(y_data, num_classes=len(chars))\n",
    "\n",
    "RNN_types = [SimpleRNN, LSTM, GRU]\n",
    "\n",
    "def build_model(RNN_list):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(seq_length, len(chars))))\n",
    "  model.add(RNN_list(512))\n",
    "  model.add(Dense(len(chars), activation='softmax'))\n",
    "  \n",
    "  model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy'\n",
    "\t)\n",
    "  return model\n",
    "\n",
    "for RNN_type in RNN_types:\n",
    "  print(f\"\\n{RNN_type.__name__} 모델 학습 및 예측\")\n",
    "  model = build_model(RNN_type)\n",
    "  model.summary()\n",
    "  \n",
    "  model.fit(\n",
    "    X, y,\n",
    "    epochs = 100,\n",
    "    verbose = 0\n",
    "\t)\n",
    "  \n",
    "  def generate_text(seed_text, n_chars):\n",
    "    result = seed_text\n",
    "    for _ in range(n_chars):\n",
    "      input_idx = [char_to_idx[c] for c in seed_text]\n",
    "      input_seq = to_categorical([input_idx], num_classes=len(chars))\n",
    "      \n",
    "      \n",
    "      pred = model.predict(input_seq, verbose=0)\n",
    "      next_idx = np.argmax(pred[0])\n",
    "      next_char = idx_to_char[next_idx]\n",
    "      \n",
    "      result += next_char\n",
    "      # print(f\"{RNN_type.__name__} 모델 문자 생성중: {result}\")\n",
    "      \n",
    "      seed_text = result[-seq_length:]\n",
    "    return result\n",
    "  \n",
    "  print(f\"\\n최종 문장: {generate_text(\"인공지능은\", 50)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
