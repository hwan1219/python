{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝을 이용한 자연어 처리\n",
    "# (Token, Word Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Token\n",
    "\n",
    "# token\n",
    "'''\n",
    "텍스트를 잘개 쪼갠 단위(토막)\n",
    "단어, 형태소, 글자, 서브워드 등 원하는 기준에 따라 달라짐\n",
    "모델이 처리할 최소 의미 단위\n",
    "'''\n",
    "\n",
    "# Tokenization\n",
    "'''\n",
    "원본 문장을 토큰들의 리스트로 변환하는 과정(알고리즘)\n",
    "전처리의 핵심 단계로 토큰화 전략에 따라 성능에 차이가 크게 발생\n",
    "공백 기준, 형태소 분석, BPE, SentencePiece 등\n",
    "\n",
    "split(), nltk.word_tokenize(), Tokenizer.texts_to_sequences()\n",
    "'''\n",
    "\n",
    "# Subword Tokenization\n",
    "'''\n",
    "단어를 부분 단위(subword)로 분할\n",
    "\"unhappiness\" → [\"un\", \"happi\", \"ness\"]\n",
    "\n",
    "tfds.deprecated.text.SubwordTextEncoder\n",
    "'''\n",
    "\n",
    "# Word Tokenization\n",
    "'''\n",
    "단어 기준으로 분할\n",
    "\"나는 밥을 먹었다\" → [\"나는\", \"밥을\", \"먹었다\"]\n",
    "\n",
    "nltk.word_tokenize(), split()\n",
    "'''\n",
    "\n",
    "# Character Tokenization\n",
    "'''\n",
    "문자 단위 분리\n",
    "\"NLP\" → [\"N\", \"L\", \"P\"]\n",
    "\n",
    "split()\n",
    "'''\n",
    "\n",
    "# Sentence Tokenization\n",
    "'''\n",
    "문장 기준 분할\n",
    "\"안녕. 나는 AI 야.\" [\"안녕.\", \"나는 AI야.\"]\n",
    "\n",
    "nltk.sent_tokenize()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 입력용 패딩 예시 (keras)\n",
    "# 여러 문장을 한 번에 시퀀스로 변환 & 패딩\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "sentences = [\"I love NLP\", \"NLP is fun\"]\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(sentences)\n",
    "seqs = tok.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(seqs, maxlen=5)\n",
    "print(f\"Padded:\\n{padded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token\n",
    "# 공백 기준 split후 str, index 표현현\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = \"I love natural language processing\"\n",
    "\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([sentence])\n",
    "\n",
    "tokens_str = sentence.split()\n",
    "tokens_seq = tok.texts_to_sequences([sentence])[0]\n",
    "\n",
    "print(f\"Token (str 기준): {tokens_str}\")\n",
    "print(f\"Token (index 기준): {tokens_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1 기본 python split\n",
    "\n",
    "text = \"I love NLP\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk 라이브러리 사용을 위해 외부 리소스 다운로드\n",
    "\n",
    "# nltk 는 오픈소스지만\n",
    "# 내부 모델이나 리소스는 용량도 크고,\n",
    "# 다국어 모델도 많고,\n",
    "# 일부는 라이센스 문제도 있을 수 있기 때문에\n",
    "# 설치 시 본체(pip install) 와 리소스 파일(nltk.download) 을 따로 분리해서 관리하는 구조\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', download_dir='C:/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러 발생 - python 3.12(최신버전) 호환성 문제 유력\n",
    "\n",
    "# # 1-2 nltk 라이브러리 사용\n",
    "\n",
    "# # nltk 라이브러리를 사용하기 위해 다운받은 외부 리소스 파일 경로 설정\n",
    "# import nltk\n",
    "# nltk.data.path.append('C:/nltk_data')\n",
    "\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# text = \"Hello world. I love NLP\"\n",
    "# print(f\"단어 기준 분할:\\n{word_tokenize(text)}\")\n",
    "# print(f\"문장 기준 분할:\\n{sent_tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 nltk 라이브러리 사용\n",
    "\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# punkt 로드\n",
    "with open(\"C:/nltk_data/tokenizers/punkt/english.pickle\", \"rb\") as f:\n",
    "  sentence_tokenizer = pickle.load(f)\n",
    "\n",
    "text = \"Hello world. I love NLP\"\n",
    "  \n",
    "# 문장 토큰화\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "print(f\"문장 기준 분할:\\n{sentences}\")\n",
    "\n",
    "# 단어 토큰화\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "print(\"\\n단어 기준 분할:\")\n",
    "for sent in sentences:\n",
    "  print(f\"{word_tokenizer.tokenize(sent)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3 tensorflow/keras Tokenizer (빈도 기반)\n",
    "# 빈도가 같다면 먼저 등장한 순으로 인덱스 부여\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "texts = [\"I love NLP\", \"NLP is fun\"]\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(texts)\n",
    "\n",
    "print(tok.word_index)\n",
    "print(tok.texts_to_sequences([texts[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-4 tensorflow datasets SubwordTextEncoder\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# *corpus = 말뭉치\n",
    "corpus = [\"TenserFlow is great\", \"Tokenization is important\"]\n",
    "encoder = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus, target_vocab_size=258)\n",
    "\n",
    "encoded = encoder.encode(corpus[1])\n",
    "decoded = encoder.decode(encoded)\n",
    "\n",
    "print(f\"텍스트 원본: {corpus[1]}\")\n",
    "print(f\"SubwordTextEncoder: {encoded}\")\n",
    "print(f\"decode: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통합 실행 예제\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import tensorflow_datasets as tfds\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 1. 샘플 텍스트\n",
    "kor_text = \"나는 밥을 먹었다\"\n",
    "eng_text = \"I love natural language processing\"\n",
    "\n",
    "# 2. Whitespace (공백 기준 분할)\n",
    "print(\"Whitespace / split()\")\n",
    "print(\"공백 기준 분할:\")\n",
    "print(kor_text.split())\n",
    "print(f\"{eng_text.split()}\")\n",
    "\n",
    "# 3. nltk\n",
    "word_tok = TreebankWordTokenizer()\n",
    "\n",
    "print(\"\\nnltk / TreebankWordTokenizer()\")\n",
    "print(\"단어 기준 분할:\")\n",
    "print(word_tok.tokenize(kor_text))\n",
    "print(f\"{word_tok.tokenize(eng_text)}\")\n",
    "\n",
    "# 4. keras Tokenizer\n",
    "tok_kor = Tokenizer()\n",
    "tok_kor.fit_on_texts([kor_text])\n",
    "\n",
    "tok_eng = Tokenizer()\n",
    "tok_eng.fit_on_texts([eng_text])\n",
    "\n",
    "print(\"\\ntensorflow.keras / Tokenizer()\")\n",
    "print(\"빈도 기반 인덱스 부여:\")\n",
    "print(tok_kor.word_index)\n",
    "print(tok_kor.texts_to_sequences([kor_text]))\n",
    "\n",
    "print(tok_eng.word_index)\n",
    "print(tok_eng.texts_to_sequences([eng_text]))\n",
    "\n",
    "# 5. SubwordTextEncdoer\n",
    "encoder_kor = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus([kor_text], target_vocab_size=258)\n",
    "encoded_kor = encoder_kor.encode(kor_text)\n",
    "\n",
    "encoder_eng = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus([eng_text], target_vocab_size=258)\n",
    "encoded_eng = encoder_eng.encode(eng_text)\n",
    "\n",
    "print(\"\\ntensorflow_datasets / SubwordTextEncoder()\")\n",
    "print(\"단어 조각 단위 분할:\")\n",
    "print(encoded_kor)\n",
    "print(encoded_eng)\n",
    "\n",
    "# 6. konlpy Okt\n",
    "okt = Okt()\n",
    "\n",
    "print(\"\\nkonlpy.tag / Okt()\")\n",
    "print(\"형태소 분석(한글 전용):\")\n",
    "print(okt.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 전체 파이프 라인\n",
    "# keras Tokenize 를 이용해 문장 → 정수 시퀀스로 바꾸는 전 과정\n",
    "# 토큰화 → 패딩 → 임베딩 레이어 → 임베딩 가중치 조회\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
    "\n",
    "# 1. 샘플 문장\n",
    "texts = [\n",
    "  \"TensorFlow is great and powerful\",\n",
    "  \"Tokenization converts words to numbers\",\n",
    "  \"TensorFlow makes tokenization easy\"\n",
    "]\n",
    "\n",
    "# 2. Tokenizer 설정 & 학습\n",
    "tok = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tok.fit_on_texts(texts)\n",
    "word_index = tok.word_index\n",
    "print(f\"단어 인덱스 매핑:\\n{word_index}\")\n",
    "\n",
    "# 3. 문장 → 정수 시퀀스\n",
    "sequences = tok.texts_to_sequences(texts)\n",
    "print(f\"\\n정수 시퀀스:\\n{sequences}\")\n",
    "\n",
    "# 4. 패딩 (길이 8로 통일)\n",
    "padded = pad_sequences(\n",
    "  sequences,\n",
    "  maxlen=8, # 원하는 고정 길이\n",
    "  padding='pre', # 'pre': 앞쪽을 0으로 채움\n",
    "  truncating='pre' # 길면 앞쪽부터 자르기\n",
    ")\n",
    "print(f\"\\n패딩 후 시퀀스(shape={padded.shape}):\\n{padded}\")\n",
    "\n",
    "# 5. Embedding 모델 정의\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "# Tokenizer는 단어 인덱스를 1부터 매기지만, Embedding은 0번 인덱스까지 처리해야 하므로 반드시 +1 을 해서 input_dim 을 설정해줘야 함\n",
    "embedding_dim = 4\n",
    "model = Sequential([\n",
    "  Input(shape=(padded.shape[1],)),\n",
    "  Embedding(\n",
    "\t\t # 패딩 maxlen 값\n",
    "\t\t# padded.shape[0] = batch_size 값\n",
    "\t\t# padded.shape[1] = sequence length 값\n",
    "\t\tinput_dim=vocab_size,\n",
    "\t\toutput_dim=embedding_dim\n",
    "\t),\n",
    "  Flatten(),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy'\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 6. 더미 예측\n",
    "_ = model.predict(padded)\n",
    "\n",
    "# 7. 임베딩 가중치 확인\n",
    "embedding_layer = model.layers[0]\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1 Word Embedding 과 One-Hot Encoding 비교\n",
    "\n",
    "# Word Embedding 과 One-Hot Encoding 은 자연어 처리에서 단어를 숫자 벡터로 표현하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1\n",
    "\n",
    "# 1) One-Hot Encoding 예제\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "words = ['apple', 'banana', 'cherry', 'apple']\n",
    "\n",
    "# 정수 인코딩\n",
    "le = LabelEncoder()\n",
    "int_encoded = le.fit_transform(words)\n",
    "\n",
    "# 원-핫 인코딩\n",
    "one_hot = to_categorical(int_encoded)\n",
    "\n",
    "print(f\"정수 인코딩:\\n{int_encoded}\\n\")\n",
    "print(f\"원-핫 인코딩:\\n{one_hot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1\n",
    "\n",
    "# 2) Word Embedding (Keras Embedding Layer) 예제\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
    "\n",
    "# 1. 정수로 인코딩된 문장 데이터\n",
    "# 각 숫자는 단어를 의미, ex) 1=apple, 2=banana, ...\n",
    "X = np.array([\n",
    "  [1, 2, 3],\n",
    "  [4, 5, 6]\n",
    "])\n",
    "\n",
    "# 2. 모델 구성\n",
    "# Sequential 모델을 사용하여 임베딩 → 평탄화 → 출력\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X.shape[1],)))\n",
    "model.add(Embedding(\n",
    "  input_dim=10, # 단어 집합 크기 (0~9까지 10개 단어로 가정)\n",
    "  output_dim=4, # 임베딩 차원 수 (단어 하나당 4차원 벡터로 표현)\n",
    "))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 컴파일 및 요약\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy'\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 4. 예측 수행 (훈련 없이 바로 predict)\n",
    "# (사실상 의미 없는) 초기 가중치로 출력값 계산\n",
    "output = model.predict(X)\n",
    "print(f\"임베딩된 벡터 → 최종 출력 결과:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding 실습\n",
    "# 3-1\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "# 전처리할 텍스트\n",
    "text = \"해보지 않으면 해낼 수 없다\"\n",
    "\n",
    "# 해당 텍스트를 토큰화\n",
    "result = text_to_word_sequence(text)\n",
    "print(f\"원문: {text}\")\n",
    "print(f\"토큰화: {result}\\n\")\n",
    "\n",
    "# 단어 빈도수 세기\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "print(f\"text 단어 빈도수:\\n{tok.word_counts}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding 실습\n",
    "# 3-2\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 전처리하려는 세 개의 문장\n",
    "docs = [\n",
    "  \"먼저 텍스트의 각 단어를 나누어 토큰화 합니다.\",\n",
    "  \"텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.\",\n",
    "  \"토큰화한 결과는 딥러닝에서 사용할 수 있습니다.\"\n",
    "]\n",
    "\n",
    "# 토큰화 함수를 이용해 전처리\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(docs)\n",
    "\n",
    "# 단어의 빈도수 세기\n",
    "print(f\"docs 단어 빈도수:\\n{tok.word_counts}\\n\")\n",
    "\n",
    "# 출력되는 순서는 랜덤\n",
    "print(f\"문장 카운트: {tok.document_count}\\n\")\n",
    "print(f\"각 단어가 몇 개의 문장에 포함되어 있는가:\\n{tok.word_docs}\\n\")\n",
    "print(f\"각 단어에 매겨진 인덱스 값:\\n{tok.word_index}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding 실습\n",
    "# 3-3\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 전처리할 텍스트\n",
    "text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n",
    "\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "x = tok.texts_to_sequences([text])\n",
    "print(f\"단어 → 정수 매핑: {tok.word_index}\\n\")\n",
    "print(f\"문장을 실제로 인덱스 시퀀스로 변환:\\n{x}\\n\")\n",
    "\n",
    "# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 생성\n",
    "word_size = len(tok.word_index) + 1\n",
    "x = to_categorical(x, num_classes=word_size)\n",
    "print(f\"One-Hot Encoding 결과:\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding 실습\n",
    "# 3-4\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "import numpy as np\n",
    "\n",
    "# 텍스트 리뷰 자료를 지정\n",
    "docs = [\n",
    "  \"너무 재밌네요\", \"최고예요\", \"참 잘 만든 영화예요\", \"추천하고 싶은 영화입니다\", \"한번 더보고싶네요\", \"글쎄요\", \"별로예요\", \"생각보다 지루하네요\", \"연기가 어색해요\", \"재미없어요\"\n",
    "]\n",
    "\n",
    "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스 지정\n",
    "classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "\n",
    "# 토큰화\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(docs)\n",
    "print(f\"단어 → 정수 매핑:\\n{tok.word_index}\\n\")\n",
    "\n",
    "# 문장을 실제로 인덱스 시퀀스로 변환\n",
    "x = tok.texts_to_sequences(docs)\n",
    "print(f\"문장 → 인덱스 시퀀스로 변환: \\n{x}\\n\")\n",
    "\n",
    "# 패딩, 서로 다른 길이의 데이터를 4로 맞춤\n",
    "padded_x = pad_sequences(x, 4)\n",
    "print(f\"패딩 결과:\\n{padded_x}\")\n",
    "\n",
    "# 임베딩에 입력될 단어의 수를 지정 (input_dim)\n",
    "word_size = len(tok.word_index) + 1\n",
    "\n",
    "# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(padded_x.shape[1],)))\n",
    "model.add(Embedding(word_size, 8))\n",
    "# Embedding(input_dim, output_dim)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  padded_x, classes,\n",
    "  epochs=20,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "loss, acc = model.evaluate(padded_x, classes)\n",
    "print(f\"\\nLoss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
